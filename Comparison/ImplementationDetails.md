
# Description

The implementation details for the PPO, A2C, and DQN algorithms applied to the Pong environment are outlined in the Results folder.<br>
Each document includes the complete training and evaluation codes for the algorithms, providing a comprehensive view of the training process as well as the evaluation of each agent over 10,000 timesteps.<br>
The hyperparameters are set to default values, and all algorithms have been trained for 10,000,000 timesteps.<br>
However, these hyperparameters can be modified as needed to suit specific experimental setups.<br>
The results are presented in two formats:
- one showing the relationship between "Average Rewards" and "Time" 
- and another illustrating "Episode Lengths" as a function of "Time.‚Äù<br>

To create smoother diagrams we averaged rewards and episode lengths over 50 episodes, which is why the term "average" is used.<br>
<br>
Additionally, a comparative analysis is provided in the Comparison file, where the performance of all three algorithms is visualized in a single diagram for clarity.<br>
The findings indicate that the PPO algorithm demonstrates superior performance, with faster improvement in average rewards, higher final average rewards, and greater stability compared to A2C and DQN.<br>
When comparing A2C and DQN, DQN exhibits faster reward improvement and achieves higher final rewards but shows less stability during training.<br>
A2C, while less performant in terms of final rewards, maintains greater stability throughout training.<br>
<br>
Regarding the "Average Episode Length," PPO again stands out with the shortest and most stable episode lengths.<br>
Conversely, DQN exhibits significant fluctuations, including a noticeable peak in the episode length plot, followed by a decrease that ultimately results in shorter episode lengths compared to A2C.<br>
The instability of DQN is evident in this analysis as well.<br>
<br>

At the end of the comparison file, the overall average rewards and episode lengths during training were computed.<br>
The overall average rewards are as follows: 
- **PPO (16)** 
- **DQN (13)**
- **A2C (-5.6)**<br>

The overall average episode lengths are:
- **PPO (6,673)**
- **DQN (7,599)**
- **A2C (6,421)**<br>

These trends are further validated during the evaluation phase. In this phase:
- PPO achieved an Average Episode Score of 21 and an Average Episode Length of 6,587.
- DQN achieved an Average Episode Score of 20.6 and an Average Episode Length of 6,834.
- A2C achieved an Average Episode Score of 18 and an Average Episode Length of 8,917.
<br>
These results underscore the overall effectiveness and stability of PPO compared to A2C and DQN in the Pong environment.

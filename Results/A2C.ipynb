{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010f0bc5-f0a4-40f9-b1d9-d86b883e8b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7487da768ce0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3598de86-2acf-491e-9718-496eb492186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PongNoFrameskip-v4 ==========\n",
      "Seed: 1497929686\n",
      "Loading hyperparameters from: /home/akari9/anaconda3/envs/sb3/lib/python3.12/site-packages/rl_zoo3/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.01),\n",
      "             ('env_wrapper',\n",
      "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
      "             ('frame_stack', 4),\n",
      "             ('n_envs', 16),\n",
      "             ('n_timesteps', 10000000.0),\n",
      "             ('policy', 'CnnPolicy'),\n",
      "             ('policy_kwargs',\n",
      "              'dict(optimizer_class=RMSpropTFLike, '\n",
      "              'optimizer_kwargs=dict(eps=1e-5))'),\n",
      "             ('vf_coef', 0.25)])\n",
      "Using 16 environments\n",
      "Creating test environment\n",
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stacking 4 frames\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cuda device\n",
      "Log path: logs/a2c/PongNoFrameskip-v4_3\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | -0.0237  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    value_loss         | 0.0946   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.57e+03 |\n",
      "|    ep_rew_mean        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1030     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | -0.0123  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.216   |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.87e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1082     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | -0.00898 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=24992, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3635.40 +/- 373.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.64e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24992    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | -0.144   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 312      |\n",
      "|    policy_loss        | 0.1      |\n",
      "|    value_loss         | 0.00395  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | -0.00922 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0502  |\n",
      "|    value_loss         | 0.0665   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 946      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | -0.0281  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.166    |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 978      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.0129   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0754   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=49984, episode_reward=-20.40 +/- 1.20\n",
      "Episode length: 3484.20 +/- 457.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.48e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49984    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.00264  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 624      |\n",
      "|    policy_loss        | 0.0286   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.0645   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0458  |\n",
      "|    value_loss         | 0.0718   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 931      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.0108   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.62e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 954      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.0175   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.08    |\n",
      "|    value_loss         | 0.077    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=74976, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3238.20 +/- 187.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.24e+03 |\n",
      "|    mean_reward        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74976    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.00682  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 937      |\n",
      "|    policy_loss        | 0.132    |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.6e+03  |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.00922  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.0655  |\n",
      "|    value_loss         | 0.0684   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.6e+03  |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 927      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.0382   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.221   |\n",
      "|    value_loss         | 0.158    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.61e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 945      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.0937   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0678  |\n",
      "|    value_loss         | 0.0927   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=99968, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3453.00 +/- 382.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.45e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99968    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | -0.0485  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1249     |\n",
      "|    policy_loss        | 0.0339   |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.0398   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.6e+03  |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 924      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.076    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0106   |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.6e+03  |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 937      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.182    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0451   |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=124960, episode_reward=-21.00 +/- 0.00\n",
      "Episode length: 3056.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.06e+03 |\n",
      "|    mean_reward        | -21      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124960   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.276    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1561     |\n",
      "|    policy_loss        | -0.00695 |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.56e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 912      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.227    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0642   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.53e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 925      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.427    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.0635   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.5e+03  |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 936      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.555    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.012    |\n",
      "|    value_loss         | 0.0846   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=149952, episode_reward=-21.00 +/- 0.00\n",
      "Episode length: 3126.40 +/- 95.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.13e+03 |\n",
      "|    mean_reward        | -21      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 149952   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.467    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1874     |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    value_loss         | 0.0762   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.47e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 914      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 166      |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.548    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0897   |\n",
      "|    value_loss         | 0.098    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.44e+03 |\n",
      "|    ep_rew_mean        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 924      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.0449   |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.41e+03 |\n",
      "|    ep_rew_mean        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 935      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.333    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0851  |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=174944, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3457.00 +/- 493.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.46e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 174944   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2186     |\n",
      "|    policy_loss        | 0.00522  |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.37e+03 |\n",
      "|    ep_rew_mean        | -20.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 913      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 192      |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.564    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0635  |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.34e+03 |\n",
      "|    ep_rew_mean        | -20.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 922      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 199      |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.0766   |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.3e+03  |\n",
      "|    ep_rew_mean        | -20.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 931      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 206      |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.684    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0666   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=199936, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3563.40 +/- 288.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.56e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 199936   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.577    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.0369   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.31e+03 |\n",
      "|    ep_rew_mean     | -20.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.3e+03  |\n",
      "|    ep_rew_mean        | -20.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 919      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 226      |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.342    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.00145  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.39e+03 |\n",
      "|    ep_rew_mean        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 927      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 232      |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.41e+03 |\n",
      "|    ep_rew_mean        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 935      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.00607 |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=224928, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 3869.00 +/- 337.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.87e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 224928   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2811     |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.0243   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.46e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 915      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 253      |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.29     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.117    |\n",
      "|    value_loss         | 0.09     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.51e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 922      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 260      |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.242    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.53e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 929      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 266      |\n",
      "|    total_timesteps    | 248000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.736    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=249920, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3465.20 +/- 161.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.47e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 249920   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3123     |\n",
      "|    policy_loss        | 0.016    |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.57e+03 |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 913      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 280      |\n",
      "|    total_timesteps    | 256000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.484    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00738 |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.59e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 920      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 286      |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.626    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.0338   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.64e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 926      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 293      |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.0606  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=274912, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3743.40 +/- 282.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.74e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 274912   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.317    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3436     |\n",
      "|    policy_loss        | 0.0336   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 911      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 307      |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.0406   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.04    |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 917      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 313      |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.66e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 923      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 320      |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.35     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.0369  |\n",
      "|    value_loss         | 0.0748   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=299904, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3622.80 +/- 403.70\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 3.62e+03  |\n",
      "|    mean_reward        | -20.8     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 299904    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | 0.66      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3748      |\n",
      "|    policy_loss        | -0.000674 |\n",
      "|    value_loss         | 0.0485    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.66e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 910      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 333      |\n",
      "|    total_timesteps    | 304000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.0442   |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 916      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 340      |\n",
      "|    total_timesteps    | 312000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.492    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.00601 |\n",
      "|    value_loss         | 0.0521   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.67e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 922      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 347      |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.652    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.00876  |\n",
      "|    value_loss         | 0.00573  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=324896, episode_reward=-19.60 +/- 0.49\n",
      "Episode length: 3941.80 +/- 429.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.94e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 324896   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4061     |\n",
      "|    policy_loss        | -0.0441  |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.68e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 360      |\n",
      "|    total_timesteps    | 328000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 913      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 367      |\n",
      "|    total_timesteps    | 336000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.00468  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 918      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 374      |\n",
      "|    total_timesteps    | 344000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.00952  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=349888, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3645.40 +/- 470.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.65e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 349888   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4373     |\n",
      "|    policy_loss        | -0.0471  |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 387      |\n",
      "|    total_timesteps    | 352000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0378   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 912      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 394      |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.082   |\n",
      "|    value_loss         | 0.0246   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 916      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 401      |\n",
      "|    total_timesteps    | 368000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.0337   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=374880, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3673.60 +/- 544.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.67e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 374880   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4685     |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.00335  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 376000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.0547   |\n",
      "|    value_loss         | 0.0048   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 911      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 421      |\n",
      "|    total_timesteps    | 384000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 915      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 428      |\n",
      "|    total_timesteps    | 392000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.00443  |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=399872, episode_reward=-20.00 +/- 1.26\n",
      "Episode length: 3805.20 +/- 288.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.81e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 399872   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4998     |\n",
      "|    policy_loss        | 0.0273   |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 441      |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.00777  |\n",
      "|    value_loss         | 0.00913  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 909      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 448      |\n",
      "|    total_timesteps    | 408000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -0.0679  |\n",
      "|    value_loss         | 0.00769  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 914      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 455      |\n",
      "|    total_timesteps    | 416000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 918      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 461      |\n",
      "|    total_timesteps    | 424000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.0687  |\n",
      "|    value_loss         | 0.0618   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=424864, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3868.40 +/- 505.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.87e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 424864   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5310     |\n",
      "|    policy_loss        | 0.0631   |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.67e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 475      |\n",
      "|    total_timesteps    | 432000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.69e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 912      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 482      |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.0565   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 915      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 489      |\n",
      "|    total_timesteps    | 448000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.612    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.0466  |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=449856, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3755.20 +/- 496.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.76e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 449856   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.741    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5623     |\n",
      "|    policy_loss        | -0.0539  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 502      |\n",
      "|    total_timesteps    | 456000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.653    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.0777   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 910      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 509      |\n",
      "|    total_timesteps    | 464000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.0379   |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 914      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 516      |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 0.0533   |\n",
      "|    value_loss         | 0.00472  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=474848, episode_reward=-20.40 +/- 1.20\n",
      "Episode length: 3819.80 +/- 430.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.82e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 474848   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5935     |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 530      |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -0.014   |\n",
      "|    value_loss         | 0.00459  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.7e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 909      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 536      |\n",
      "|    total_timesteps    | 488000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | -0.009   |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 912      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 543      |\n",
      "|    total_timesteps    | 496000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.566    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.137    |\n",
      "|    value_loss         | 0.0568   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=499840, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3654.60 +/- 191.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.65e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 499840   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6247     |\n",
      "|    policy_loss        | -0.064   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.69e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 557      |\n",
      "|    total_timesteps    | 504000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.574    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.068    |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.69e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 563      |\n",
      "|    total_timesteps    | 512000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.137   |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 911      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 570      |\n",
      "|    total_timesteps    | 520000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.0353   |\n",
      "|    value_loss         | 0.0311   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=524832, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3414.80 +/- 188.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.41e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 524832   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6560     |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 583      |\n",
      "|    total_timesteps    | 528000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.0561  |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.67e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 590      |\n",
      "|    total_timesteps    | 536000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 911      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 596      |\n",
      "|    total_timesteps    | 544000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 0.00832  |\n",
      "|    value_loss         | 0.0062   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=549824, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3814.40 +/- 271.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.81e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 549824   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6872     |\n",
      "|    policy_loss        | 0.0138   |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.6e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 610      |\n",
      "|    total_timesteps    | 552000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 0.035    |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.59e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 617      |\n",
      "|    total_timesteps    | 560000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.0796   |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.57e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 910      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 624      |\n",
      "|    total_timesteps    | 568000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.583    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0599   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=574816, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3392.20 +/- 176.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.39e+03 |\n",
      "|    mean_reward        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 574816   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7185     |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.0489   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.56e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 637      |\n",
      "|    total_timesteps    | 576000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.54e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 643      |\n",
      "|    total_timesteps    | 584000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.0766   |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.56e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 910      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 650      |\n",
      "|    total_timesteps    | 592000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -0.0708  |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=599808, episode_reward=-20.20 +/- 0.98\n",
      "Episode length: 3781.00 +/- 340.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.78e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 599808   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7497     |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.57e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 663      |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 0.0822   |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.56e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 670      |\n",
      "|    total_timesteps    | 608000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 0.00468  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.57e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 909      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 677      |\n",
      "|    total_timesteps    | 616000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.601    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.17    |\n",
      "|    value_loss         | 0.0701   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.59e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 912      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 683      |\n",
      "|    total_timesteps    | 624000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=624800, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3789.60 +/- 263.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.79e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 624800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7809     |\n",
      "|    policy_loss        | -0.0949  |\n",
      "|    value_loss         | 0.0759   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.61e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 697      |\n",
      "|    total_timesteps    | 632000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.0365   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.63e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 704      |\n",
      "|    total_timesteps    | 640000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.02    |\n",
      "|    value_loss         | 0.00897  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.7e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 911      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 711      |\n",
      "|    total_timesteps    | 648000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    value_loss         | 0.0577   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=649792, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3455.40 +/- 228.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.46e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 649792   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.708    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8122     |\n",
      "|    policy_loss        | -0.0326  |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 724      |\n",
      "|    total_timesteps    | 656000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 731      |\n",
      "|    total_timesteps    | 664000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 910      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 737      |\n",
      "|    total_timesteps    | 672000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.775    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -0.0597  |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=674784, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3891.60 +/- 330.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.89e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 674784   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8434     |\n",
      "|    policy_loss        | -0.0665  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 751      |\n",
      "|    total_timesteps    | 680000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 0.0492   |\n",
      "|    value_loss         | 0.00561  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 758      |\n",
      "|    total_timesteps    | 688000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 909      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 765      |\n",
      "|    total_timesteps    | 696000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.0934   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=699776, episode_reward=-19.20 +/- 1.17\n",
      "Episode length: 4346.20 +/- 659.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.35e+03 |\n",
      "|    mean_reward        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 699776   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8747     |\n",
      "|    policy_loss        | -0.0388  |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 779      |\n",
      "|    total_timesteps    | 704000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 0.00852  |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 786      |\n",
      "|    total_timesteps    | 712000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -0.00277 |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 793      |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.0174   |\n",
      "|    value_loss         | 0.00718  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=724768, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3641.00 +/- 457.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.64e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 724768   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9059     |\n",
      "|    policy_loss        | -0.0316  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 806      |\n",
      "|    total_timesteps    | 728000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.046   |\n",
      "|    value_loss         | 0.00658  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 813      |\n",
      "|    total_timesteps    | 736000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -0.00136 |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 819      |\n",
      "|    total_timesteps    | 744000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.0263  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=749760, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3561.20 +/- 178.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.56e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 749760   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9371     |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.71e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 832      |\n",
      "|    total_timesteps    | 752000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.642    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.0291   |\n",
      "|    value_loss         | 0.0903   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.7e+03  |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 839      |\n",
      "|    total_timesteps    | 760000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 846      |\n",
      "|    total_timesteps    | 768000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=774752, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3628.80 +/- 287.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.63e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 774752   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9684     |\n",
      "|    policy_loss        | -0.0624  |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 859      |\n",
      "|    total_timesteps    | 776000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 0.00692  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 866      |\n",
      "|    total_timesteps    | 784000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.055    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 873      |\n",
      "|    total_timesteps    | 792000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.0783  |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=799744, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3682.40 +/- 319.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.68e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 799744   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9996     |\n",
      "|    policy_loss        | 0.0244   |\n",
      "|    value_loss         | 0.0405   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 886      |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -0.0625  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 893      |\n",
      "|    total_timesteps    | 808000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.553    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | 0.00704  |\n",
      "|    value_loss         | 0.177    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 899      |\n",
      "|    total_timesteps    | 816000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.588    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.0783   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 906      |\n",
      "|    total_timesteps    | 824000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -0.09    |\n",
      "|    value_loss         | 0.0467   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=824736, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3514.60 +/- 263.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.51e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 824736   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10309    |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 919      |\n",
      "|    total_timesteps    | 832000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | -0.0519  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 10500    |\n",
      "|    time_elapsed       | 926      |\n",
      "|    total_timesteps    | 840000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.0844   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 933      |\n",
      "|    total_timesteps    | 848000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | 0.0749   |\n",
      "|    value_loss         | 0.0497   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=849728, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3617.60 +/- 341.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.62e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 849728   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10621    |\n",
      "|    policy_loss        | 0.0909   |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 946      |\n",
      "|    total_timesteps    | 856000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | -0.00543 |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 10800    |\n",
      "|    time_elapsed       | 953      |\n",
      "|    total_timesteps    | 864000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | 0.166    |\n",
      "|    value_loss         | 0.0576   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 908      |\n",
      "|    iterations         | 10900    |\n",
      "|    time_elapsed       | 959      |\n",
      "|    total_timesteps    | 872000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=874720, episode_reward=-19.60 +/- 1.36\n",
      "Episode length: 4121.80 +/- 409.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.12e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 874720   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10933    |\n",
      "|    policy_loss        | 0.0376   |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 11000    |\n",
      "|    time_elapsed       | 973      |\n",
      "|    total_timesteps    | 880000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 11100    |\n",
      "|    time_elapsed       | 980      |\n",
      "|    total_timesteps    | 888000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | -0.0333  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 11200    |\n",
      "|    time_elapsed       | 987      |\n",
      "|    total_timesteps    | 896000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | -0.0466  |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=899712, episode_reward=-19.80 +/- 0.98\n",
      "Episode length: 4018.00 +/- 289.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.02e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 899712   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11246    |\n",
      "|    policy_loss        | -0.0565  |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 1001     |\n",
      "|    total_timesteps    | 904000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | -0.034   |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 11400    |\n",
      "|    time_elapsed       | 1008     |\n",
      "|    total_timesteps    | 912000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0.0409  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 1014     |\n",
      "|    total_timesteps    | 920000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=924704, episode_reward=-21.00 +/- 0.00\n",
      "Episode length: 3568.60 +/- 252.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.57e+03 |\n",
      "|    mean_reward        | -21      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 924704   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11558    |\n",
      "|    policy_loss        | -0.0486  |\n",
      "|    value_loss         | 0.00918  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 1027     |\n",
      "|    total_timesteps    | 928000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | 0.0198   |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 11700    |\n",
      "|    time_elapsed       | 1034     |\n",
      "|    total_timesteps    | 936000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | -0.199   |\n",
      "|    value_loss         | 0.097    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 11800    |\n",
      "|    time_elapsed       | 1041     |\n",
      "|    total_timesteps    | 944000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -0.0759  |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=949696, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3815.00 +/- 514.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.82e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 949696   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.625    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11871    |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0379   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 1054     |\n",
      "|    total_timesteps    | 952000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.7e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 12000    |\n",
      "|    time_elapsed       | 1061     |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | 0.0351   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 1068     |\n",
      "|    total_timesteps    | 968000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | -0.0486  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=974688, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3601.60 +/- 346.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.6e+03  |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 974688   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12183    |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 1081     |\n",
      "|    total_timesteps    | 976000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.0926  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 1088     |\n",
      "|    total_timesteps    | 984000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 1094     |\n",
      "|    total_timesteps    | 992000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | -0.081   |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=999680, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3728.40 +/- 580.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.73e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 999680   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12495    |\n",
      "|    policy_loss        | 0.0133   |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 1108     |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.0381   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 12600    |\n",
      "|    time_elapsed       | 1114     |\n",
      "|    total_timesteps    | 1008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | 0.193    |\n",
      "|    value_loss         | 0.0622   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 1121     |\n",
      "|    total_timesteps    | 1016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 907      |\n",
      "|    iterations         | 12800    |\n",
      "|    time_elapsed       | 1128     |\n",
      "|    total_timesteps    | 1024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1024672, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 4207.60 +/- 412.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.21e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1024672  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12808    |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 1142     |\n",
      "|    total_timesteps    | 1032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -0.00234 |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 13000    |\n",
      "|    time_elapsed       | 1149     |\n",
      "|    total_timesteps    | 1040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0.00527 |\n",
      "|    value_loss         | 0.00908  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 906      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 1156     |\n",
      "|    total_timesteps    | 1048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1049664, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3917.60 +/- 455.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.92e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1049664  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13120    |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.84e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 1169     |\n",
      "|    total_timesteps    | 1056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -0.202   |\n",
      "|    value_loss         | 0.0619   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 1176     |\n",
      "|    total_timesteps    | 1064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | 0.0566   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 1183     |\n",
      "|    total_timesteps    | 1072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.0684  |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1074656, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3545.80 +/- 166.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.55e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1074656  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13433    |\n",
      "|    policy_loss        | -0.00512 |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 13500    |\n",
      "|    time_elapsed       | 1196     |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.00818  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 1203     |\n",
      "|    total_timesteps    | 1088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | 0.000396 |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.69e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 1210     |\n",
      "|    total_timesteps    | 1096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 0.00906  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1099648, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3514.20 +/- 263.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.51e+03 |\n",
      "|    mean_reward        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1099648  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13745    |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.67e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 1223     |\n",
      "|    total_timesteps    | 1104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | 0.013    |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.64e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 1229     |\n",
      "|    total_timesteps    | 1112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 14000    |\n",
      "|    time_elapsed       | 1236     |\n",
      "|    total_timesteps    | 1120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | 0.0658   |\n",
      "|    value_loss         | 0.046    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1124640, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3855.40 +/- 317.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1124640  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14057    |\n",
      "|    policy_loss        | -0.0279  |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 14100    |\n",
      "|    time_elapsed       | 1250     |\n",
      "|    total_timesteps    | 1128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | -0.00643 |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.65e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 14200    |\n",
      "|    time_elapsed       | 1257     |\n",
      "|    total_timesteps    | 1136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | 0.0451   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.66e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 1263     |\n",
      "|    total_timesteps    | 1144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 0.106    |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1149632, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 3958.00 +/- 367.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.96e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1149632  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14370    |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.66e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 1277     |\n",
      "|    total_timesteps    | 1152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | 0.0728   |\n",
      "|    value_loss         | 0.00741  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.68e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 1284     |\n",
      "|    total_timesteps    | 1160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | -0.0655  |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.7e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 14600    |\n",
      "|    time_elapsed       | 1291     |\n",
      "|    total_timesteps    | 1168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 0.124    |\n",
      "|    value_loss         | 0.0496   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1174624, episode_reward=-19.40 +/- 1.20\n",
      "Episode length: 3911.20 +/- 563.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.91e+03 |\n",
      "|    mean_reward        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1174624  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14682    |\n",
      "|    policy_loss        | 0.00945  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 1305     |\n",
      "|    total_timesteps    | 1176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | -0.0959  |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 14800    |\n",
      "|    time_elapsed       | 1312     |\n",
      "|    total_timesteps    | 1184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 0.0649   |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 1318     |\n",
      "|    total_timesteps    | 1192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | 0.00844  |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1199616, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3758.60 +/- 310.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.76e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1199616  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14995    |\n",
      "|    policy_loss        | 0.0275   |\n",
      "|    value_loss         | 0.00921  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 15000    |\n",
      "|    time_elapsed       | 1332     |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 0.0422   |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 15100    |\n",
      "|    time_elapsed       | 1339     |\n",
      "|    total_timesteps    | 1208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15099    |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 1345     |\n",
      "|    total_timesteps    | 1216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.00661  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 905      |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 1352     |\n",
      "|    total_timesteps    | 1224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | 0.12     |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1224608, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3705.20 +/- 368.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.71e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1224608  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15307    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 15400    |\n",
      "|    time_elapsed       | 1365     |\n",
      "|    total_timesteps    | 1232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | -0.0439  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 1372     |\n",
      "|    total_timesteps    | 1240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 15600    |\n",
      "|    time_elapsed       | 1379     |\n",
      "|    total_timesteps    | 1248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | -0.0858  |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1249600, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3793.40 +/- 303.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.79e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1249600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15619    |\n",
      "|    policy_loss        | -0.0897  |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 15700    |\n",
      "|    time_elapsed       | 1392     |\n",
      "|    total_timesteps    | 1256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | -0.0021  |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 15800    |\n",
      "|    time_elapsed       | 1399     |\n",
      "|    total_timesteps    | 1264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | 0.00448  |\n",
      "|    value_loss         | 0.00781  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 15900    |\n",
      "|    time_elapsed       | 1405     |\n",
      "|    total_timesteps    | 1272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | 0.106    |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1274592, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3495.60 +/- 234.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.5e+03  |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1274592  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15932    |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 1418     |\n",
      "|    total_timesteps    | 1280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.0548  |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 1425     |\n",
      "|    total_timesteps    | 1288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | 0.0329   |\n",
      "|    value_loss         | 0.00743  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 1432     |\n",
      "|    total_timesteps    | 1296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.036   |\n",
      "|    value_loss         | 0.00776  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1299584, episode_reward=-19.80 +/- 1.47\n",
      "Episode length: 3987.00 +/- 678.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.99e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1299584  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16244    |\n",
      "|    policy_loss        | 0.0501   |\n",
      "|    value_loss         | 0.00906  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 1446     |\n",
      "|    total_timesteps    | 1304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.00891  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 1452     |\n",
      "|    total_timesteps    | 1312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 1459     |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | 0.0376   |\n",
      "|    value_loss         | 0.00971  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1324576, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 4210.20 +/- 532.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.21e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1324576  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16557    |\n",
      "|    policy_loss        | 0.0691   |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 16600    |\n",
      "|    time_elapsed       | 1473     |\n",
      "|    total_timesteps    | 1328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 1480     |\n",
      "|    total_timesteps    | 1336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.00896  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 16800    |\n",
      "|    time_elapsed       | 1487     |\n",
      "|    total_timesteps    | 1344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16799    |\n",
      "|    policy_loss        | -0.00333 |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1349568, episode_reward=-19.80 +/- 0.40\n",
      "Episode length: 3742.80 +/- 64.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.74e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1349568  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16869    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 1500     |\n",
      "|    total_timesteps    | 1352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | 0.0528   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 17000    |\n",
      "|    time_elapsed       | 1507     |\n",
      "|    total_timesteps    | 1360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1513     |\n",
      "|    total_timesteps    | 1368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.659    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -0.00054 |\n",
      "|    value_loss         | 0.0839   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1374560, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3900.80 +/- 387.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.9e+03  |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1374560  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17181    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 1527     |\n",
      "|    total_timesteps    | 1376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.00883  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 1534     |\n",
      "|    total_timesteps    | 1384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -0.0622  |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 17400    |\n",
      "|    time_elapsed       | 1540     |\n",
      "|    total_timesteps    | 1392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | -0.0673  |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1399552, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3870.00 +/- 471.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.87e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1399552  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17494    |\n",
      "|    policy_loss        | -0.0324  |\n",
      "|    value_loss         | 0.00707  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 17500    |\n",
      "|    time_elapsed       | 1554     |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | -0.0327  |\n",
      "|    value_loss         | 0.00872  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 17600    |\n",
      "|    time_elapsed       | 1561     |\n",
      "|    total_timesteps    | 1408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | 0.115    |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.84e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1567     |\n",
      "|    total_timesteps    | 1416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | 0.00209  |\n",
      "|    value_loss         | 0.00758  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.83e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 1574     |\n",
      "|    total_timesteps    | 1424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 0.0589   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1424544, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3694.60 +/- 150.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.69e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1424544  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17806    |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.84e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 1587     |\n",
      "|    total_timesteps    | 1432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | 0.0416   |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 18000    |\n",
      "|    time_elapsed       | 1594     |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17999    |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 18100    |\n",
      "|    time_elapsed       | 1601     |\n",
      "|    total_timesteps    | 1448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18099    |\n",
      "|    policy_loss        | -0.0692  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1449536, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3748.80 +/- 308.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.75e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1449536  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18119    |\n",
      "|    policy_loss        | 0.099    |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 1614     |\n",
      "|    total_timesteps    | 1456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 18300    |\n",
      "|    time_elapsed       | 1621     |\n",
      "|    total_timesteps    | 1464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.0076   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 1628     |\n",
      "|    total_timesteps    | 1472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | 0.0468   |\n",
      "|    value_loss         | 0.0257   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1474528, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3646.40 +/- 339.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.65e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1474528  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18431    |\n",
      "|    policy_loss        | 0.0689   |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.77e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 18500    |\n",
      "|    time_elapsed       | 1641     |\n",
      "|    total_timesteps    | 1480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.00886  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 1648     |\n",
      "|    total_timesteps    | 1488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 1654     |\n",
      "|    total_timesteps    | 1496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | -0.0111  |\n",
      "|    value_loss         | 0.0456   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1499520, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3585.40 +/- 390.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.59e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1499520  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18743    |\n",
      "|    policy_loss        | 0.00618  |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 1667     |\n",
      "|    total_timesteps    | 1504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | -0.0547  |\n",
      "|    value_loss         | 0.0083   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 18900    |\n",
      "|    time_elapsed       | 1674     |\n",
      "|    total_timesteps    | 1512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | 0.0454   |\n",
      "|    value_loss         | 0.047    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 19000    |\n",
      "|    time_elapsed       | 1681     |\n",
      "|    total_timesteps    | 1520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | -0.0158  |\n",
      "|    value_loss         | 0.0091   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1524512, episode_reward=-20.20 +/- 0.98\n",
      "Episode length: 3912.00 +/- 448.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.91e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1524512  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19056    |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 19100    |\n",
      "|    time_elapsed       | 1694     |\n",
      "|    total_timesteps    | 1528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | -0.0437  |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 1701     |\n",
      "|    total_timesteps    | 1536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 0.0293   |\n",
      "|    value_loss         | 0.0623   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.72e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 1708     |\n",
      "|    total_timesteps    | 1544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | 0.0524   |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1549504, episode_reward=-19.80 +/- 1.17\n",
      "Episode length: 3767.60 +/- 334.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.77e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1549504  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19368    |\n",
      "|    policy_loss        | -0.00706 |\n",
      "|    value_loss         | 0.00647  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.73e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 1721     |\n",
      "|    total_timesteps    | 1552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.701    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.066   |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 19500    |\n",
      "|    time_elapsed       | 1728     |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | -0.0768  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.74e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 1735     |\n",
      "|    total_timesteps    | 1568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 0.0215   |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1574496, episode_reward=-19.80 +/- 1.17\n",
      "Episode length: 3809.80 +/- 218.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.81e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1574496  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19681    |\n",
      "|    policy_loss        | 0.0237   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 1748     |\n",
      "|    total_timesteps    | 1576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -0.0517  |\n",
      "|    value_loss         | 0.00997  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 19800    |\n",
      "|    time_elapsed       | 1755     |\n",
      "|    total_timesteps    | 1584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.76e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 1762     |\n",
      "|    total_timesteps    | 1592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.00972  |\n",
      "|    value_loss         | 0.00642  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1599488, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4148.00 +/- 343.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.15e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1599488  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19993    |\n",
      "|    policy_loss        | -0.00498 |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.75e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 20000    |\n",
      "|    time_elapsed       | 1776     |\n",
      "|    total_timesteps    | 1600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | 0.002    |\n",
      "|    value_loss         | 0.00644  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.78e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 20100    |\n",
      "|    time_elapsed       | 1782     |\n",
      "|    total_timesteps    | 1608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | 0.00177  |\n",
      "|    value_loss         | 0.00915  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 20200    |\n",
      "|    time_elapsed       | 1789     |\n",
      "|    total_timesteps    | 1616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | -0.0287  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 20300    |\n",
      "|    time_elapsed       | 1796     |\n",
      "|    total_timesteps    | 1624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1624480, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3686.40 +/- 229.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.69e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1624480  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20305    |\n",
      "|    policy_loss        | 0.0445   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.79e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 20400    |\n",
      "|    time_elapsed       | 1809     |\n",
      "|    total_timesteps    | 1632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | 0.0661   |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 20500    |\n",
      "|    time_elapsed       | 1816     |\n",
      "|    total_timesteps    | 1640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | 0.00962  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 20600    |\n",
      "|    time_elapsed       | 1823     |\n",
      "|    total_timesteps    | 1648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | -0.00089 |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1649472, episode_reward=-20.80 +/- 0.40\n",
      "Episode length: 3590.80 +/- 421.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.59e+03 |\n",
      "|    mean_reward        | -20.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1649472  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20618    |\n",
      "|    policy_loss        | 0.0358   |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 20700    |\n",
      "|    time_elapsed       | 1836     |\n",
      "|    total_timesteps    | 1656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | 0.00014  |\n",
      "|    value_loss         | 0.00595  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.81e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 20800    |\n",
      "|    time_elapsed       | 1842     |\n",
      "|    total_timesteps    | 1664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -0.0525  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 20900    |\n",
      "|    time_elapsed       | 1849     |\n",
      "|    total_timesteps    | 1672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | -0.0847  |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1674464, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3645.20 +/- 453.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.65e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1674464  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20930    |\n",
      "|    policy_loss        | -0.0375  |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.8e+03  |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 21000    |\n",
      "|    time_elapsed       | 1862     |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | 0.0337   |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 21100    |\n",
      "|    time_elapsed       | 1869     |\n",
      "|    total_timesteps    | 1688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.00869  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 21200    |\n",
      "|    time_elapsed       | 1876     |\n",
      "|    total_timesteps    | 1696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1699456, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3659.60 +/- 429.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.66e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1699456  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21243    |\n",
      "|    policy_loss        | 0.00786  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.82e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 21300    |\n",
      "|    time_elapsed       | 1889     |\n",
      "|    total_timesteps    | 1704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 21400    |\n",
      "|    time_elapsed       | 1896     |\n",
      "|    total_timesteps    | 1712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | -0.033   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 21500    |\n",
      "|    time_elapsed       | 1902     |\n",
      "|    total_timesteps    | 1720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | 0.0837   |\n",
      "|    value_loss         | 0.0405   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1724448, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3852.80 +/- 258.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.85e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1724448  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21555    |\n",
      "|    policy_loss        | 0.0971   |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.87e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 21600    |\n",
      "|    time_elapsed       | 1916     |\n",
      "|    total_timesteps    | 1728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | 0.0177   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.87e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 21700    |\n",
      "|    time_elapsed       | 1923     |\n",
      "|    total_timesteps    | 1736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | -0.00353 |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.85e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 21800    |\n",
      "|    time_elapsed       | 1929     |\n",
      "|    total_timesteps    | 1744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 0.00984  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1749440, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3676.60 +/- 473.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.68e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1749440  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21867    |\n",
      "|    policy_loss        | -0.0441  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.86e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 21900    |\n",
      "|    time_elapsed       | 1943     |\n",
      "|    total_timesteps    | 1752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | 0.0389   |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.87e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 22000    |\n",
      "|    time_elapsed       | 1949     |\n",
      "|    total_timesteps    | 1760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0218   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.88e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 22100    |\n",
      "|    time_elapsed       | 1956     |\n",
      "|    total_timesteps    | 1768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | -0.114   |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1774432, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 3865.80 +/- 500.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.87e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1774432  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22180    |\n",
      "|    policy_loss        | 0.0818   |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.88e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 22200    |\n",
      "|    time_elapsed       | 1970     |\n",
      "|    total_timesteps    | 1776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | -0.00601 |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 22300    |\n",
      "|    time_elapsed       | 1976     |\n",
      "|    total_timesteps    | 1784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22299    |\n",
      "|    policy_loss        | 0.121    |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 22400    |\n",
      "|    time_elapsed       | 1983     |\n",
      "|    total_timesteps    | 1792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 0.0483   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1799424, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3422.20 +/- 195.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.42e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1799424  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.61     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22492    |\n",
      "|    policy_loss        | 0.00273  |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.88e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 22500    |\n",
      "|    time_elapsed       | 1996     |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | 0.00744  |\n",
      "|    value_loss         | 0.00851  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.86e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 22600    |\n",
      "|    time_elapsed       | 2003     |\n",
      "|    total_timesteps    | 1808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 22700    |\n",
      "|    time_elapsed       | 2009     |\n",
      "|    total_timesteps    | 1816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.0554   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 22800    |\n",
      "|    time_elapsed       | 2016     |\n",
      "|    total_timesteps    | 1824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22799    |\n",
      "|    policy_loss        | 0.0346   |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1824416, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3615.60 +/- 354.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.62e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1824416  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22805    |\n",
      "|    policy_loss        | 0.029    |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 22900    |\n",
      "|    time_elapsed       | 2030     |\n",
      "|    total_timesteps    | 1832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 23000    |\n",
      "|    time_elapsed       | 2036     |\n",
      "|    total_timesteps    | 1840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | 0.0479   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.94e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 904      |\n",
      "|    iterations         | 23100    |\n",
      "|    time_elapsed       | 2043     |\n",
      "|    total_timesteps    | 1848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23099    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1849408, episode_reward=-19.40 +/- 1.20\n",
      "Episode length: 4074.60 +/- 535.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.07e+03 |\n",
      "|    mean_reward        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1849408  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23117    |\n",
      "|    policy_loss        | 0.0713   |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.95e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 23200    |\n",
      "|    time_elapsed       | 2057     |\n",
      "|    total_timesteps    | 1856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.94e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 23300    |\n",
      "|    time_elapsed       | 2064     |\n",
      "|    total_timesteps    | 1864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | -0.00744 |\n",
      "|    value_loss         | 0.00732  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 23400    |\n",
      "|    time_elapsed       | 2071     |\n",
      "|    total_timesteps    | 1872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1874400, episode_reward=-20.40 +/- 0.49\n",
      "Episode length: 3576.00 +/- 248.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.58e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1874400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23429    |\n",
      "|    policy_loss        | -0.122   |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 23500    |\n",
      "|    time_elapsed       | 2084     |\n",
      "|    total_timesteps    | 1880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.00735  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 23600    |\n",
      "|    time_elapsed       | 2091     |\n",
      "|    total_timesteps    | 1888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.0083   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.91e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 23700    |\n",
      "|    time_elapsed       | 2097     |\n",
      "|    total_timesteps    | 1896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | -0.0407  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1899392, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3862.40 +/- 422.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1899392  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23742    |\n",
      "|    policy_loss        | -0.029   |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 23800    |\n",
      "|    time_elapsed       | 2111     |\n",
      "|    total_timesteps    | 1904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 23900    |\n",
      "|    time_elapsed       | 2118     |\n",
      "|    total_timesteps    | 1912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | -0.0426  |\n",
      "|    value_loss         | 0.00524  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.88e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 24000    |\n",
      "|    time_elapsed       | 2124     |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | -0.0833  |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1924384, episode_reward=-20.40 +/- 0.80\n",
      "Episode length: 4070.20 +/- 193.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.07e+03 |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1924384  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24054    |\n",
      "|    policy_loss        | -0.0928  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.87e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 24100    |\n",
      "|    time_elapsed       | 2139     |\n",
      "|    total_timesteps    | 1928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.0198   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 24200    |\n",
      "|    time_elapsed       | 2145     |\n",
      "|    total_timesteps    | 1936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | 0.0492   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.9e+03  |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 24300    |\n",
      "|    time_elapsed       | 2152     |\n",
      "|    total_timesteps    | 1944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | 0.0197   |\n",
      "|    value_loss         | 0.00855  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1949376, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 3874.80 +/- 346.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.87e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1949376  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24367    |\n",
      "|    policy_loss        | -0.00583 |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.93e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 24400    |\n",
      "|    time_elapsed       | 2166     |\n",
      "|    total_timesteps    | 1952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.95e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 24500    |\n",
      "|    time_elapsed       | 2172     |\n",
      "|    total_timesteps    | 1960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | -0.0985  |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 24600    |\n",
      "|    time_elapsed       | 2179     |\n",
      "|    total_timesteps    | 1968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24599    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1974368, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 3831.60 +/- 420.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.83e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1974368  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24679    |\n",
      "|    policy_loss        | 0.0472   |\n",
      "|    value_loss         | 0.00905  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 24700    |\n",
      "|    time_elapsed       | 2193     |\n",
      "|    total_timesteps    | 1976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24699    |\n",
      "|    policy_loss        | -0.0291  |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 24800    |\n",
      "|    time_elapsed       | 2199     |\n",
      "|    total_timesteps    | 1984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24799    |\n",
      "|    policy_loss        | 0.0281   |\n",
      "|    value_loss         | 0.00996  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 24900    |\n",
      "|    time_elapsed       | 2206     |\n",
      "|    total_timesteps    | 1992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | 0.00823  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1999360, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3924.40 +/- 590.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.92e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1999360  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24991    |\n",
      "|    policy_loss        | -0.0786  |\n",
      "|    value_loss         | 0.0384   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.04e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 25000    |\n",
      "|    time_elapsed       | 2220     |\n",
      "|    total_timesteps    | 2000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.02e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 25100    |\n",
      "|    time_elapsed       | 2227     |\n",
      "|    total_timesteps    | 2008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25099    |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    value_loss         | 0.00701  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 25200    |\n",
      "|    time_elapsed       | 2233     |\n",
      "|    total_timesteps    | 2016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | 0.0786   |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 903      |\n",
      "|    iterations         | 25300    |\n",
      "|    time_elapsed       | 2240     |\n",
      "|    total_timesteps    | 2024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2024352, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 3950.40 +/- 265.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.95e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2024352  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25304    |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 25400    |\n",
      "|    time_elapsed       | 2254     |\n",
      "|    total_timesteps    | 2032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 0.1      |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 25500    |\n",
      "|    time_elapsed       | 2261     |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25499    |\n",
      "|    policy_loss        | -0.0612  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 3.97e+03  |\n",
      "|    ep_rew_mean        | -19.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 903       |\n",
      "|    iterations         | 25600     |\n",
      "|    time_elapsed       | 2267      |\n",
      "|    total_timesteps    | 2048000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | 0.913     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25599     |\n",
      "|    policy_loss        | -0.000521 |\n",
      "|    value_loss         | 0.0501    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=2049344, episode_reward=-20.20 +/- 1.17\n",
      "Episode length: 3919.00 +/- 534.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.92e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2049344  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25616    |\n",
      "|    policy_loss        | 0.0596   |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.97e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 25700    |\n",
      "|    time_elapsed       | 2281     |\n",
      "|    total_timesteps    | 2056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | 0.0552   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.97e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 25800    |\n",
      "|    time_elapsed       | 2288     |\n",
      "|    total_timesteps    | 2064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | 0.0573   |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.97e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 25900    |\n",
      "|    time_elapsed       | 2295     |\n",
      "|    total_timesteps    | 2072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2074336, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3608.40 +/- 247.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.61e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2074336  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25929    |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 26000    |\n",
      "|    time_elapsed       | 2308     |\n",
      "|    total_timesteps    | 2080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25999    |\n",
      "|    policy_loss        | -0.0363  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 26100    |\n",
      "|    time_elapsed       | 2314     |\n",
      "|    total_timesteps    | 2088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | -0.0615  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 26200    |\n",
      "|    time_elapsed       | 2321     |\n",
      "|    total_timesteps    | 2096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26199    |\n",
      "|    policy_loss        | 0.00173  |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2099328, episode_reward=-20.60 +/- 0.80\n",
      "Episode length: 3784.60 +/- 271.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.78e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2099328  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26241    |\n",
      "|    policy_loss        | -0.00573 |\n",
      "|    value_loss         | 0.00964  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 26300    |\n",
      "|    time_elapsed       | 2335     |\n",
      "|    total_timesteps    | 2104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | -0.0587  |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.02e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 26400    |\n",
      "|    time_elapsed       | 2341     |\n",
      "|    total_timesteps    | 2112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26399    |\n",
      "|    policy_loss        | -0.106   |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.02e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 26500    |\n",
      "|    time_elapsed       | 2348     |\n",
      "|    total_timesteps    | 2120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | 0.0212   |\n",
      "|    value_loss         | 0.00441  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2124320, episode_reward=-20.00 +/- 1.55\n",
      "Episode length: 4176.20 +/- 797.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.18e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2124320  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26553    |\n",
      "|    policy_loss        | 0.0638   |\n",
      "|    value_loss         | 0.0372   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.05e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 26600    |\n",
      "|    time_elapsed       | 2362     |\n",
      "|    total_timesteps    | 2128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | 0.148    |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 26700    |\n",
      "|    time_elapsed       | 2369     |\n",
      "|    total_timesteps    | 2136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | -0.0817  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 26800    |\n",
      "|    time_elapsed       | 2376     |\n",
      "|    total_timesteps    | 2144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | 0.0662   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2149312, episode_reward=-20.40 +/- 1.20\n",
      "Episode length: 4004.40 +/- 438.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4e+03    |\n",
      "|    mean_reward        | -20.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2149312  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26866    |\n",
      "|    policy_loss        | -0.0529  |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 26900    |\n",
      "|    time_elapsed       | 2389     |\n",
      "|    total_timesteps    | 2152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | 0.0113   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.03e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 27000    |\n",
      "|    time_elapsed       | 2396     |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | -0.0595  |\n",
      "|    value_loss         | 0.00879  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.04e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 27100    |\n",
      "|    time_elapsed       | 2403     |\n",
      "|    total_timesteps    | 2168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | 0.0971   |\n",
      "|    value_loss         | 0.0304   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2174304, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 3743.60 +/- 372.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.74e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2174304  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27178    |\n",
      "|    policy_loss        | -0.0352  |\n",
      "|    value_loss         | 0.0403   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.03e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 27200    |\n",
      "|    time_elapsed       | 2416     |\n",
      "|    total_timesteps    | 2176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 27300    |\n",
      "|    time_elapsed       | 2423     |\n",
      "|    total_timesteps    | 2184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | -0.0483  |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 27400    |\n",
      "|    time_elapsed       | 2430     |\n",
      "|    total_timesteps    | 2192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | -0.009   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2199296, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 4047.00 +/- 532.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.05e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2199296  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27491    |\n",
      "|    policy_loss        | -0.0659  |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 27500    |\n",
      "|    time_elapsed       | 2444     |\n",
      "|    total_timesteps    | 2200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.666    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | -0.0699  |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.99e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 27600    |\n",
      "|    time_elapsed       | 2450     |\n",
      "|    total_timesteps    | 2208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | -0.0148  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 27700    |\n",
      "|    time_elapsed       | 2457     |\n",
      "|    total_timesteps    | 2216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27699    |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.96e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 27800    |\n",
      "|    time_elapsed       | 2464     |\n",
      "|    total_timesteps    | 2224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.00591  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2224288, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 4229.20 +/- 250.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.23e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2224288  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27803    |\n",
      "|    policy_loss        | -0.0972  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.97e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 27900    |\n",
      "|    time_elapsed       | 2478     |\n",
      "|    total_timesteps    | 2232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27899    |\n",
      "|    policy_loss        | 0.0486   |\n",
      "|    value_loss         | 0.00925  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.96e+03 |\n",
      "|    ep_rew_mean        | -20.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 28000    |\n",
      "|    time_elapsed       | 2485     |\n",
      "|    total_timesteps    | 2240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27999    |\n",
      "|    policy_loss        | 0.0067   |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 3.98e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 902      |\n",
      "|    iterations         | 28100    |\n",
      "|    time_elapsed       | 2492     |\n",
      "|    total_timesteps    | 2248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | 0.0598   |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2249280, episode_reward=-19.80 +/- 1.17\n",
      "Episode length: 4293.00 +/- 551.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.29e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2249280  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28115    |\n",
      "|    policy_loss        | 0.153    |\n",
      "|    value_loss         | 0.0295   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.01e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 28200    |\n",
      "|    time_elapsed       | 2506     |\n",
      "|    total_timesteps    | 2256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.01e+03 |\n",
      "|    ep_rew_mean        | -20      |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 28300    |\n",
      "|    time_elapsed       | 2513     |\n",
      "|    total_timesteps    | 2264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28299    |\n",
      "|    policy_loss        | -0.0561  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4e+03    |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 28400    |\n",
      "|    time_elapsed       | 2520     |\n",
      "|    total_timesteps    | 2272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28399    |\n",
      "|    policy_loss        | 0.0454   |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2274272, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4279.60 +/- 481.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.28e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2274272  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28428    |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.03e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 28500    |\n",
      "|    time_elapsed       | 2534     |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | 0.0765   |\n",
      "|    value_loss         | 0.0228   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.04e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 28600    |\n",
      "|    time_elapsed       | 2541     |\n",
      "|    total_timesteps    | 2288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | 0.025    |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.03e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 28700    |\n",
      "|    time_elapsed       | 2547     |\n",
      "|    total_timesteps    | 2296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | -0.00985 |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2299264, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 4026.00 +/- 504.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.03e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2299264  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28740    |\n",
      "|    policy_loss        | -0.00726 |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.02e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 28800    |\n",
      "|    time_elapsed       | 2561     |\n",
      "|    total_timesteps    | 2304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.02e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 28900    |\n",
      "|    time_elapsed       | 2568     |\n",
      "|    total_timesteps    | 2312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28899    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 29000    |\n",
      "|    time_elapsed       | 2574     |\n",
      "|    total_timesteps    | 2320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | 0.00313  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2324256, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 3969.20 +/- 262.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.97e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2324256  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29053    |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.08e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 29100    |\n",
      "|    time_elapsed       | 2588     |\n",
      "|    total_timesteps    | 2328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.05e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 29200    |\n",
      "|    time_elapsed       | 2594     |\n",
      "|    total_timesteps    | 2336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.07e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 29300    |\n",
      "|    time_elapsed       | 2601     |\n",
      "|    total_timesteps    | 2344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2349248, episode_reward=-20.00 +/- 0.63\n",
      "Episode length: 3670.80 +/- 288.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.67e+03 |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2349248  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29365    |\n",
      "|    policy_loss        | -0.0529  |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.09e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 29400    |\n",
      "|    time_elapsed       | 2614     |\n",
      "|    total_timesteps    | 2352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | -0.0278  |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.08e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 29500    |\n",
      "|    time_elapsed       | 2621     |\n",
      "|    total_timesteps    | 2360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.09e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 29600    |\n",
      "|    time_elapsed       | 2628     |\n",
      "|    total_timesteps    | 2368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | -0.00121 |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2374240, episode_reward=-19.00 +/- 0.63\n",
      "Episode length: 4495.40 +/- 397.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.5e+03  |\n",
      "|    mean_reward        | -19      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2374240  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29677    |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.08e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 29700    |\n",
      "|    time_elapsed       | 2643     |\n",
      "|    total_timesteps    | 2376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.09e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 29800    |\n",
      "|    time_elapsed       | 2649     |\n",
      "|    total_timesteps    | 2384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.0262   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.11e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 29900    |\n",
      "|    time_elapsed       | 2656     |\n",
      "|    total_timesteps    | 2392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | 0.00233  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2399232, episode_reward=-19.60 +/- 0.80\n",
      "Episode length: 3717.60 +/- 259.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.72e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2399232  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29990    |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    value_loss         | 0.00812  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.12e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 30000    |\n",
      "|    time_elapsed       | 2669     |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | 0.00339  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.12e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 30100    |\n",
      "|    time_elapsed       | 2676     |\n",
      "|    total_timesteps    | 2408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30099    |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.11e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 30200    |\n",
      "|    time_elapsed       | 2683     |\n",
      "|    total_timesteps    | 2416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30199    |\n",
      "|    policy_loss        | -0.0655  |\n",
      "|    value_loss         | 0.00876  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.09e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 901      |\n",
      "|    iterations         | 30300    |\n",
      "|    time_elapsed       | 2690     |\n",
      "|    total_timesteps    | 2424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30299    |\n",
      "|    policy_loss        | 0.0193   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2424224, episode_reward=-19.60 +/- 0.49\n",
      "Episode length: 4359.80 +/- 414.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.36e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2424224  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30302    |\n",
      "|    policy_loss        | 0.043    |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.1e+03  |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 30400    |\n",
      "|    time_elapsed       | 2704     |\n",
      "|    total_timesteps    | 2432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30399    |\n",
      "|    policy_loss        | -0.132   |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.09e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 30500    |\n",
      "|    time_elapsed       | 2711     |\n",
      "|    total_timesteps    | 2440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30499    |\n",
      "|    policy_loss        | 0.0414   |\n",
      "|    value_loss         | 0.0365   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.13e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 30600    |\n",
      "|    time_elapsed       | 2718     |\n",
      "|    total_timesteps    | 2448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30599    |\n",
      "|    policy_loss        | 0.0438   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2449216, episode_reward=-20.20 +/- 0.75\n",
      "Episode length: 3666.80 +/- 128.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.67e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2449216  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30615    |\n",
      "|    policy_loss        | 0.0938   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.13e+03 |\n",
      "|    ep_rew_mean        | -19.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 30700    |\n",
      "|    time_elapsed       | 2731     |\n",
      "|    total_timesteps    | 2456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30699    |\n",
      "|    policy_loss        | 0.0124   |\n",
      "|    value_loss         | 0.00637  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.13e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 30800    |\n",
      "|    time_elapsed       | 2738     |\n",
      "|    total_timesteps    | 2464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30799    |\n",
      "|    policy_loss        | 0.00491  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.13e+03 |\n",
      "|    ep_rew_mean        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 30900    |\n",
      "|    time_elapsed       | 2744     |\n",
      "|    total_timesteps    | 2472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30899    |\n",
      "|    policy_loss        | 0.0242   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2474208, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 4003.00 +/- 347.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4e+03    |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2474208  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30927    |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.17e+03 |\n",
      "|    ep_rew_mean        | -19.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 31000    |\n",
      "|    time_elapsed       | 2758     |\n",
      "|    total_timesteps    | 2480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30999    |\n",
      "|    policy_loss        | 0.0255   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.18e+03 |\n",
      "|    ep_rew_mean        | -19.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 31100    |\n",
      "|    time_elapsed       | 2765     |\n",
      "|    total_timesteps    | 2488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31099    |\n",
      "|    policy_loss        | 0.0994   |\n",
      "|    value_loss         | 0.0304   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.19e+03 |\n",
      "|    ep_rew_mean        | -19.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 31200    |\n",
      "|    time_elapsed       | 2772     |\n",
      "|    total_timesteps    | 2496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31199    |\n",
      "|    policy_loss        | 0.00853  |\n",
      "|    value_loss         | 0.00957  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2499200, episode_reward=-19.40 +/- 1.36\n",
      "Episode length: 4203.60 +/- 506.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.2e+03  |\n",
      "|    mean_reward        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2499200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31239    |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.18e+03 |\n",
      "|    ep_rew_mean        | -19.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 31300    |\n",
      "|    time_elapsed       | 2786     |\n",
      "|    total_timesteps    | 2504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31299    |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.23e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 31400    |\n",
      "|    time_elapsed       | 2793     |\n",
      "|    total_timesteps    | 2512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | 0.0619   |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.24e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 900      |\n",
      "|    iterations         | 31500    |\n",
      "|    time_elapsed       | 2799     |\n",
      "|    total_timesteps    | 2520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31499    |\n",
      "|    policy_loss        | 0.0273   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2524192, episode_reward=-20.20 +/- 0.40\n",
      "Episode length: 3887.60 +/- 232.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.89e+03 |\n",
      "|    mean_reward        | -20.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2524192  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.713    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31552    |\n",
      "|    policy_loss        | 0.0762   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.21e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 31600    |\n",
      "|    time_elapsed       | 2813     |\n",
      "|    total_timesteps    | 2528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31599    |\n",
      "|    policy_loss        | 0.0506   |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.2e+03  |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 31700    |\n",
      "|    time_elapsed       | 2820     |\n",
      "|    total_timesteps    | 2536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31699    |\n",
      "|    policy_loss        | 0.041    |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.2e+03  |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 31800    |\n",
      "|    time_elapsed       | 2826     |\n",
      "|    total_timesteps    | 2544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31799    |\n",
      "|    policy_loss        | 0.0544   |\n",
      "|    value_loss         | 0.0483   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2549184, episode_reward=-20.00 +/- 0.89\n",
      "Episode length: 4200.80 +/- 309.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.2e+03  |\n",
      "|    mean_reward        | -20      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2549184  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31864    |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.19e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 31900    |\n",
      "|    time_elapsed       | 2841     |\n",
      "|    total_timesteps    | 2552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31899    |\n",
      "|    policy_loss        | -0.067   |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.19e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 32000    |\n",
      "|    time_elapsed       | 2848     |\n",
      "|    total_timesteps    | 2560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31999    |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.0428   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.19e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 32100    |\n",
      "|    time_elapsed       | 2854     |\n",
      "|    total_timesteps    | 2568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32099    |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2574176, episode_reward=-19.80 +/- 1.47\n",
      "Episode length: 4159.40 +/- 427.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 4.16e+03  |\n",
      "|    mean_reward        | -19.8     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 2574176   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | 0.9       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32177     |\n",
      "|    policy_loss        | -0.000926 |\n",
      "|    value_loss         | 0.0266    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.2e+03  |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 32200    |\n",
      "|    time_elapsed       | 2869     |\n",
      "|    total_timesteps    | 2576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32199    |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.23e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 32300    |\n",
      "|    time_elapsed       | 2875     |\n",
      "|    total_timesteps    | 2584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32299    |\n",
      "|    policy_loss        | -0.043   |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.21e+03 |\n",
      "|    ep_rew_mean        | -19.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 32400    |\n",
      "|    time_elapsed       | 2882     |\n",
      "|    total_timesteps    | 2592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32399    |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2599168, episode_reward=-19.80 +/- 0.40\n",
      "Episode length: 4384.60 +/- 320.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.38e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2599168  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32489    |\n",
      "|    policy_loss        | -0.0712  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.25e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 32500    |\n",
      "|    time_elapsed       | 2897     |\n",
      "|    total_timesteps    | 2600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32499    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.27e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 32600    |\n",
      "|    time_elapsed       | 2903     |\n",
      "|    total_timesteps    | 2608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32599    |\n",
      "|    policy_loss        | -0.0647  |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.26e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 32700    |\n",
      "|    time_elapsed       | 2910     |\n",
      "|    total_timesteps    | 2616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32699    |\n",
      "|    policy_loss        | 0.00328  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.25e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 32800    |\n",
      "|    time_elapsed       | 2916     |\n",
      "|    total_timesteps    | 2624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32799    |\n",
      "|    policy_loss        | 0.0395   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2624160, episode_reward=-19.20 +/- 0.75\n",
      "Episode length: 4505.80 +/- 266.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.51e+03 |\n",
      "|    mean_reward        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2624160  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32801    |\n",
      "|    policy_loss        | -0.0471  |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.31e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 32900    |\n",
      "|    time_elapsed       | 2931     |\n",
      "|    total_timesteps    | 2632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32899    |\n",
      "|    policy_loss        | 0.0741   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.33e+03 |\n",
      "|    ep_rew_mean        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 33000    |\n",
      "|    time_elapsed       | 2938     |\n",
      "|    total_timesteps    | 2640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32999    |\n",
      "|    policy_loss        | 0.0355   |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.37e+03 |\n",
      "|    ep_rew_mean        | -19.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 33100    |\n",
      "|    time_elapsed       | 2945     |\n",
      "|    total_timesteps    | 2648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33099    |\n",
      "|    policy_loss        | 0.125    |\n",
      "|    value_loss         | 0.0684   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2649152, episode_reward=-19.00 +/- 0.89\n",
      "Episode length: 4723.80 +/- 747.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.72e+03 |\n",
      "|    mean_reward        | -19      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2649152  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.674    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33114    |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.35e+03 |\n",
      "|    ep_rew_mean        | -19.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 33200    |\n",
      "|    time_elapsed       | 2960     |\n",
      "|    total_timesteps    | 2656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33199    |\n",
      "|    policy_loss        | 0.0224   |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.41e+03 |\n",
      "|    ep_rew_mean        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 33300    |\n",
      "|    time_elapsed       | 2967     |\n",
      "|    total_timesteps    | 2664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33299    |\n",
      "|    policy_loss        | -0.0329  |\n",
      "|    value_loss         | 0.00941  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.44e+03 |\n",
      "|    ep_rew_mean        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 33400    |\n",
      "|    time_elapsed       | 2974     |\n",
      "|    total_timesteps    | 2672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.599    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33399    |\n",
      "|    policy_loss        | -0.023   |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2674144, episode_reward=-19.80 +/- 0.98\n",
      "Episode length: 4199.00 +/- 437.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.2e+03  |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2674144  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33426    |\n",
      "|    policy_loss        | 0.00895  |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.46e+03 |\n",
      "|    ep_rew_mean        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 33500    |\n",
      "|    time_elapsed       | 2988     |\n",
      "|    total_timesteps    | 2680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33499    |\n",
      "|    policy_loss        | -0.0344  |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.5e+03  |\n",
      "|    ep_rew_mean        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 33600    |\n",
      "|    time_elapsed       | 2995     |\n",
      "|    total_timesteps    | 2688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33599    |\n",
      "|    policy_loss        | 0.106    |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.5e+03  |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 898      |\n",
      "|    iterations         | 33700    |\n",
      "|    time_elapsed       | 3002     |\n",
      "|    total_timesteps    | 2696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33699    |\n",
      "|    policy_loss        | -0.0788  |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2699136, episode_reward=-20.60 +/- 0.49\n",
      "Episode length: 4259.60 +/- 330.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.26e+03 |\n",
      "|    mean_reward        | -20.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2699136  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33739    |\n",
      "|    policy_loss        | -0.00777 |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.48e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 33800    |\n",
      "|    time_elapsed       | 3016     |\n",
      "|    total_timesteps    | 2704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33799    |\n",
      "|    policy_loss        | 0.0468   |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.48e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 33900    |\n",
      "|    time_elapsed       | 3023     |\n",
      "|    total_timesteps    | 2712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33899    |\n",
      "|    policy_loss        | -0.0443  |\n",
      "|    value_loss         | 0.00837  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.48e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 34000    |\n",
      "|    time_elapsed       | 3030     |\n",
      "|    total_timesteps    | 2720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.802    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | 0.000416 |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2724128, episode_reward=-19.60 +/- 1.50\n",
      "Episode length: 4731.60 +/- 833.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.73e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2724128  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34051    |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.46e+03 |\n",
      "|    ep_rew_mean        | -19.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 34100    |\n",
      "|    time_elapsed       | 3045     |\n",
      "|    total_timesteps    | 2728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34099    |\n",
      "|    policy_loss        | 0.0423   |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.49e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 34200    |\n",
      "|    time_elapsed       | 3052     |\n",
      "|    total_timesteps    | 2736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | 0.0397   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.47e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 897      |\n",
      "|    iterations         | 34300    |\n",
      "|    time_elapsed       | 3058     |\n",
      "|    total_timesteps    | 2744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34299    |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2749120, episode_reward=-19.00 +/- 1.26\n",
      "Episode length: 4641.40 +/- 254.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.64e+03 |\n",
      "|    mean_reward        | -19      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2749120  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34363    |\n",
      "|    policy_loss        | -0.0033  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.49e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 34400    |\n",
      "|    time_elapsed       | 3074     |\n",
      "|    total_timesteps    | 2752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34399    |\n",
      "|    policy_loss        | 0.0712   |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.53e+03 |\n",
      "|    ep_rew_mean        | -19.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 34500    |\n",
      "|    time_elapsed       | 3080     |\n",
      "|    total_timesteps    | 2760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34499    |\n",
      "|    policy_loss        | 0.124    |\n",
      "|    value_loss         | 0.0484   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.55e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 34600    |\n",
      "|    time_elapsed       | 3087     |\n",
      "|    total_timesteps    | 2768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34599    |\n",
      "|    policy_loss        | -0.0975  |\n",
      "|    value_loss         | 0.046    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2774112, episode_reward=-19.80 +/- 0.98\n",
      "Episode length: 4437.60 +/- 728.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.44e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2774112  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34676    |\n",
      "|    policy_loss        | -0.0636  |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.54e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 34700    |\n",
      "|    time_elapsed       | 3102     |\n",
      "|    total_timesteps    | 2776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34699    |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.56e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 34800    |\n",
      "|    time_elapsed       | 3109     |\n",
      "|    total_timesteps    | 2784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34799    |\n",
      "|    policy_loss        | -0.0987  |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.54e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 34900    |\n",
      "|    time_elapsed       | 3115     |\n",
      "|    total_timesteps    | 2792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34899    |\n",
      "|    policy_loss        | -0.0201  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2799104, episode_reward=-18.80 +/- 0.98\n",
      "Episode length: 4577.40 +/- 556.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.58e+03 |\n",
      "|    mean_reward        | -18.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2799104  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34988    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.55e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 35000    |\n",
      "|    time_elapsed       | 3131     |\n",
      "|    total_timesteps    | 2800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34999    |\n",
      "|    policy_loss        | -0.0734  |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.53e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 35100    |\n",
      "|    time_elapsed       | 3138     |\n",
      "|    total_timesteps    | 2808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35099    |\n",
      "|    policy_loss        | 0.105    |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.54e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 35200    |\n",
      "|    time_elapsed       | 3144     |\n",
      "|    total_timesteps    | 2816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35199    |\n",
      "|    policy_loss        | 0.00674  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.58e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 35300    |\n",
      "|    time_elapsed       | 3151     |\n",
      "|    total_timesteps    | 2824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35299    |\n",
      "|    policy_loss        | -0.0637  |\n",
      "|    value_loss         | 0.0493   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2824096, episode_reward=-19.80 +/- 0.75\n",
      "Episode length: 3720.40 +/- 384.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.72e+03 |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2824096  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35301    |\n",
      "|    policy_loss        | 0.0072   |\n",
      "|    value_loss         | 0.00923  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.6e+03  |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 35400    |\n",
      "|    time_elapsed       | 3165     |\n",
      "|    total_timesteps    | 2832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35399    |\n",
      "|    policy_loss        | 0.0575   |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.58e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 895      |\n",
      "|    iterations         | 35500    |\n",
      "|    time_elapsed       | 3171     |\n",
      "|    total_timesteps    | 2840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35499    |\n",
      "|    policy_loss        | 0.0773   |\n",
      "|    value_loss         | 0.0664   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.63e+03 |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 896      |\n",
      "|    iterations         | 35600    |\n",
      "|    time_elapsed       | 3178     |\n",
      "|    total_timesteps    | 2848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35599    |\n",
      "|    policy_loss        | 0.0665   |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2849088, episode_reward=-19.60 +/- 1.02\n",
      "Episode length: 4519.00 +/- 497.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.52e+03 |\n",
      "|    mean_reward        | -19.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2849088  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35613    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.66e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 35700    |\n",
      "|    time_elapsed       | 3193     |\n",
      "|    total_timesteps    | 2856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35699    |\n",
      "|    policy_loss        | 0.0091   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.67e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 35800    |\n",
      "|    time_elapsed       | 3200     |\n",
      "|    total_timesteps    | 2864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.637    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35799    |\n",
      "|    policy_loss        | 0.0253   |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 4.71e+03  |\n",
      "|    ep_rew_mean        | -19.1     |\n",
      "| time/                 |           |\n",
      "|    fps                | 895       |\n",
      "|    iterations         | 35900     |\n",
      "|    time_elapsed       | 3206      |\n",
      "|    total_timesteps    | 2872000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | 0.812     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35899     |\n",
      "|    policy_loss        | -0.000227 |\n",
      "|    value_loss         | 0.0101    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=2874080, episode_reward=-19.20 +/- 0.98\n",
      "Episode length: 5251.40 +/- 401.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.25e+03 |\n",
      "|    mean_reward        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2874080  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35925    |\n",
      "|    policy_loss        | -0.00466 |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.7e+03  |\n",
      "|    ep_rew_mean        | -19.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 36000    |\n",
      "|    time_elapsed       | 3223     |\n",
      "|    total_timesteps    | 2880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35999    |\n",
      "|    policy_loss        | 0.0221   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.74e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 36100    |\n",
      "|    time_elapsed       | 3230     |\n",
      "|    total_timesteps    | 2888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | -0.0879  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.79e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 894      |\n",
      "|    iterations         | 36200    |\n",
      "|    time_elapsed       | 3236     |\n",
      "|    total_timesteps    | 2896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36199    |\n",
      "|    policy_loss        | 0.071    |\n",
      "|    value_loss         | 0.0602   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2899072, episode_reward=-18.80 +/- 1.33\n",
      "Episode length: 5256.20 +/- 723.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.26e+03 |\n",
      "|    mean_reward        | -18.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2899072  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36238    |\n",
      "|    policy_loss        | -0.0495  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.79e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 892      |\n",
      "|    iterations         | 36300    |\n",
      "|    time_elapsed       | 3253     |\n",
      "|    total_timesteps    | 2904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36299    |\n",
      "|    policy_loss        | -0.0525  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.81e+03 |\n",
      "|    ep_rew_mean        | -19.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 36400    |\n",
      "|    time_elapsed       | 3259     |\n",
      "|    total_timesteps    | 2912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36399    |\n",
      "|    policy_loss        | -0.0209  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.86e+03 |\n",
      "|    ep_rew_mean        | -19      |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 36500    |\n",
      "|    time_elapsed       | 3266     |\n",
      "|    total_timesteps    | 2920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36499    |\n",
      "|    policy_loss        | -0.0352  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2924064, episode_reward=-19.80 +/- 0.40\n",
      "Episode length: 4201.20 +/- 442.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.2e+03  |\n",
      "|    mean_reward        | -19.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2924064  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36550    |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.94e+03 |\n",
      "|    ep_rew_mean        | -19      |\n",
      "| time/                 |          |\n",
      "|    fps                | 892      |\n",
      "|    iterations         | 36600    |\n",
      "|    time_elapsed       | 3281     |\n",
      "|    total_timesteps    | 2928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 4.96e+03 |\n",
      "|    ep_rew_mean        | -19      |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 36700    |\n",
      "|    time_elapsed       | 3287     |\n",
      "|    total_timesteps    | 2936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36699    |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.04e+03 |\n",
      "|    ep_rew_mean        | -18.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 893      |\n",
      "|    iterations         | 36800    |\n",
      "|    time_elapsed       | 3294     |\n",
      "|    total_timesteps    | 2944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36799    |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2949056, episode_reward=-18.20 +/- 1.60\n",
      "Episode length: 5636.80 +/- 561.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.64e+03 |\n",
      "|    mean_reward        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2949056  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36863    |\n",
      "|    policy_loss        | 0.0286   |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.1e+03  |\n",
      "|    ep_rew_mean        | -18.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 891      |\n",
      "|    iterations         | 36900    |\n",
      "|    time_elapsed       | 3311     |\n",
      "|    total_timesteps    | 2952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36899    |\n",
      "|    policy_loss        | -0.033   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.18e+03 |\n",
      "|    ep_rew_mean        | -18.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 891      |\n",
      "|    iterations         | 37000    |\n",
      "|    time_elapsed       | 3318     |\n",
      "|    total_timesteps    | 2960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36999    |\n",
      "|    policy_loss        | -0.0878  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.22e+03 |\n",
      "|    ep_rew_mean        | -18.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 892      |\n",
      "|    iterations         | 37100    |\n",
      "|    time_elapsed       | 3324     |\n",
      "|    total_timesteps    | 2968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37099    |\n",
      "|    policy_loss        | 0.0282   |\n",
      "|    value_loss         | 0.00789  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2974048, episode_reward=-17.40 +/- 2.42\n",
      "Episode length: 6421.60 +/- 869.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.42e+03 |\n",
      "|    mean_reward        | -17.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2974048  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37175    |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.27e+03 |\n",
      "|    ep_rew_mean        | -18.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 890      |\n",
      "|    iterations         | 37200    |\n",
      "|    time_elapsed       | 3343     |\n",
      "|    total_timesteps    | 2976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37199    |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.33e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 890      |\n",
      "|    iterations         | 37300    |\n",
      "|    time_elapsed       | 3349     |\n",
      "|    total_timesteps    | 2984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37299    |\n",
      "|    policy_loss        | 0.0619   |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.33e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 891      |\n",
      "|    iterations         | 37400    |\n",
      "|    time_elapsed       | 3356     |\n",
      "|    total_timesteps    | 2992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37399    |\n",
      "|    policy_loss        | -0.0313  |\n",
      "|    value_loss         | 0.0265   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2999040, episode_reward=-18.20 +/- 1.72\n",
      "Episode length: 5558.00 +/- 359.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.56e+03 |\n",
      "|    mean_reward        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2999040  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37487    |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.42e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 889      |\n",
      "|    iterations         | 37500    |\n",
      "|    time_elapsed       | 3373     |\n",
      "|    total_timesteps    | 3000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37499    |\n",
      "|    policy_loss        | 0.0355   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.45e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 889      |\n",
      "|    iterations         | 37600    |\n",
      "|    time_elapsed       | 3379     |\n",
      "|    total_timesteps    | 3008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37599    |\n",
      "|    policy_loss        | 0.0266   |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.46e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 890      |\n",
      "|    iterations         | 37700    |\n",
      "|    time_elapsed       | 3386     |\n",
      "|    total_timesteps    | 3016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37699    |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.49e+03 |\n",
      "|    ep_rew_mean        | -18.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 891      |\n",
      "|    iterations         | 37800    |\n",
      "|    time_elapsed       | 3393     |\n",
      "|    total_timesteps    | 3024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37799    |\n",
      "|    policy_loss        | 0.0337   |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3024032, episode_reward=-18.00 +/- 1.41\n",
      "Episode length: 6200.40 +/- 756.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.2e+03  |\n",
      "|    mean_reward        | -18      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3024032  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37800    |\n",
      "|    policy_loss        | 0.0378   |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.55e+03 |\n",
      "|    ep_rew_mean        | -18.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 888      |\n",
      "|    iterations         | 37900    |\n",
      "|    time_elapsed       | 3411     |\n",
      "|    total_timesteps    | 3032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | -0.0611  |\n",
      "|    value_loss         | 0.018    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.57e+03 |\n",
      "|    ep_rew_mean        | -18.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 889      |\n",
      "|    iterations         | 38000    |\n",
      "|    time_elapsed       | 3418     |\n",
      "|    total_timesteps    | 3040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.668    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37999    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.62e+03 |\n",
      "|    ep_rew_mean        | -18.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 889      |\n",
      "|    iterations         | 38100    |\n",
      "|    time_elapsed       | 3424     |\n",
      "|    total_timesteps    | 3048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38099    |\n",
      "|    policy_loss        | -0.0609  |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3049024, episode_reward=-17.20 +/- 1.17\n",
      "Episode length: 5990.40 +/- 420.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | -17.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3049024  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38112    |\n",
      "|    policy_loss        | 0.0125   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.63e+03 |\n",
      "|    ep_rew_mean        | -18.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 887      |\n",
      "|    iterations         | 38200    |\n",
      "|    time_elapsed       | 3442     |\n",
      "|    total_timesteps    | 3056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | -0.0435  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.67e+03 |\n",
      "|    ep_rew_mean        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 888      |\n",
      "|    iterations         | 38300    |\n",
      "|    time_elapsed       | 3449     |\n",
      "|    total_timesteps    | 3064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.761    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38299    |\n",
      "|    policy_loss        | 0.0199   |\n",
      "|    value_loss         | 0.03     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.7e+03  |\n",
      "|    ep_rew_mean        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 888      |\n",
      "|    iterations         | 38400    |\n",
      "|    time_elapsed       | 3455     |\n",
      "|    total_timesteps    | 3072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38399    |\n",
      "|    policy_loss        | 0.0567   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3074016, episode_reward=-17.80 +/- 1.94\n",
      "Episode length: 5828.80 +/- 397.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.83e+03 |\n",
      "|    mean_reward        | -17.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3074016  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38425    |\n",
      "|    policy_loss        | 0.0549   |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.72e+03 |\n",
      "|    ep_rew_mean        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 886      |\n",
      "|    iterations         | 38500    |\n",
      "|    time_elapsed       | 3473     |\n",
      "|    total_timesteps    | 3080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38499    |\n",
      "|    policy_loss        | 0.0244   |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.73e+03 |\n",
      "|    ep_rew_mean        | -18.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 887      |\n",
      "|    iterations         | 38600    |\n",
      "|    time_elapsed       | 3479     |\n",
      "|    total_timesteps    | 3088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38599    |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.78e+03 |\n",
      "|    ep_rew_mean        | -18.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 887      |\n",
      "|    iterations         | 38700    |\n",
      "|    time_elapsed       | 3486     |\n",
      "|    total_timesteps    | 3096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38699    |\n",
      "|    policy_loss        | 0.0921   |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3099008, episode_reward=-16.60 +/- 1.85\n",
      "Episode length: 6792.40 +/- 739.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.79e+03 |\n",
      "|    mean_reward        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3099008  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.465    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38737    |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.81e+03 |\n",
      "|    ep_rew_mean        | -18.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 885      |\n",
      "|    iterations         | 38800    |\n",
      "|    time_elapsed       | 3505     |\n",
      "|    total_timesteps    | 3104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38799    |\n",
      "|    policy_loss        | -0.0151  |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.82e+03 |\n",
      "|    ep_rew_mean        | -18      |\n",
      "| time/                 |          |\n",
      "|    fps                | 885      |\n",
      "|    iterations         | 38900    |\n",
      "|    time_elapsed       | 3512     |\n",
      "|    total_timesteps    | 3112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38899    |\n",
      "|    policy_loss        | 0.048    |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.83e+03 |\n",
      "|    ep_rew_mean        | -18      |\n",
      "| time/                 |          |\n",
      "|    fps                | 886      |\n",
      "|    iterations         | 39000    |\n",
      "|    time_elapsed       | 3519     |\n",
      "|    total_timesteps    | 3120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.745    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38999    |\n",
      "|    policy_loss        | -0.0117  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3124000, episode_reward=-17.60 +/- 1.62\n",
      "Episode length: 5741.40 +/- 339.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.74e+03 |\n",
      "|    mean_reward        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3124000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39049    |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.00862  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.87e+03 |\n",
      "|    ep_rew_mean        | -18      |\n",
      "| time/                 |          |\n",
      "|    fps                | 884      |\n",
      "|    iterations         | 39100    |\n",
      "|    time_elapsed       | 3536     |\n",
      "|    total_timesteps    | 3128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39099    |\n",
      "|    policy_loss        | -0.0433  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.92e+03 |\n",
      "|    ep_rew_mean        | -17.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 885      |\n",
      "|    iterations         | 39200    |\n",
      "|    time_elapsed       | 3543     |\n",
      "|    total_timesteps    | 3136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39199    |\n",
      "|    policy_loss        | -0.0638  |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.95e+03 |\n",
      "|    ep_rew_mean        | -17.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 885      |\n",
      "|    iterations         | 39300    |\n",
      "|    time_elapsed       | 3549     |\n",
      "|    total_timesteps    | 3144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.595    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3148992, episode_reward=-18.00 +/- 1.90\n",
      "Episode length: 5459.20 +/- 229.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.46e+03 |\n",
      "|    mean_reward        | -18      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3148992  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39362    |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.96e+03 |\n",
      "|    ep_rew_mean        | -17.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 883      |\n",
      "|    iterations         | 39400    |\n",
      "|    time_elapsed       | 3566     |\n",
      "|    total_timesteps    | 3152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39399    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0198   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.03e+03 |\n",
      "|    ep_rew_mean        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 884      |\n",
      "|    iterations         | 39500    |\n",
      "|    time_elapsed       | 3573     |\n",
      "|    total_timesteps    | 3160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39499    |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.04e+03 |\n",
      "|    ep_rew_mean        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 884      |\n",
      "|    iterations         | 39600    |\n",
      "|    time_elapsed       | 3579     |\n",
      "|    total_timesteps    | 3168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.577    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39599    |\n",
      "|    policy_loss        | -0.00318 |\n",
      "|    value_loss         | 0.0581   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3173984, episode_reward=-17.20 +/- 2.32\n",
      "Episode length: 6731.20 +/- 646.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.73e+03 |\n",
      "|    mean_reward        | -17.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3173984  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39674    |\n",
      "|    policy_loss        | 0.0261   |\n",
      "|    value_loss         | 0.041    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.13e+03 |\n",
      "|    ep_rew_mean        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 882      |\n",
      "|    iterations         | 39700    |\n",
      "|    time_elapsed       | 3599     |\n",
      "|    total_timesteps    | 3176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.734    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39699    |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.15e+03 |\n",
      "|    ep_rew_mean        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 883      |\n",
      "|    iterations         | 39800    |\n",
      "|    time_elapsed       | 3605     |\n",
      "|    total_timesteps    | 3184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | 0.12     |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.17e+03 |\n",
      "|    ep_rew_mean        | -17.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 883      |\n",
      "|    iterations         | 39900    |\n",
      "|    time_elapsed       | 3612     |\n",
      "|    total_timesteps    | 3192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.721    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39899    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3198976, episode_reward=-17.60 +/- 2.87\n",
      "Episode length: 6763.60 +/- 430.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.76e+03 |\n",
      "|    mean_reward        | -17.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3198976  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39987    |\n",
      "|    policy_loss        | 0.0576   |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.22e+03 |\n",
      "|    ep_rew_mean        | -17.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 881      |\n",
      "|    iterations         | 40000    |\n",
      "|    time_elapsed       | 3631     |\n",
      "|    total_timesteps    | 3200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39999    |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.25e+03 |\n",
      "|    ep_rew_mean        | -17.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 881      |\n",
      "|    iterations         | 40100    |\n",
      "|    time_elapsed       | 3638     |\n",
      "|    total_timesteps    | 3208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40099    |\n",
      "|    policy_loss        | 0.0578   |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.31e+03 |\n",
      "|    ep_rew_mean        | -17.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 882      |\n",
      "|    iterations         | 40200    |\n",
      "|    time_elapsed       | 3644     |\n",
      "|    total_timesteps    | 3216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3223968, episode_reward=-16.80 +/- 4.02\n",
      "Episode length: 7013.80 +/- 1009.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.01e+03 |\n",
      "|    mean_reward        | -16.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3223968  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.561    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | -0.0477  |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.35e+03 |\n",
      "|    ep_rew_mean     | -17.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 879      |\n",
      "|    iterations      | 40300    |\n",
      "|    time_elapsed    | 3664     |\n",
      "|    total_timesteps | 3224000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.37e+03 |\n",
      "|    ep_rew_mean        | -17.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 880      |\n",
      "|    iterations         | 40400    |\n",
      "|    time_elapsed       | 3670     |\n",
      "|    total_timesteps    | 3232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40399    |\n",
      "|    policy_loss        | -0.0588  |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.46e+03 |\n",
      "|    ep_rew_mean        | -17.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 881      |\n",
      "|    iterations         | 40500    |\n",
      "|    time_elapsed       | 3677     |\n",
      "|    total_timesteps    | 3240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40499    |\n",
      "|    policy_loss        | 0.00894  |\n",
      "|    value_loss         | 0.00697  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.49e+03 |\n",
      "|    ep_rew_mean        | -17.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 881      |\n",
      "|    iterations         | 40600    |\n",
      "|    time_elapsed       | 3683     |\n",
      "|    total_timesteps    | 3248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40599    |\n",
      "|    policy_loss        | 0.00629  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3248960, episode_reward=-17.40 +/- 2.42\n",
      "Episode length: 6227.40 +/- 1063.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.23e+03 |\n",
      "|    mean_reward        | -17.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3248960  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40611    |\n",
      "|    policy_loss        | -0.0133  |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.48e+03 |\n",
      "|    ep_rew_mean        | -17.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 879      |\n",
      "|    iterations         | 40700    |\n",
      "|    time_elapsed       | 3701     |\n",
      "|    total_timesteps    | 3256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40699    |\n",
      "|    policy_loss        | -0.0443  |\n",
      "|    value_loss         | 0.0089   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.52e+03 |\n",
      "|    ep_rew_mean        | -17.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 880      |\n",
      "|    iterations         | 40800    |\n",
      "|    time_elapsed       | 3708     |\n",
      "|    total_timesteps    | 3264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40799    |\n",
      "|    policy_loss        | -0.0358  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.57e+03 |\n",
      "|    ep_rew_mean        | -17      |\n",
      "| time/                 |          |\n",
      "|    fps                | 880      |\n",
      "|    iterations         | 40900    |\n",
      "|    time_elapsed       | 3715     |\n",
      "|    total_timesteps    | 3272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40899    |\n",
      "|    policy_loss        | -0.00821 |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3273952, episode_reward=-17.40 +/- 2.42\n",
      "Episode length: 6400.00 +/- 622.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.4e+03  |\n",
      "|    mean_reward        | -17.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3273952  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40924    |\n",
      "|    policy_loss        | -0.15    |\n",
      "|    value_loss         | 0.0481   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.63e+03 |\n",
      "|    ep_rew_mean        | -16.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 878      |\n",
      "|    iterations         | 41000    |\n",
      "|    time_elapsed       | 3733     |\n",
      "|    total_timesteps    | 3280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40999    |\n",
      "|    policy_loss        | 0.0261   |\n",
      "|    value_loss         | 0.00902  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.65e+03 |\n",
      "|    ep_rew_mean        | -16.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 879      |\n",
      "|    iterations         | 41100    |\n",
      "|    time_elapsed       | 3740     |\n",
      "|    total_timesteps    | 3288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41099    |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.72e+03 |\n",
      "|    ep_rew_mean        | -16.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 879      |\n",
      "|    iterations         | 41200    |\n",
      "|    time_elapsed       | 3746     |\n",
      "|    total_timesteps    | 3296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.701    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41199    |\n",
      "|    policy_loss        | -0.009   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3298944, episode_reward=-18.40 +/- 1.02\n",
      "Episode length: 4791.80 +/- 354.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.79e+03 |\n",
      "|    mean_reward        | -18.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3298944  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41236    |\n",
      "|    policy_loss        | -0.198   |\n",
      "|    value_loss         | 0.074    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.76e+03 |\n",
      "|    ep_rew_mean        | -16.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 878      |\n",
      "|    iterations         | 41300    |\n",
      "|    time_elapsed       | 3762     |\n",
      "|    total_timesteps    | 3304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41299    |\n",
      "|    policy_loss        | -0.00285 |\n",
      "|    value_loss         | 0.00814  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.78e+03 |\n",
      "|    ep_rew_mean        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 878      |\n",
      "|    iterations         | 41400    |\n",
      "|    time_elapsed       | 3769     |\n",
      "|    total_timesteps    | 3312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41399    |\n",
      "|    policy_loss        | 0.0356   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.82e+03 |\n",
      "|    ep_rew_mean        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 879      |\n",
      "|    iterations         | 41500    |\n",
      "|    time_elapsed       | 3775     |\n",
      "|    total_timesteps    | 3320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41499    |\n",
      "|    policy_loss        | -0.0713  |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3323936, episode_reward=-16.60 +/- 1.50\n",
      "Episode length: 6738.00 +/- 653.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.74e+03 |\n",
      "|    mean_reward        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3323936  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41549    |\n",
      "|    policy_loss        | 0.0278   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.86e+03 |\n",
      "|    ep_rew_mean        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 876      |\n",
      "|    iterations         | 41600    |\n",
      "|    time_elapsed       | 3794     |\n",
      "|    total_timesteps    | 3328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.567    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41599    |\n",
      "|    policy_loss        | -0.0877  |\n",
      "|    value_loss         | 0.0529   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.88e+03 |\n",
      "|    ep_rew_mean        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 877      |\n",
      "|    iterations         | 41700    |\n",
      "|    time_elapsed       | 3801     |\n",
      "|    total_timesteps    | 3336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41699    |\n",
      "|    policy_loss        | -0.0483  |\n",
      "|    value_loss         | 0.0421   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.94e+03 |\n",
      "|    ep_rew_mean        | -16.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 878      |\n",
      "|    iterations         | 41800    |\n",
      "|    time_elapsed       | 3808     |\n",
      "|    total_timesteps    | 3344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    value_loss         | 0.00911  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3348928, episode_reward=-16.20 +/- 2.40\n",
      "Episode length: 7491.20 +/- 796.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.49e+03 |\n",
      "|    mean_reward        | -16.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3348928  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41861    |\n",
      "|    policy_loss        | -0.0926  |\n",
      "|    value_loss         | 0.0453   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.95e+03 |\n",
      "|    ep_rew_mean        | -16.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 875      |\n",
      "|    iterations         | 41900    |\n",
      "|    time_elapsed       | 3828     |\n",
      "|    total_timesteps    | 3352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41899    |\n",
      "|    policy_loss        | -0.0744  |\n",
      "|    value_loss         | 0.0156   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 6.97e+03 |\n",
      "|    ep_rew_mean        | -16.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 876      |\n",
      "|    iterations         | 42000    |\n",
      "|    time_elapsed       | 3835     |\n",
      "|    total_timesteps    | 3360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41999    |\n",
      "|    policy_loss        | 0.0325   |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.06e+03 |\n",
      "|    ep_rew_mean        | -16.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 876      |\n",
      "|    iterations         | 42100    |\n",
      "|    time_elapsed       | 3842     |\n",
      "|    total_timesteps    | 3368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | -0.0201  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3373920, episode_reward=-17.20 +/- 1.47\n",
      "Episode length: 7590.20 +/- 608.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.59e+03 |\n",
      "|    mean_reward        | -17.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3373920  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42173    |\n",
      "|    policy_loss        | -0.0566  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.11e+03 |\n",
      "|    ep_rew_mean        | -16.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 874      |\n",
      "|    iterations         | 42200    |\n",
      "|    time_elapsed       | 3862     |\n",
      "|    total_timesteps    | 3376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42199    |\n",
      "|    policy_loss        | -0.0309  |\n",
      "|    value_loss         | 0.00923  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.15e+03 |\n",
      "|    ep_rew_mean        | -16      |\n",
      "| time/                 |          |\n",
      "|    fps                | 874      |\n",
      "|    iterations         | 42300    |\n",
      "|    time_elapsed       | 3869     |\n",
      "|    total_timesteps    | 3384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42299    |\n",
      "|    policy_loss        | -0.049   |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.19e+03 |\n",
      "|    ep_rew_mean        | -16      |\n",
      "| time/                 |          |\n",
      "|    fps                | 875      |\n",
      "|    iterations         | 42400    |\n",
      "|    time_elapsed       | 3875     |\n",
      "|    total_timesteps    | 3392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42399    |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3398912, episode_reward=-16.40 +/- 3.07\n",
      "Episode length: 7570.40 +/- 912.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.57e+03 |\n",
      "|    mean_reward        | -16.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3398912  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42486    |\n",
      "|    policy_loss        | -0.0522  |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.22e+03 |\n",
      "|    ep_rew_mean        | -16      |\n",
      "| time/                 |          |\n",
      "|    fps                | 872      |\n",
      "|    iterations         | 42500    |\n",
      "|    time_elapsed       | 3896     |\n",
      "|    total_timesteps    | 3400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.58     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42499    |\n",
      "|    policy_loss        | -0.0603  |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.19e+03 |\n",
      "|    ep_rew_mean        | -16.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 873      |\n",
      "|    iterations         | 42600    |\n",
      "|    time_elapsed       | 3902     |\n",
      "|    total_timesteps    | 3408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42599    |\n",
      "|    policy_loss        | -0.0283  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.19e+03 |\n",
      "|    ep_rew_mean        | -16.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 873      |\n",
      "|    iterations         | 42700    |\n",
      "|    time_elapsed       | 3909     |\n",
      "|    total_timesteps    | 3416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42699    |\n",
      "|    policy_loss        | -0.0866  |\n",
      "|    value_loss         | 0.031    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3423904, episode_reward=-18.80 +/- 1.72\n",
      "Episode length: 6488.60 +/- 1068.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.49e+03 |\n",
      "|    mean_reward        | -18.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3423904  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.558    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42798    |\n",
      "|    policy_loss        | 0.0631   |\n",
      "|    value_loss         | 0.0277   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.18e+03 |\n",
      "|    ep_rew_mean        | -16.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 871      |\n",
      "|    iterations         | 42800    |\n",
      "|    time_elapsed       | 3928     |\n",
      "|    total_timesteps    | 3424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42799    |\n",
      "|    policy_loss        | 0.08     |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.19e+03 |\n",
      "|    ep_rew_mean        | -16.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 872      |\n",
      "|    iterations         | 42900    |\n",
      "|    time_elapsed       | 3934     |\n",
      "|    total_timesteps    | 3432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42899    |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.00841  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.22e+03 |\n",
      "|    ep_rew_mean        | -16      |\n",
      "| time/                 |          |\n",
      "|    fps                | 872      |\n",
      "|    iterations         | 43000    |\n",
      "|    time_elapsed       | 3941     |\n",
      "|    total_timesteps    | 3440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42999    |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.2e+03  |\n",
      "|    ep_rew_mean        | -16.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 873      |\n",
      "|    iterations         | 43100    |\n",
      "|    time_elapsed       | 3948     |\n",
      "|    total_timesteps    | 3448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43099    |\n",
      "|    policy_loss        | 0.0952   |\n",
      "|    value_loss         | 0.0269   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3448896, episode_reward=-16.80 +/- 3.06\n",
      "Episode length: 7017.80 +/- 796.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.02e+03 |\n",
      "|    mean_reward        | -16.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3448896  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43111    |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.24e+03 |\n",
      "|    ep_rew_mean        | -16      |\n",
      "| time/                 |          |\n",
      "|    fps                | 871      |\n",
      "|    iterations         | 43200    |\n",
      "|    time_elapsed       | 3967     |\n",
      "|    total_timesteps    | 3456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43199    |\n",
      "|    policy_loss        | 0.0264   |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.25e+03 |\n",
      "|    ep_rew_mean        | -15.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 871      |\n",
      "|    iterations         | 43300    |\n",
      "|    time_elapsed       | 3974     |\n",
      "|    total_timesteps    | 3464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | 0.0117   |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.3e+03  |\n",
      "|    ep_rew_mean        | -15.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 872      |\n",
      "|    iterations         | 43400    |\n",
      "|    time_elapsed       | 3981     |\n",
      "|    total_timesteps    | 3472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43399    |\n",
      "|    policy_loss        | -0.0284  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3473888, episode_reward=-14.60 +/- 5.00\n",
      "Episode length: 7936.40 +/- 1985.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.94e+03 |\n",
      "|    mean_reward        | -14.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3473888  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43423    |\n",
      "|    policy_loss        | -0.00257 |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.32e+03 |\n",
      "|    ep_rew_mean        | -15.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 869      |\n",
      "|    iterations         | 43500    |\n",
      "|    time_elapsed       | 4002     |\n",
      "|    total_timesteps    | 3480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43499    |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.36e+03 |\n",
      "|    ep_rew_mean        | -15.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 870      |\n",
      "|    iterations         | 43600    |\n",
      "|    time_elapsed       | 4009     |\n",
      "|    total_timesteps    | 3488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | -0.117   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | -0.0226  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.39e+03 |\n",
      "|    ep_rew_mean        | -15.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 870      |\n",
      "|    iterations         | 43700    |\n",
      "|    time_elapsed       | 4015     |\n",
      "|    total_timesteps    | 3496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43699    |\n",
      "|    policy_loss        | 0.00678  |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3498880, episode_reward=-17.00 +/- 0.89\n",
      "Episode length: 7114.60 +/- 371.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.11e+03 |\n",
      "|    mean_reward        | -17      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3498880  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43735    |\n",
      "|    policy_loss        | -0.0422  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.4e+03  |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 868      |\n",
      "|    iterations         | 43800    |\n",
      "|    time_elapsed       | 4035     |\n",
      "|    total_timesteps    | 3504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43799    |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 868      |\n",
      "|    iterations         | 43900    |\n",
      "|    time_elapsed       | 4042     |\n",
      "|    total_timesteps    | 3512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43899    |\n",
      "|    policy_loss        | -0.0623  |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 869      |\n",
      "|    iterations         | 44000    |\n",
      "|    time_elapsed       | 4048     |\n",
      "|    total_timesteps    | 3520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43999    |\n",
      "|    policy_loss        | 0.0891   |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3523872, episode_reward=-15.40 +/- 2.87\n",
      "Episode length: 7545.40 +/- 344.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.55e+03 |\n",
      "|    mean_reward        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3523872  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44048    |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.41e+03 |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 866      |\n",
      "|    iterations         | 44100    |\n",
      "|    time_elapsed       | 4069     |\n",
      "|    total_timesteps    | 3528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | -0.042   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.43e+03 |\n",
      "|    ep_rew_mean        | -15.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 867      |\n",
      "|    iterations         | 44200    |\n",
      "|    time_elapsed       | 4076     |\n",
      "|    total_timesteps    | 3536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44199    |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 868      |\n",
      "|    iterations         | 44300    |\n",
      "|    time_elapsed       | 4082     |\n",
      "|    total_timesteps    | 3544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44299    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3548864, episode_reward=-14.40 +/- 5.43\n",
      "Episode length: 7929.80 +/- 1295.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.93e+03 |\n",
      "|    mean_reward        | -14.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3548864  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44360    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.00558  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 865      |\n",
      "|    iterations         | 44400    |\n",
      "|    time_elapsed       | 4104     |\n",
      "|    total_timesteps    | 3552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44399    |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.46e+03 |\n",
      "|    ep_rew_mean        | -15.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 865      |\n",
      "|    iterations         | 44500    |\n",
      "|    time_elapsed       | 4110     |\n",
      "|    total_timesteps    | 3560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44499    |\n",
      "|    policy_loss        | -0.0652  |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 866      |\n",
      "|    iterations         | 44600    |\n",
      "|    time_elapsed       | 4117     |\n",
      "|    total_timesteps    | 3568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44599    |\n",
      "|    policy_loss        | -0.0685  |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3573856, episode_reward=-12.20 +/- 3.12\n",
      "Episode length: 8618.60 +/- 919.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.62e+03 |\n",
      "|    mean_reward        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3573856  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44673    |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.00796  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.42e+03 |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 863      |\n",
      "|    iterations         | 44700    |\n",
      "|    time_elapsed       | 4140     |\n",
      "|    total_timesteps    | 3576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44699    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.43e+03 |\n",
      "|    ep_rew_mean        | -15.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 864      |\n",
      "|    iterations         | 44800    |\n",
      "|    time_elapsed       | 4146     |\n",
      "|    total_timesteps    | 3584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44799    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.44e+03 |\n",
      "|    ep_rew_mean        | -15.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 864      |\n",
      "|    iterations         | 44900    |\n",
      "|    time_elapsed       | 4153     |\n",
      "|    total_timesteps    | 3592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44899    |\n",
      "|    policy_loss        | -0.0725  |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3598848, episode_reward=-17.40 +/- 3.20\n",
      "Episode length: 7555.20 +/- 1111.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.56e+03 |\n",
      "|    mean_reward        | -17.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3598848  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44985    |\n",
      "|    policy_loss        | 0.047    |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.46e+03 |\n",
      "|    ep_rew_mean        | -15.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 862      |\n",
      "|    iterations         | 45000    |\n",
      "|    time_elapsed       | 4174     |\n",
      "|    total_timesteps    | 3600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44999    |\n",
      "|    policy_loss        | -0.106   |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.5e+03  |\n",
      "|    ep_rew_mean        | -15.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 862      |\n",
      "|    iterations         | 45100    |\n",
      "|    time_elapsed       | 4180     |\n",
      "|    total_timesteps    | 3608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45099    |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.53e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 863      |\n",
      "|    iterations         | 45200    |\n",
      "|    time_elapsed       | 4187     |\n",
      "|    total_timesteps    | 3616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.701    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45199    |\n",
      "|    policy_loss        | 0.00214  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3623840, episode_reward=-16.60 +/- 2.06\n",
      "Episode length: 7242.80 +/- 770.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.24e+03 |\n",
      "|    mean_reward        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3623840  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45297    |\n",
      "|    policy_loss        | -0.0592  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.56e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 861      |\n",
      "|    iterations         | 45300    |\n",
      "|    time_elapsed       | 4207     |\n",
      "|    total_timesteps    | 3624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45299    |\n",
      "|    policy_loss        | 0.083    |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.59e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 861      |\n",
      "|    iterations         | 45400    |\n",
      "|    time_elapsed       | 4214     |\n",
      "|    total_timesteps    | 3632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45399    |\n",
      "|    policy_loss        | 0.0364   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.58e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 862      |\n",
      "|    iterations         | 45500    |\n",
      "|    time_elapsed       | 4220     |\n",
      "|    total_timesteps    | 3640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45499    |\n",
      "|    policy_loss        | -0.0289  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.61e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 862      |\n",
      "|    iterations         | 45600    |\n",
      "|    time_elapsed       | 4227     |\n",
      "|    total_timesteps    | 3648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.475    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45599    |\n",
      "|    policy_loss        | 0.00336  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3648832, episode_reward=-17.20 +/- 2.71\n",
      "Episode length: 7602.20 +/- 669.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.6e+03  |\n",
      "|    mean_reward        | -17.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3648832  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.582    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45610    |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.0466   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.64e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 860      |\n",
      "|    iterations         | 45700    |\n",
      "|    time_elapsed       | 4248     |\n",
      "|    total_timesteps    | 3656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45699    |\n",
      "|    policy_loss        | 0.0244   |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.6e+03  |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 861      |\n",
      "|    iterations         | 45800    |\n",
      "|    time_elapsed       | 4254     |\n",
      "|    total_timesteps    | 3664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45799    |\n",
      "|    policy_loss        | -0.0617  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.64e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 861      |\n",
      "|    iterations         | 45900    |\n",
      "|    time_elapsed       | 4261     |\n",
      "|    total_timesteps    | 3672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45899    |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3673824, episode_reward=-15.00 +/- 4.52\n",
      "Episode length: 7209.00 +/- 1263.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.21e+03 |\n",
      "|    mean_reward        | -15      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3673824  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.708    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45922    |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    value_loss         | 0.0457   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.64e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 859      |\n",
      "|    iterations         | 46000    |\n",
      "|    time_elapsed       | 4281     |\n",
      "|    total_timesteps    | 3680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.136    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45999    |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.67e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 860      |\n",
      "|    iterations         | 46100    |\n",
      "|    time_elapsed       | 4288     |\n",
      "|    total_timesteps    | 3688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46099    |\n",
      "|    policy_loss        | -0.0595  |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 7.64e+03  |\n",
      "|    ep_rew_mean        | -15.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 860       |\n",
      "|    iterations         | 46200     |\n",
      "|    time_elapsed       | 4295      |\n",
      "|    total_timesteps    | 3696000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0.859     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46199     |\n",
      "|    policy_loss        | -0.000886 |\n",
      "|    value_loss         | 0.0156    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=3698816, episode_reward=-11.40 +/- 5.04\n",
      "Episode length: 8478.60 +/- 1317.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.48e+03 |\n",
      "|    mean_reward        | -11.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3698816  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.464    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46235    |\n",
      "|    policy_loss        | 0.0741   |\n",
      "|    value_loss         | 0.0426   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.68e+03 |\n",
      "|    ep_rew_mean        | -15.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 857      |\n",
      "|    iterations         | 46300    |\n",
      "|    time_elapsed       | 4317     |\n",
      "|    total_timesteps    | 3704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46299    |\n",
      "|    policy_loss        | 0.00775  |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.73e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 858      |\n",
      "|    iterations         | 46400    |\n",
      "|    time_elapsed       | 4324     |\n",
      "|    total_timesteps    | 3712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.591    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46399    |\n",
      "|    policy_loss        | 0.0552   |\n",
      "|    value_loss         | 0.0531   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.76e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 858      |\n",
      "|    iterations         | 46500    |\n",
      "|    time_elapsed       | 4330     |\n",
      "|    total_timesteps    | 3720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46499    |\n",
      "|    policy_loss        | -0.0408  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3723808, episode_reward=-14.20 +/- 4.17\n",
      "Episode length: 8194.40 +/- 1591.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.19e+03 |\n",
      "|    mean_reward        | -14.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3723808  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46547    |\n",
      "|    policy_loss        | 0.138    |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.73e+03 |\n",
      "|    ep_rew_mean        | -15.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 856      |\n",
      "|    iterations         | 46600    |\n",
      "|    time_elapsed       | 4352     |\n",
      "|    total_timesteps    | 3728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46599    |\n",
      "|    policy_loss        | 0.0761   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.77e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 856      |\n",
      "|    iterations         | 46700    |\n",
      "|    time_elapsed       | 4359     |\n",
      "|    total_timesteps    | 3736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46699    |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.77e+03 |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 857      |\n",
      "|    iterations         | 46800    |\n",
      "|    time_elapsed       | 4366     |\n",
      "|    total_timesteps    | 3744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46799    |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3748800, episode_reward=-14.60 +/- 2.87\n",
      "Episode length: 8086.20 +/- 1244.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | -14.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3748800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46859    |\n",
      "|    policy_loss        | -0.0236  |\n",
      "|    value_loss         | 0.0266   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.79e+03 |\n",
      "|    ep_rew_mean        | -15      |\n",
      "| time/                 |          |\n",
      "|    fps                | 855      |\n",
      "|    iterations         | 46900    |\n",
      "|    time_elapsed       | 4387     |\n",
      "|    total_timesteps    | 3752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | -0.00456 |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.8e+03  |\n",
      "|    ep_rew_mean        | -15      |\n",
      "| time/                 |          |\n",
      "|    fps                | 855      |\n",
      "|    iterations         | 47000    |\n",
      "|    time_elapsed       | 4394     |\n",
      "|    total_timesteps    | 3760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46999    |\n",
      "|    policy_loss        | 0.0075   |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.83e+03 |\n",
      "|    ep_rew_mean        | -15      |\n",
      "| time/                 |          |\n",
      "|    fps                | 856      |\n",
      "|    iterations         | 47100    |\n",
      "|    time_elapsed       | 4401     |\n",
      "|    total_timesteps    | 3768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47099    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3773792, episode_reward=-10.80 +/- 3.97\n",
      "Episode length: 8776.40 +/- 1056.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.78e+03 |\n",
      "|    mean_reward        | -10.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3773792  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47172    |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    value_loss         | 0.0341   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 7.82e+03  |\n",
      "|    ep_rew_mean        | -15       |\n",
      "| time/                 |           |\n",
      "|    fps                | 853       |\n",
      "|    iterations         | 47200     |\n",
      "|    time_elapsed       | 4424      |\n",
      "|    total_timesteps    | 3776000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0.912     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47199     |\n",
      "|    policy_loss        | -0.000875 |\n",
      "|    value_loss         | 0.00841   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.83e+03 |\n",
      "|    ep_rew_mean        | -15      |\n",
      "| time/                 |          |\n",
      "|    fps                | 853      |\n",
      "|    iterations         | 47300    |\n",
      "|    time_elapsed       | 4431     |\n",
      "|    total_timesteps    | 3784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47299    |\n",
      "|    policy_loss        | 0.0837   |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.89e+03 |\n",
      "|    ep_rew_mean        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 854      |\n",
      "|    iterations         | 47400    |\n",
      "|    time_elapsed       | 4437     |\n",
      "|    total_timesteps    | 3792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47399    |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.0631   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3798784, episode_reward=-12.80 +/- 2.79\n",
      "Episode length: 8671.80 +/- 663.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.67e+03 |\n",
      "|    mean_reward        | -12.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3798784  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.649    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47484    |\n",
      "|    policy_loss        | 0.118    |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.9e+03  |\n",
      "|    ep_rew_mean        | -14.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 851      |\n",
      "|    iterations         | 47500    |\n",
      "|    time_elapsed       | 4460     |\n",
      "|    total_timesteps    | 3800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47499    |\n",
      "|    policy_loss        | 0.00294  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.91e+03 |\n",
      "|    ep_rew_mean        | -14.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 852      |\n",
      "|    iterations         | 47600    |\n",
      "|    time_elapsed       | 4466     |\n",
      "|    total_timesteps    | 3808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47599    |\n",
      "|    policy_loss        | 0.0727   |\n",
      "|    value_loss         | 0.0243   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.91e+03 |\n",
      "|    ep_rew_mean        | -14.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 853      |\n",
      "|    iterations         | 47700    |\n",
      "|    time_elapsed       | 4473     |\n",
      "|    total_timesteps    | 3816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47699    |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3823776, episode_reward=-16.60 +/- 5.54\n",
      "Episode length: 7709.40 +/- 1484.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.71e+03 |\n",
      "|    mean_reward        | -16.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3823776  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47797    |\n",
      "|    policy_loss        | 0.0468   |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.89e+03 |\n",
      "|    ep_rew_mean        | -14.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 850      |\n",
      "|    iterations         | 47800    |\n",
      "|    time_elapsed       | 4494     |\n",
      "|    total_timesteps    | 3824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47799    |\n",
      "|    policy_loss        | -0.0295  |\n",
      "|    value_loss         | 0.00797  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.95e+03 |\n",
      "|    ep_rew_mean        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 851      |\n",
      "|    iterations         | 47900    |\n",
      "|    time_elapsed       | 4500     |\n",
      "|    total_timesteps    | 3832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47899    |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.96e+03 |\n",
      "|    ep_rew_mean        | -14.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 851      |\n",
      "|    iterations         | 48000    |\n",
      "|    time_elapsed       | 4507     |\n",
      "|    total_timesteps    | 3840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47999    |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.94e+03 |\n",
      "|    ep_rew_mean        | -14.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 852      |\n",
      "|    iterations         | 48100    |\n",
      "|    time_elapsed       | 4514     |\n",
      "|    total_timesteps    | 3848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48099    |\n",
      "|    policy_loss        | 0.0834   |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3848768, episode_reward=-14.80 +/- 4.26\n",
      "Episode length: 8330.40 +/- 1308.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.33e+03 |\n",
      "|    mean_reward        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3848768  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48109    |\n",
      "|    policy_loss        | 0.0321   |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.92e+03 |\n",
      "|    ep_rew_mean        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 849      |\n",
      "|    iterations         | 48200    |\n",
      "|    time_elapsed       | 4536     |\n",
      "|    total_timesteps    | 3856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48199    |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.96e+03 |\n",
      "|    ep_rew_mean        | -14.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 850      |\n",
      "|    iterations         | 48300    |\n",
      "|    time_elapsed       | 4543     |\n",
      "|    total_timesteps    | 3864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48299    |\n",
      "|    policy_loss        | -0.0385  |\n",
      "|    value_loss         | 0.041    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.05e+03 |\n",
      "|    ep_rew_mean        | -14.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 851      |\n",
      "|    iterations         | 48400    |\n",
      "|    time_elapsed       | 4549     |\n",
      "|    total_timesteps    | 3872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48399    |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.00834  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3873760, episode_reward=-14.80 +/- 3.71\n",
      "Episode length: 8316.80 +/- 1626.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.32e+03 |\n",
      "|    mean_reward        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3873760  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48421    |\n",
      "|    policy_loss        | 0.0399   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.05e+03 |\n",
      "|    ep_rew_mean        | -14.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 848      |\n",
      "|    iterations         | 48500    |\n",
      "|    time_elapsed       | 4571     |\n",
      "|    total_timesteps    | 3880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | -14.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 849      |\n",
      "|    iterations         | 48600    |\n",
      "|    time_elapsed       | 4578     |\n",
      "|    total_timesteps    | 3888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48599    |\n",
      "|    policy_loss        | -0.0478  |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.09e+03 |\n",
      "|    ep_rew_mean        | -14.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 849      |\n",
      "|    iterations         | 48700    |\n",
      "|    time_elapsed       | 4585     |\n",
      "|    total_timesteps    | 3896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48699    |\n",
      "|    policy_loss        | 0.0626   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3898752, episode_reward=-13.20 +/- 4.83\n",
      "Episode length: 8675.60 +/- 1267.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.68e+03 |\n",
      "|    mean_reward        | -13.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3898752  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48734    |\n",
      "|    policy_loss        | 0.0801   |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | -13.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 847      |\n",
      "|    iterations         | 48800    |\n",
      "|    time_elapsed       | 4607     |\n",
      "|    total_timesteps    | 3904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.409    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | -0.0322  |\n",
      "|    value_loss         | 0.0322   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | -13.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 847      |\n",
      "|    iterations         | 48900    |\n",
      "|    time_elapsed       | 4614     |\n",
      "|    total_timesteps    | 3912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48899    |\n",
      "|    policy_loss        | 0.0346   |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | -13.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 848      |\n",
      "|    iterations         | 49000    |\n",
      "|    time_elapsed       | 4621     |\n",
      "|    total_timesteps    | 3920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48999    |\n",
      "|    policy_loss        | 0.0529   |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3923744, episode_reward=-14.80 +/- 3.66\n",
      "Episode length: 8187.20 +/- 1273.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.19e+03 |\n",
      "|    mean_reward        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3923744  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49046    |\n",
      "|    policy_loss        | 0.0739   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | -13.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 845      |\n",
      "|    iterations         | 49100    |\n",
      "|    time_elapsed       | 4643     |\n",
      "|    total_timesteps    | 3928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49099    |\n",
      "|    policy_loss        | -0.1     |\n",
      "|    value_loss         | 0.0411   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | -13.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 846      |\n",
      "|    iterations         | 49200    |\n",
      "|    time_elapsed       | 4650     |\n",
      "|    total_timesteps    | 3936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49199    |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | -13.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 846      |\n",
      "|    iterations         | 49300    |\n",
      "|    time_elapsed       | 4656     |\n",
      "|    total_timesteps    | 3944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49299    |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3948736, episode_reward=-15.60 +/- 3.26\n",
      "Episode length: 8642.60 +/- 1317.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | -15.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3948736  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.643    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49359    |\n",
      "|    policy_loss        | 0.0722   |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | -13.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 844      |\n",
      "|    iterations         | 49400    |\n",
      "|    time_elapsed       | 4679     |\n",
      "|    total_timesteps    | 3952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | -0.00378 |\n",
      "|    value_loss         | 0.0272   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | -13.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 845      |\n",
      "|    iterations         | 49500    |\n",
      "|    time_elapsed       | 4686     |\n",
      "|    total_timesteps    | 3960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49499    |\n",
      "|    policy_loss        | -0.00633 |\n",
      "|    value_loss         | 0.0499   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | -13.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 845      |\n",
      "|    iterations         | 49600    |\n",
      "|    time_elapsed       | 4692     |\n",
      "|    total_timesteps    | 3968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49599    |\n",
      "|    policy_loss        | 0.0381   |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3973728, episode_reward=-14.80 +/- 3.49\n",
      "Episode length: 7910.60 +/- 1185.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.91e+03 |\n",
      "|    mean_reward        | -14.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3973728  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49671    |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | -13.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 843      |\n",
      "|    iterations         | 49700    |\n",
      "|    time_elapsed       | 4714     |\n",
      "|    total_timesteps    | 3976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.538    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | -0.0571  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | -13.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 843      |\n",
      "|    iterations         | 49800    |\n",
      "|    time_elapsed       | 4720     |\n",
      "|    total_timesteps    | 3984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49799    |\n",
      "|    policy_loss        | -0.00292 |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | -13.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 844      |\n",
      "|    iterations         | 49900    |\n",
      "|    time_elapsed       | 4727     |\n",
      "|    total_timesteps    | 3992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49899    |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3998720, episode_reward=-16.80 +/- 3.43\n",
      "Episode length: 7571.20 +/- 1029.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.57e+03 |\n",
      "|    mean_reward        | -16.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3998720  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49983    |\n",
      "|    policy_loss        | -0.0417  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | -13.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 842      |\n",
      "|    iterations         | 50000    |\n",
      "|    time_elapsed       | 4748     |\n",
      "|    total_timesteps    | 4000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49999    |\n",
      "|    policy_loss        | 0.0201   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | -13.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 842      |\n",
      "|    iterations         | 50100    |\n",
      "|    time_elapsed       | 4755     |\n",
      "|    total_timesteps    | 4008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50099    |\n",
      "|    policy_loss        | -0.0031  |\n",
      "|    value_loss         | 0.00979  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | -13      |\n",
      "| time/                 |          |\n",
      "|    fps                | 843      |\n",
      "|    iterations         | 50200    |\n",
      "|    time_elapsed       | 4761     |\n",
      "|    total_timesteps    | 4016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50199    |\n",
      "|    policy_loss        | 0.0672   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4023712, episode_reward=-9.20 +/- 4.71\n",
      "Episode length: 9906.00 +/- 1582.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.91e+03 |\n",
      "|    mean_reward        | -9.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4023712  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50296    |\n",
      "|    policy_loss        | -0.0532  |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | -12.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 840      |\n",
      "|    iterations         | 50300    |\n",
      "|    time_elapsed       | 4786     |\n",
      "|    total_timesteps    | 4024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50299    |\n",
      "|    policy_loss        | 0.0427   |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | -12.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 841      |\n",
      "|    iterations         | 50400    |\n",
      "|    time_elapsed       | 4793     |\n",
      "|    total_timesteps    | 4032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50399    |\n",
      "|    policy_loss        | -0.0762  |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | -12.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 841      |\n",
      "|    iterations         | 50500    |\n",
      "|    time_elapsed       | 4800     |\n",
      "|    total_timesteps    | 4040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50499    |\n",
      "|    policy_loss        | 0.0405   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.7e+03  |\n",
      "|    ep_rew_mean        | -12.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 842      |\n",
      "|    iterations         | 50600    |\n",
      "|    time_elapsed       | 4806     |\n",
      "|    total_timesteps    | 4048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50599    |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4048704, episode_reward=-12.40 +/- 4.96\n",
      "Episode length: 8663.60 +/- 1213.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.66e+03 |\n",
      "|    mean_reward        | -12.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4048704  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50608    |\n",
      "|    policy_loss        | 0.00203  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.8e+03  |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 839      |\n",
      "|    iterations         | 50700    |\n",
      "|    time_elapsed       | 4829     |\n",
      "|    total_timesteps    | 4056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | -0.00559 |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.79e+03 |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 840      |\n",
      "|    iterations         | 50800    |\n",
      "|    time_elapsed       | 4836     |\n",
      "|    total_timesteps    | 4064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.877    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50799    |\n",
      "|    policy_loss        | -0.0394  |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.84e+03 |\n",
      "|    ep_rew_mean        | -12.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 840      |\n",
      "|    iterations         | 50900    |\n",
      "|    time_elapsed       | 4843     |\n",
      "|    total_timesteps    | 4072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50899    |\n",
      "|    policy_loss        | 7.58e-06 |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4073696, episode_reward=-7.20 +/- 6.49\n",
      "Episode length: 10368.20 +/- 1996.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | -7.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4073696  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50921    |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.85e+03 |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 838      |\n",
      "|    iterations         | 51000    |\n",
      "|    time_elapsed       | 4868     |\n",
      "|    total_timesteps    | 4080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50999    |\n",
      "|    policy_loss        | 0.0406   |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.83e+03 |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 838      |\n",
      "|    iterations         | 51100    |\n",
      "|    time_elapsed       | 4875     |\n",
      "|    total_timesteps    | 4088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51099    |\n",
      "|    policy_loss        | -0.0533  |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.78e+03 |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 838      |\n",
      "|    iterations         | 51200    |\n",
      "|    time_elapsed       | 4882     |\n",
      "|    total_timesteps    | 4096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51199    |\n",
      "|    policy_loss        | 0.0183   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4098688, episode_reward=-14.40 +/- 5.85\n",
      "Episode length: 8453.00 +/- 1916.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.45e+03 |\n",
      "|    mean_reward        | -14.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4098688  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51233    |\n",
      "|    policy_loss        | 0.0034   |\n",
      "|    value_loss         | 0.00795  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.74e+03 |\n",
      "|    ep_rew_mean        | -12.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 836      |\n",
      "|    iterations         | 51300    |\n",
      "|    time_elapsed       | 4904     |\n",
      "|    total_timesteps    | 4104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51299    |\n",
      "|    policy_loss        | -0.0489  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.7e+03  |\n",
      "|    ep_rew_mean        | -12.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 837      |\n",
      "|    iterations         | 51400    |\n",
      "|    time_elapsed       | 4910     |\n",
      "|    total_timesteps    | 4112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51399    |\n",
      "|    policy_loss        | 0.0568   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.68e+03 |\n",
      "|    ep_rew_mean        | -12.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 837      |\n",
      "|    iterations         | 51500    |\n",
      "|    time_elapsed       | 4917     |\n",
      "|    total_timesteps    | 4120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.544    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51499    |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4123680, episode_reward=-8.80 +/- 7.14\n",
      "Episode length: 9432.00 +/- 1797.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.43e+03 |\n",
      "|    mean_reward        | -8.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4123680  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51545    |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | -12.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 835      |\n",
      "|    iterations         | 51600    |\n",
      "|    time_elapsed       | 4941     |\n",
      "|    total_timesteps    | 4128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51599    |\n",
      "|    policy_loss        | -0.0204  |\n",
      "|    value_loss         | 0.0296   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | -12.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 835      |\n",
      "|    iterations         | 51700    |\n",
      "|    time_elapsed       | 4947     |\n",
      "|    total_timesteps    | 4136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51699    |\n",
      "|    policy_loss        | -0.0906  |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | -12.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 836      |\n",
      "|    iterations         | 51800    |\n",
      "|    time_elapsed       | 4954     |\n",
      "|    total_timesteps    | 4144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51799    |\n",
      "|    policy_loss        | 0.094    |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4148672, episode_reward=-11.80 +/- 4.35\n",
      "Episode length: 9281.40 +/- 1783.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.28e+03 |\n",
      "|    mean_reward        | -11.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4148672  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51858    |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | -12.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 834      |\n",
      "|    iterations         | 51900    |\n",
      "|    time_elapsed       | 4978     |\n",
      "|    total_timesteps    | 4152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51899    |\n",
      "|    policy_loss        | -0.0899  |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.67e+03 |\n",
      "|    ep_rew_mean        | -12.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 834      |\n",
      "|    iterations         | 52000    |\n",
      "|    time_elapsed       | 4984     |\n",
      "|    total_timesteps    | 4160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51999    |\n",
      "|    policy_loss        | 0.0775   |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | -12.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 835      |\n",
      "|    iterations         | 52100    |\n",
      "|    time_elapsed       | 4991     |\n",
      "|    total_timesteps    | 4168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.0227   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4173664, episode_reward=-13.60 +/- 2.87\n",
      "Episode length: 7981.40 +/- 932.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.98e+03 |\n",
      "|    mean_reward        | -13.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4173664  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.515    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52170    |\n",
      "|    policy_loss        | -0.0214  |\n",
      "|    value_loss         | 0.0708   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.8e+03  |\n",
      "|    ep_rew_mean        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 52200    |\n",
      "|    time_elapsed       | 5012     |\n",
      "|    total_timesteps    | 4176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | -0.0389  |\n",
      "|    value_loss         | 0.045    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.78e+03 |\n",
      "|    ep_rew_mean        | -12.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 833      |\n",
      "|    iterations         | 52300    |\n",
      "|    time_elapsed       | 5019     |\n",
      "|    total_timesteps    | 4184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52299    |\n",
      "|    policy_loss        | -0.0755  |\n",
      "|    value_loss         | 0.043    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.9e+03  |\n",
      "|    ep_rew_mean        | -11.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 834      |\n",
      "|    iterations         | 52400    |\n",
      "|    time_elapsed       | 5025     |\n",
      "|    total_timesteps    | 4192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52399    |\n",
      "|    policy_loss        | 0.0493   |\n",
      "|    value_loss         | 0.0316   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4198656, episode_reward=-10.40 +/- 7.06\n",
      "Episode length: 9267.60 +/- 1934.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.27e+03 |\n",
      "|    mean_reward        | -10.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4198656  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52483    |\n",
      "|    policy_loss        | -0.0563  |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.92e+03 |\n",
      "|    ep_rew_mean        | -12      |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 52500    |\n",
      "|    time_elapsed       | 5049     |\n",
      "|    total_timesteps    | 4200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52499    |\n",
      "|    policy_loss        | 0.0244   |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.94e+03 |\n",
      "|    ep_rew_mean        | -11.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 52600    |\n",
      "|    time_elapsed       | 5056     |\n",
      "|    total_timesteps    | 4208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52599    |\n",
      "|    policy_loss        | -0.0757  |\n",
      "|    value_loss         | 0.0316   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.95e+03 |\n",
      "|    ep_rew_mean        | -11.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 52700    |\n",
      "|    time_elapsed       | 5062     |\n",
      "|    total_timesteps    | 4216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52699    |\n",
      "|    policy_loss        | -0.0442  |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4223648, episode_reward=-11.20 +/- 3.43\n",
      "Episode length: 9695.60 +/- 953.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.7e+03  |\n",
      "|    mean_reward        | -11.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4223648  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52795    |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.95e+03 |\n",
      "|    ep_rew_mean        | -11.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 52800    |\n",
      "|    time_elapsed       | 5086     |\n",
      "|    total_timesteps    | 4224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52799    |\n",
      "|    policy_loss        | 0.0873   |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02e+03 |\n",
      "|    ep_rew_mean        | -11.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 52900    |\n",
      "|    time_elapsed       | 5093     |\n",
      "|    total_timesteps    | 4232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52899    |\n",
      "|    policy_loss        | 0.00104  |\n",
      "|    value_loss         | 0.0144   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.05e+03 |\n",
      "|    ep_rew_mean        | -11.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 53000    |\n",
      "|    time_elapsed       | 5100     |\n",
      "|    total_timesteps    | 4240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52999    |\n",
      "|    policy_loss        | 0.00915  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.06e+03 |\n",
      "|    ep_rew_mean        | -11.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 831      |\n",
      "|    iterations         | 53100    |\n",
      "|    time_elapsed       | 5107     |\n",
      "|    total_timesteps    | 4248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53099    |\n",
      "|    policy_loss        | 0.0598   |\n",
      "|    value_loss         | 0.0254   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4248640, episode_reward=-8.80 +/- 3.19\n",
      "Episode length: 10347.80 +/- 1181.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | -8.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4248640  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.66     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53107    |\n",
      "|    policy_loss        | -0.0375  |\n",
      "|    value_loss         | 0.0435   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.06e+03 |\n",
      "|    ep_rew_mean        | -11.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 53200    |\n",
      "|    time_elapsed       | 5132     |\n",
      "|    total_timesteps    | 4256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.745    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53199    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0515   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.04e+03 |\n",
      "|    ep_rew_mean        | -11.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 829      |\n",
      "|    iterations         | 53300    |\n",
      "|    time_elapsed       | 5139     |\n",
      "|    total_timesteps    | 4264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53299    |\n",
      "|    policy_loss        | -0.00716 |\n",
      "|    value_loss         | 0.0237   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.05e+03 |\n",
      "|    ep_rew_mean        | -11.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 830      |\n",
      "|    iterations         | 53400    |\n",
      "|    time_elapsed       | 5145     |\n",
      "|    total_timesteps    | 4272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53399    |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    value_loss         | 0.00948  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4273632, episode_reward=-13.40 +/- 3.14\n",
      "Episode length: 9154.20 +/- 2319.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.15e+03 |\n",
      "|    mean_reward        | -13.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4273632  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53420    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.09e+03 |\n",
      "|    ep_rew_mean        | -11.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 53500    |\n",
      "|    time_elapsed       | 5169     |\n",
      "|    total_timesteps    | 4280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53499    |\n",
      "|    policy_loss        | 0.0209   |\n",
      "|    value_loss         | 0.00786  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.17e+03 |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 53600    |\n",
      "|    time_elapsed       | 5175     |\n",
      "|    total_timesteps    | 4288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53599    |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.2e+03  |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 828      |\n",
      "|    iterations         | 53700    |\n",
      "|    time_elapsed       | 5182     |\n",
      "|    total_timesteps    | 4296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53699    |\n",
      "|    policy_loss        | -0.0959  |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4298624, episode_reward=-10.40 +/- 5.24\n",
      "Episode length: 9711.00 +/- 820.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.71e+03 |\n",
      "|    mean_reward        | -10.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4298624  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53732    |\n",
      "|    policy_loss        | 0.0592   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.2e+03  |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 53800    |\n",
      "|    time_elapsed       | 5206     |\n",
      "|    total_timesteps    | 4304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53799    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.22e+03 |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 53900    |\n",
      "|    time_elapsed       | 5213     |\n",
      "|    total_timesteps    | 4312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53899    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.28e+03 |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 827      |\n",
      "|    iterations         | 54000    |\n",
      "|    time_elapsed       | 5220     |\n",
      "|    total_timesteps    | 4320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53999    |\n",
      "|    policy_loss        | -0.0404  |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4323616, episode_reward=-10.40 +/- 5.46\n",
      "Episode length: 9279.20 +/- 1361.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.28e+03 |\n",
      "|    mean_reward        | -10.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4323616  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54045    |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.29e+03 |\n",
      "|    ep_rew_mean        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 54100    |\n",
      "|    time_elapsed       | 5243     |\n",
      "|    total_timesteps    | 4328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54099    |\n",
      "|    policy_loss        | -0.0376  |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.36e+03 |\n",
      "|    ep_rew_mean        | -11.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 825      |\n",
      "|    iterations         | 54200    |\n",
      "|    time_elapsed       | 5250     |\n",
      "|    total_timesteps    | 4336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | -0.0158  |\n",
      "|    value_loss         | 0.00838  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.45e+03 |\n",
      "|    ep_rew_mean        | -11.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 54300    |\n",
      "|    time_elapsed       | 5257     |\n",
      "|    total_timesteps    | 4344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54299    |\n",
      "|    policy_loss        | 0.00621  |\n",
      "|    value_loss         | 0.0378   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4348608, episode_reward=-10.20 +/- 3.06\n",
      "Episode length: 10041.80 +/- 1112.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | -10.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4348608  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54357    |\n",
      "|    policy_loss        | -0.00612 |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.45e+03 |\n",
      "|    ep_rew_mean        | -11.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 54400    |\n",
      "|    time_elapsed       | 5281     |\n",
      "|    total_timesteps    | 4352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54399    |\n",
      "|    policy_loss        | 0.189    |\n",
      "|    value_loss         | 0.0463   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.49e+03 |\n",
      "|    ep_rew_mean        | -11.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 54500    |\n",
      "|    time_elapsed       | 5288     |\n",
      "|    total_timesteps    | 4360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.769    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54499    |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    value_loss         | 0.0477   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.52e+03 |\n",
      "|    ep_rew_mean        | -11.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 54600    |\n",
      "|    time_elapsed       | 5294     |\n",
      "|    total_timesteps    | 4368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54599    |\n",
      "|    policy_loss        | 0.000369 |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4373600, episode_reward=-11.60 +/- 3.77\n",
      "Episode length: 9698.40 +/- 1116.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.7e+03  |\n",
      "|    mean_reward        | -11.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4373600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54669    |\n",
      "|    policy_loss        | 0.071    |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.62e+03 |\n",
      "|    ep_rew_mean        | -11      |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 54700    |\n",
      "|    time_elapsed       | 5319     |\n",
      "|    total_timesteps    | 4376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54699    |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.0486   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.7e+03  |\n",
      "|    ep_rew_mean        | -10.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 54800    |\n",
      "|    time_elapsed       | 5325     |\n",
      "|    total_timesteps    | 4384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54799    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | -10.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 823      |\n",
      "|    iterations         | 54900    |\n",
      "|    time_elapsed       | 5332     |\n",
      "|    total_timesteps    | 4392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54899    |\n",
      "|    policy_loss        | -0.0234  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4398592, episode_reward=-9.80 +/- 3.97\n",
      "Episode length: 9549.80 +/- 1149.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.55e+03 |\n",
      "|    mean_reward        | -9.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4398592  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.17     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54982    |\n",
      "|    policy_loss        | -0.0802  |\n",
      "|    value_loss         | 0.0615   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.83e+03 |\n",
      "|    ep_rew_mean        | -10.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 55000    |\n",
      "|    time_elapsed       | 5356     |\n",
      "|    total_timesteps    | 4400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54999    |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | -10.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 55100    |\n",
      "|    time_elapsed       | 5363     |\n",
      "|    total_timesteps    | 4408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55099    |\n",
      "|    policy_loss        | -0.0956  |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | -10.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 822      |\n",
      "|    iterations         | 55200    |\n",
      "|    time_elapsed       | 5369     |\n",
      "|    total_timesteps    | 4416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55199    |\n",
      "|    policy_loss        | 0.00479  |\n",
      "|    value_loss         | 0.0332   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4423584, episode_reward=-7.20 +/- 6.31\n",
      "Episode length: 11080.00 +/- 2104.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | -7.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4423584  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55294    |\n",
      "|    policy_loss        | -0.0776  |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.75e+03 |\n",
      "|    ep_rew_mean        | -10.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 55300    |\n",
      "|    time_elapsed       | 5396     |\n",
      "|    total_timesteps    | 4424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55299    |\n",
      "|    policy_loss        | 0.00244  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.76e+03 |\n",
      "|    ep_rew_mean        | -10.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 55400    |\n",
      "|    time_elapsed       | 5403     |\n",
      "|    total_timesteps    | 4432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55399    |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.73e+03 |\n",
      "|    ep_rew_mean        | -10.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 55500    |\n",
      "|    time_elapsed       | 5409     |\n",
      "|    total_timesteps    | 4440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55499    |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.81e+03 |\n",
      "|    ep_rew_mean        | -10.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 821      |\n",
      "|    iterations         | 55600    |\n",
      "|    time_elapsed       | 5416     |\n",
      "|    total_timesteps    | 4448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | 0.0554   |\n",
      "|    value_loss         | 0.0478   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4448576, episode_reward=-12.20 +/- 4.96\n",
      "Episode length: 10016.80 +/- 1748.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | -12.2    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4448576  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55607    |\n",
      "|    policy_loss        | 0.0202   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.89e+03 |\n",
      "|    ep_rew_mean        | -10.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 55700    |\n",
      "|    time_elapsed       | 5441     |\n",
      "|    total_timesteps    | 4456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | 0.00836  |\n",
      "|    value_loss         | 0.0569   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.94e+03 |\n",
      "|    ep_rew_mean        | -9.95    |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 55800    |\n",
      "|    time_elapsed       | 5448     |\n",
      "|    total_timesteps    | 4464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55799    |\n",
      "|    policy_loss        | -0.0497  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.97e+03 |\n",
      "|    ep_rew_mean        | -9.92    |\n",
      "| time/                 |          |\n",
      "|    fps                | 819      |\n",
      "|    iterations         | 55900    |\n",
      "|    time_elapsed       | 5454     |\n",
      "|    total_timesteps    | 4472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55899    |\n",
      "|    policy_loss        | -0.0839  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4473568, episode_reward=-11.80 +/- 5.42\n",
      "Episode length: 8272.60 +/- 1495.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.27e+03 |\n",
      "|    mean_reward        | -11.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4473568  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55919    |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.97e+03 |\n",
      "|    ep_rew_mean        | -9.93    |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 56000    |\n",
      "|    time_elapsed       | 5476     |\n",
      "|    total_timesteps    | 4480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55999    |\n",
      "|    policy_loss        | 0.0736   |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | -9.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 56100    |\n",
      "|    time_elapsed       | 5483     |\n",
      "|    total_timesteps    | 4488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56099    |\n",
      "|    policy_loss        | 0.00701  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | -9.67    |\n",
      "| time/                 |          |\n",
      "|    fps                | 818      |\n",
      "|    iterations         | 56200    |\n",
      "|    time_elapsed       | 5489     |\n",
      "|    total_timesteps    | 4496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56199    |\n",
      "|    policy_loss        | -0.0756  |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4498560, episode_reward=-4.20 +/- 5.27\n",
      "Episode length: 11709.60 +/- 1419.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | -4.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4498560  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56231    |\n",
      "|    policy_loss        | 0.0306   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+04 |\n",
      "|    ep_rew_mean        | -9.49    |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 56300    |\n",
      "|    time_elapsed       | 5518     |\n",
      "|    total_timesteps    | 4504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56299    |\n",
      "|    policy_loss        | -0.0876  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | -9.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 816      |\n",
      "|    iterations         | 56400    |\n",
      "|    time_elapsed       | 5524     |\n",
      "|    total_timesteps    | 4512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56399    |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | -9.16    |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 56500    |\n",
      "|    time_elapsed       | 5531     |\n",
      "|    total_timesteps    | 4520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56499    |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4523552, episode_reward=-9.40 +/- 3.20\n",
      "Episode length: 10061.00 +/- 1349.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | -9.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4523552  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56544    |\n",
      "|    policy_loss        | 0.00315  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | -9.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 56600    |\n",
      "|    time_elapsed       | 5557     |\n",
      "|    total_timesteps    | 4528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | -0.0734  |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | -9.24    |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 56700    |\n",
      "|    time_elapsed       | 5563     |\n",
      "|    total_timesteps    | 4536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56699    |\n",
      "|    policy_loss        | -0.0833  |\n",
      "|    value_loss         | 0.0509   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | -9.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 815      |\n",
      "|    iterations         | 56800    |\n",
      "|    time_elapsed       | 5570     |\n",
      "|    total_timesteps    | 4544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56799    |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.0381   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4548544, episode_reward=-8.40 +/- 3.38\n",
      "Episode length: 10638.80 +/- 1249.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | -8.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4548544  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56856    |\n",
      "|    policy_loss        | 0.00548  |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | -9.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 56900    |\n",
      "|    time_elapsed       | 5596     |\n",
      "|    total_timesteps    | 4552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56899    |\n",
      "|    policy_loss        | 0.0629   |\n",
      "|    value_loss         | 0.0453   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | -9.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 813      |\n",
      "|    iterations         | 57000    |\n",
      "|    time_elapsed       | 5603     |\n",
      "|    total_timesteps    | 4560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | -0.134   |\n",
      "|    value_loss         | 0.06     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | -9.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 814      |\n",
      "|    iterations         | 57100    |\n",
      "|    time_elapsed       | 5609     |\n",
      "|    total_timesteps    | 4568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57099    |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4573536, episode_reward=-5.00 +/- 4.77\n",
      "Episode length: 11536.40 +/- 1415.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.15e+04 |\n",
      "|    mean_reward        | -5       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4573536  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57169    |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | -8.75    |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 57200    |\n",
      "|    time_elapsed       | 5637     |\n",
      "|    total_timesteps    | 4576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57199    |\n",
      "|    policy_loss        | 0.0321   |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | -8.51    |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 57300    |\n",
      "|    time_elapsed       | 5644     |\n",
      "|    total_timesteps    | 4584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57299    |\n",
      "|    policy_loss        | 0.0308   |\n",
      "|    value_loss         | 0.0243   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+04 |\n",
      "|    ep_rew_mean        | -8.61    |\n",
      "| time/                 |          |\n",
      "|    fps                | 812      |\n",
      "|    iterations         | 57400    |\n",
      "|    time_elapsed       | 5651     |\n",
      "|    total_timesteps    | 4592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57399    |\n",
      "|    policy_loss        | -0.0797  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4598528, episode_reward=-6.60 +/- 2.58\n",
      "Episode length: 10512.60 +/- 837.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | -6.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4598528  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57481    |\n",
      "|    policy_loss        | -0.0493  |\n",
      "|    value_loss         | 0.0546   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | -8.56    |\n",
      "| time/                 |          |\n",
      "|    fps                | 810      |\n",
      "|    iterations         | 57500    |\n",
      "|    time_elapsed       | 5676     |\n",
      "|    total_timesteps    | 4600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57499    |\n",
      "|    policy_loss        | -0.0797  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+04 |\n",
      "|    ep_rew_mean        | -8.44    |\n",
      "| time/                 |          |\n",
      "|    fps                | 810      |\n",
      "|    iterations         | 57600    |\n",
      "|    time_elapsed       | 5683     |\n",
      "|    total_timesteps    | 4608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57599    |\n",
      "|    policy_loss        | -0.0569  |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | -8.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 57700    |\n",
      "|    time_elapsed       | 5690     |\n",
      "|    total_timesteps    | 4616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57699    |\n",
      "|    policy_loss        | -0.0516  |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4623520, episode_reward=-5.40 +/- 2.58\n",
      "Episode length: 11745.60 +/- 1358.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | -5.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4623520  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57793    |\n",
      "|    policy_loss        | 0.00241  |\n",
      "|    value_loss         | 0.00856  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | -8.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 808      |\n",
      "|    iterations         | 57800    |\n",
      "|    time_elapsed       | 5718     |\n",
      "|    total_timesteps    | 4624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57799    |\n",
      "|    policy_loss        | 0.147    |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | -7.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 57900    |\n",
      "|    time_elapsed       | 5725     |\n",
      "|    total_timesteps    | 4632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57899    |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | -7.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 809      |\n",
      "|    iterations         | 58000    |\n",
      "|    time_elapsed       | 5732     |\n",
      "|    total_timesteps    | 4640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57999    |\n",
      "|    policy_loss        | -0.0822  |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08e+04  |\n",
      "|    ep_rew_mean        | -7.62     |\n",
      "| time/                 |           |\n",
      "|    fps                | 809       |\n",
      "|    iterations         | 58100     |\n",
      "|    time_elapsed       | 5738      |\n",
      "|    total_timesteps    | 4648000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0.751     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58099     |\n",
      "|    policy_loss        | -0.000308 |\n",
      "|    value_loss         | 0.0292    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=4648512, episode_reward=-6.40 +/- 1.85\n",
      "Episode length: 11114.80 +/- 859.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+04 |\n",
      "|    mean_reward        | -6.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4648512  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.778    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58106    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.034    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+04 |\n",
      "|    ep_rew_mean        | -7.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 58200    |\n",
      "|    time_elapsed       | 5766     |\n",
      "|    total_timesteps    | 4656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58199    |\n",
      "|    policy_loss        | -0.0128  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | -7.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 807      |\n",
      "|    iterations         | 58300    |\n",
      "|    time_elapsed       | 5772     |\n",
      "|    total_timesteps    | 4664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58299    |\n",
      "|    policy_loss        | -0.0244  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | -7.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 808      |\n",
      "|    iterations         | 58400    |\n",
      "|    time_elapsed       | 5779     |\n",
      "|    total_timesteps    | 4672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58399    |\n",
      "|    policy_loss        | 0.00411  |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4673504, episode_reward=-2.20 +/- 2.04\n",
      "Episode length: 13487.20 +/- 681.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | -2.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4673504  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58418    |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | -7.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 58500    |\n",
      "|    time_elapsed       | 5810     |\n",
      "|    total_timesteps    | 4680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58499    |\n",
      "|    policy_loss        | -0.0531  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | -6.79    |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 58600    |\n",
      "|    time_elapsed       | 5817     |\n",
      "|    total_timesteps    | 4688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58599    |\n",
      "|    policy_loss        | -0.0767  |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | -6.56    |\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 58700    |\n",
      "|    time_elapsed       | 5824     |\n",
      "|    total_timesteps    | 4696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58699    |\n",
      "|    policy_loss        | 0.0844   |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4698496, episode_reward=-7.80 +/- 1.47\n",
      "Episode length: 10576.60 +/- 833.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+04 |\n",
      "|    mean_reward        | -7.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4698496  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58731    |\n",
      "|    policy_loss        | -0.0664  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | -6.51    |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 58800    |\n",
      "|    time_elapsed       | 5849     |\n",
      "|    total_timesteps    | 4704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.741    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.0455   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | -6.56    |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 58900    |\n",
      "|    time_elapsed       | 5856     |\n",
      "|    total_timesteps    | 4712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58899    |\n",
      "|    policy_loss        | -0.0434  |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | -6.46    |\n",
      "| time/                 |          |\n",
      "|    fps                | 805      |\n",
      "|    iterations         | 59000    |\n",
      "|    time_elapsed       | 5863     |\n",
      "|    total_timesteps    | 4720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.581    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58999    |\n",
      "|    policy_loss        | -0.0387  |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4723488, episode_reward=-1.60 +/- 4.32\n",
      "Episode length: 12662.80 +/- 1657.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | -1.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4723488  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59043    |\n",
      "|    policy_loss        | -0.0642  |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12e+04 |\n",
      "|    ep_rew_mean        | -6.68    |\n",
      "| time/                 |          |\n",
      "|    fps                | 802      |\n",
      "|    iterations         | 59100    |\n",
      "|    time_elapsed       | 5893     |\n",
      "|    total_timesteps    | 4728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59099    |\n",
      "|    policy_loss        | -0.0601  |\n",
      "|    value_loss         | 0.0089   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | -6.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 802      |\n",
      "|    iterations         | 59200    |\n",
      "|    time_elapsed       | 5900     |\n",
      "|    total_timesteps    | 4736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | 0.0215   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+04 |\n",
      "|    ep_rew_mean        | -6.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 59300    |\n",
      "|    time_elapsed       | 5906     |\n",
      "|    total_timesteps    | 4744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59299    |\n",
      "|    policy_loss        | 0.0253   |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4748480, episode_reward=-3.80 +/- 5.11\n",
      "Episode length: 11553.00 +/- 2072.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | -3.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4748480  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59355    |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | -6.55    |\n",
      "| time/                 |          |\n",
      "|    fps                | 800      |\n",
      "|    iterations         | 59400    |\n",
      "|    time_elapsed       | 5934     |\n",
      "|    total_timesteps    | 4752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59399    |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | -6.49    |\n",
      "| time/                 |          |\n",
      "|    fps                | 801      |\n",
      "|    iterations         | 59500    |\n",
      "|    time_elapsed       | 5941     |\n",
      "|    total_timesteps    | 4760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59499    |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | -6.34    |\n",
      "| time/                 |          |\n",
      "|    fps                | 801      |\n",
      "|    iterations         | 59600    |\n",
      "|    time_elapsed       | 5947     |\n",
      "|    total_timesteps    | 4768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59599    |\n",
      "|    policy_loss        | -0.0271  |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4773472, episode_reward=-5.20 +/- 3.12\n",
      "Episode length: 12045.00 +/- 850.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.2e+04  |\n",
      "|    mean_reward        | -5.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4773472  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59668    |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | -6.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 799      |\n",
      "|    iterations         | 59700    |\n",
      "|    time_elapsed       | 5976     |\n",
      "|    total_timesteps    | 4776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59699    |\n",
      "|    policy_loss        | 0.00826  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | -6.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 799      |\n",
      "|    iterations         | 59800    |\n",
      "|    time_elapsed       | 5983     |\n",
      "|    total_timesteps    | 4784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59799    |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | -6.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 799      |\n",
      "|    iterations         | 59900    |\n",
      "|    time_elapsed       | 5990     |\n",
      "|    total_timesteps    | 4792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59899    |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4798464, episode_reward=-3.80 +/- 1.94\n",
      "Episode length: 11803.80 +/- 1030.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.18e+04 |\n",
      "|    mean_reward        | -3.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4798464  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59980    |\n",
      "|    policy_loss        | -0.0576  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | -6.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 797      |\n",
      "|    iterations         | 60000    |\n",
      "|    time_elapsed       | 6018     |\n",
      "|    total_timesteps    | 4800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59999    |\n",
      "|    policy_loss        | 0.0274   |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | -5.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 798      |\n",
      "|    iterations         | 60100    |\n",
      "|    time_elapsed       | 6024     |\n",
      "|    total_timesteps    | 4808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60099    |\n",
      "|    policy_loss        | 0.044    |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | -5.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 798      |\n",
      "|    iterations         | 60200    |\n",
      "|    time_elapsed       | 6031     |\n",
      "|    total_timesteps    | 4816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60199    |\n",
      "|    policy_loss        | 0.00814  |\n",
      "|    value_loss         | 0.00973  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4823456, episode_reward=-5.20 +/- 3.37\n",
      "Episode length: 12680.40 +/- 1584.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.27e+04 |\n",
      "|    mean_reward        | -5.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4823456  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60293    |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.00955  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | -5.75    |\n",
      "| time/                 |          |\n",
      "|    fps                | 795      |\n",
      "|    iterations         | 60300    |\n",
      "|    time_elapsed       | 6061     |\n",
      "|    total_timesteps    | 4824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60299    |\n",
      "|    policy_loss        | 0.00703  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | -5.66    |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 60400    |\n",
      "|    time_elapsed       | 6068     |\n",
      "|    total_timesteps    | 4832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60399    |\n",
      "|    policy_loss        | 0.0343   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | -5.69    |\n",
      "| time/                 |          |\n",
      "|    fps                | 796      |\n",
      "|    iterations         | 60500    |\n",
      "|    time_elapsed       | 6074     |\n",
      "|    total_timesteps    | 4840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60499    |\n",
      "|    policy_loss        | -0.00144 |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | -5.69    |\n",
      "| time/                 |          |\n",
      "|    fps                | 797      |\n",
      "|    iterations         | 60600    |\n",
      "|    time_elapsed       | 6081     |\n",
      "|    total_timesteps    | 4848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60599    |\n",
      "|    policy_loss        | -0.0795  |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4848448, episode_reward=-4.00 +/- 4.34\n",
      "Episode length: 11640.80 +/- 1750.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+04 |\n",
      "|    mean_reward        | -4       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4848448  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60605    |\n",
      "|    policy_loss        | 0.0323   |\n",
      "|    value_loss         | 0.0251   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | -5.37    |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 60700    |\n",
      "|    time_elapsed       | 6109     |\n",
      "|    total_timesteps    | 4856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60699    |\n",
      "|    policy_loss        | -0.00497 |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | -5.34    |\n",
      "| time/                 |          |\n",
      "|    fps                | 795      |\n",
      "|    iterations         | 60800    |\n",
      "|    time_elapsed       | 6116     |\n",
      "|    total_timesteps    | 4864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60799    |\n",
      "|    policy_loss        | 0.102    |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | -5.27    |\n",
      "| time/                 |          |\n",
      "|    fps                | 795      |\n",
      "|    iterations         | 60900    |\n",
      "|    time_elapsed       | 6123     |\n",
      "|    total_timesteps    | 4872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | -0.0197  |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4873440, episode_reward=-7.80 +/- 4.49\n",
      "Episode length: 10117.20 +/- 1721.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+04 |\n",
      "|    mean_reward        | -7.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4873440  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60917    |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+04 |\n",
      "|    ep_rew_mean        | -5.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 793      |\n",
      "|    iterations         | 61000    |\n",
      "|    time_elapsed       | 6148     |\n",
      "|    total_timesteps    | 4880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.67     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60999    |\n",
      "|    policy_loss        | 0.000407 |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | -5.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 61100    |\n",
      "|    time_elapsed       | 6154     |\n",
      "|    total_timesteps    | 4888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61099    |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.0375   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | -5.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 794      |\n",
      "|    iterations         | 61200    |\n",
      "|    time_elapsed       | 6161     |\n",
      "|    total_timesteps    | 4896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.516    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61199    |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4898432, episode_reward=1.00 +/- 1.90\n",
      "Episode length: 13907.80 +/- 853.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 1        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4898432  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61230    |\n",
      "|    policy_loss        | 0.0581   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | -5.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 791      |\n",
      "|    iterations         | 61300    |\n",
      "|    time_elapsed       | 6193     |\n",
      "|    total_timesteps    | 4904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61299    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | -5.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 792      |\n",
      "|    iterations         | 61400    |\n",
      "|    time_elapsed       | 6200     |\n",
      "|    total_timesteps    | 4912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61399    |\n",
      "|    policy_loss        | -0.00479 |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | -4.91    |\n",
      "| time/                 |          |\n",
      "|    fps                | 792      |\n",
      "|    iterations         | 61500    |\n",
      "|    time_elapsed       | 6206     |\n",
      "|    total_timesteps    | 4920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61499    |\n",
      "|    policy_loss        | 0.0227   |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4923424, episode_reward=-1.80 +/- 3.43\n",
      "Episode length: 12790.40 +/- 465.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | -1.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4923424  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61542    |\n",
      "|    policy_loss        | 0.094    |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+04  |\n",
      "|    ep_rew_mean        | -4.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 790      |\n",
      "|    iterations         | 61600    |\n",
      "|    time_elapsed       | 6236     |\n",
      "|    total_timesteps    | 4928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61599    |\n",
      "|    policy_loss        | -0.0461  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 790      |\n",
      "|    iterations         | 61700    |\n",
      "|    time_elapsed       | 6243     |\n",
      "|    total_timesteps    | 4936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61699    |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.0466   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | -4.63    |\n",
      "| time/                 |          |\n",
      "|    fps                | 791      |\n",
      "|    iterations         | 61800    |\n",
      "|    time_elapsed       | 6250     |\n",
      "|    total_timesteps    | 4944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61799    |\n",
      "|    policy_loss        | 0.028    |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4948416, episode_reward=1.60 +/- 1.96\n",
      "Episode length: 12453.40 +/- 416.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+04 |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4948416  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61855    |\n",
      "|    policy_loss        | 0.0221   |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | -4.66    |\n",
      "| time/                 |          |\n",
      "|    fps                | 788      |\n",
      "|    iterations         | 61900    |\n",
      "|    time_elapsed       | 6279     |\n",
      "|    total_timesteps    | 4952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61899    |\n",
      "|    policy_loss        | -0.0595  |\n",
      "|    value_loss         | 0.0444   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | -4.59    |\n",
      "| time/                 |          |\n",
      "|    fps                | 789      |\n",
      "|    iterations         | 62000    |\n",
      "|    time_elapsed       | 6286     |\n",
      "|    total_timesteps    | 4960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61999    |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | -4.51    |\n",
      "| time/                 |          |\n",
      "|    fps                | 789      |\n",
      "|    iterations         | 62100    |\n",
      "|    time_elapsed       | 6293     |\n",
      "|    total_timesteps    | 4968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62099    |\n",
      "|    policy_loss        | -0.0708  |\n",
      "|    value_loss         | 0.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4973408, episode_reward=-2.80 +/- 1.17\n",
      "Episode length: 12771.40 +/- 890.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | -2.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4973408  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62167    |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | -4.35    |\n",
      "| time/                 |          |\n",
      "|    fps                | 786      |\n",
      "|    iterations         | 62200    |\n",
      "|    time_elapsed       | 6323     |\n",
      "|    total_timesteps    | 4976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | -0.0239  |\n",
      "|    value_loss         | 0.00983  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | -4.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 787      |\n",
      "|    iterations         | 62300    |\n",
      "|    time_elapsed       | 6329     |\n",
      "|    total_timesteps    | 4984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.434    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62299    |\n",
      "|    policy_loss        | -0.0213  |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | -4.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 787      |\n",
      "|    iterations         | 62400    |\n",
      "|    time_elapsed       | 6336     |\n",
      "|    total_timesteps    | 4992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62399    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4998400, episode_reward=0.60 +/- 1.50\n",
      "Episode length: 13478.00 +/- 1228.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 0.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4998400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62479    |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | -4.16    |\n",
      "| time/                 |          |\n",
      "|    fps                | 785      |\n",
      "|    iterations         | 62500    |\n",
      "|    time_elapsed       | 6367     |\n",
      "|    total_timesteps    | 5000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62499    |\n",
      "|    policy_loss        | 0.049    |\n",
      "|    value_loss         | 0.00859  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | -3.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 785      |\n",
      "|    iterations         | 62600    |\n",
      "|    time_elapsed       | 6374     |\n",
      "|    total_timesteps    | 5008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62599    |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | -3.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 786      |\n",
      "|    iterations         | 62700    |\n",
      "|    time_elapsed       | 6381     |\n",
      "|    total_timesteps    | 5016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62699    |\n",
      "|    policy_loss        | 0.0497   |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5023392, episode_reward=-2.00 +/- 2.68\n",
      "Episode length: 12914.80 +/- 332.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | -2       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5023392  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62792    |\n",
      "|    policy_loss        | -0.00552 |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24e+04 |\n",
      "|    ep_rew_mean        | -3.63    |\n",
      "| time/                 |          |\n",
      "|    fps                | 783      |\n",
      "|    iterations         | 62800    |\n",
      "|    time_elapsed       | 6411     |\n",
      "|    total_timesteps    | 5024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62799    |\n",
      "|    policy_loss        | -0.0475  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | -3.46    |\n",
      "| time/                 |          |\n",
      "|    fps                | 784      |\n",
      "|    iterations         | 62900    |\n",
      "|    time_elapsed       | 6418     |\n",
      "|    total_timesteps    | 5032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62899    |\n",
      "|    policy_loss        | 0.0466   |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | -3.51    |\n",
      "| time/                 |          |\n",
      "|    fps                | 784      |\n",
      "|    iterations         | 63000    |\n",
      "|    time_elapsed       | 6424     |\n",
      "|    total_timesteps    | 5040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62999    |\n",
      "|    policy_loss        | -0.00722 |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | -3.33    |\n",
      "| time/                 |          |\n",
      "|    fps                | 784      |\n",
      "|    iterations         | 63100    |\n",
      "|    time_elapsed       | 6431     |\n",
      "|    total_timesteps    | 5048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63099    |\n",
      "|    policy_loss        | -0.0342  |\n",
      "|    value_loss         | 0.00942  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5048384, episode_reward=1.80 +/- 3.31\n",
      "Episode length: 12781.60 +/- 1125.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 1.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5048384  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63104    |\n",
      "|    policy_loss        | -0.0631  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.26e+04  |\n",
      "|    ep_rew_mean        | -2.92     |\n",
      "| time/                 |           |\n",
      "|    fps                | 782       |\n",
      "|    iterations         | 63200     |\n",
      "|    time_elapsed       | 6461      |\n",
      "|    total_timesteps    | 5056000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63199     |\n",
      "|    policy_loss        | -0.000375 |\n",
      "|    value_loss         | 0.00701   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | -2.99    |\n",
      "| time/                 |          |\n",
      "|    fps                | 782      |\n",
      "|    iterations         | 63300    |\n",
      "|    time_elapsed       | 6468     |\n",
      "|    total_timesteps    | 5064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63299    |\n",
      "|    policy_loss        | -0.0112  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | -2.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 783      |\n",
      "|    iterations         | 63400    |\n",
      "|    time_elapsed       | 6474     |\n",
      "|    total_timesteps    | 5072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.769    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63399    |\n",
      "|    policy_loss        | -0.072   |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5073376, episode_reward=-1.60 +/- 3.83\n",
      "Episode length: 13195.40 +/- 2115.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.32e+04 |\n",
      "|    mean_reward        | -1.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5073376  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63417    |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.0214   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | -2.77    |\n",
      "| time/                 |          |\n",
      "|    fps                | 780      |\n",
      "|    iterations         | 63500    |\n",
      "|    time_elapsed       | 6505     |\n",
      "|    total_timesteps    | 5080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | -0.0269  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | -2.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 781      |\n",
      "|    iterations         | 63600    |\n",
      "|    time_elapsed       | 6512     |\n",
      "|    total_timesteps    | 5088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63599    |\n",
      "|    policy_loss        | 0.0349   |\n",
      "|    value_loss         | 0.0474   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+04 |\n",
      "|    ep_rew_mean        | -2.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 781      |\n",
      "|    iterations         | 63700    |\n",
      "|    time_elapsed       | 6519     |\n",
      "|    total_timesteps    | 5096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63699    |\n",
      "|    policy_loss        | -0.0275  |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5098368, episode_reward=2.20 +/- 1.83\n",
      "Episode length: 13547.60 +/- 859.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 2.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5098368  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63729    |\n",
      "|    policy_loss        | -0.0275  |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | -2.63    |\n",
      "| time/                 |          |\n",
      "|    fps                | 779      |\n",
      "|    iterations         | 63800    |\n",
      "|    time_elapsed       | 6550     |\n",
      "|    total_timesteps    | 5104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63799    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | -2.32    |\n",
      "| time/                 |          |\n",
      "|    fps                | 779      |\n",
      "|    iterations         | 63900    |\n",
      "|    time_elapsed       | 6557     |\n",
      "|    total_timesteps    | 5112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63899    |\n",
      "|    policy_loss        | 0.031    |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | -2.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 780      |\n",
      "|    iterations         | 64000    |\n",
      "|    time_elapsed       | 6563     |\n",
      "|    total_timesteps    | 5120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63999    |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5123360, episode_reward=4.80 +/- 3.66\n",
      "Episode length: 13078.80 +/- 1473.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.31e+04 |\n",
      "|    mean_reward        | 4.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5123360  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64041    |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | -2.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 777      |\n",
      "|    iterations         | 64100    |\n",
      "|    time_elapsed       | 6594     |\n",
      "|    total_timesteps    | 5128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64099    |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | -2.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 778      |\n",
      "|    iterations         | 64200    |\n",
      "|    time_elapsed       | 6601     |\n",
      "|    total_timesteps    | 5136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64199    |\n",
      "|    policy_loss        | 0.0037   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | -1.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 778      |\n",
      "|    iterations         | 64300    |\n",
      "|    time_elapsed       | 6607     |\n",
      "|    total_timesteps    | 5144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64299    |\n",
      "|    policy_loss        | 0.000354 |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5148352, episode_reward=0.00 +/- 3.63\n",
      "Episode length: 12833.20 +/- 887.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.28e+04 |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5148352  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64354    |\n",
      "|    policy_loss        | 0.0693   |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+04 |\n",
      "|    ep_rew_mean        | -1.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 776      |\n",
      "|    iterations         | 64400    |\n",
      "|    time_elapsed       | 6637     |\n",
      "|    total_timesteps    | 5152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64399    |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | -1.58    |\n",
      "| time/                 |          |\n",
      "|    fps                | 776      |\n",
      "|    iterations         | 64500    |\n",
      "|    time_elapsed       | 6644     |\n",
      "|    total_timesteps    | 5160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64499    |\n",
      "|    policy_loss        | 0.0604   |\n",
      "|    value_loss         | 0.0338   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | -1.41    |\n",
      "| time/                 |          |\n",
      "|    fps                | 777      |\n",
      "|    iterations         | 64600    |\n",
      "|    time_elapsed       | 6651     |\n",
      "|    total_timesteps    | 5168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.616    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64599    |\n",
      "|    policy_loss        | 0.0501   |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5173344, episode_reward=5.00 +/- 2.76\n",
      "Episode length: 13251.40 +/- 1234.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 5        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5173344  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64666    |\n",
      "|    policy_loss        | -0.0246  |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.31e+04 |\n",
      "|    ep_rew_mean        | -1.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 774      |\n",
      "|    iterations         | 64700    |\n",
      "|    time_elapsed       | 6682     |\n",
      "|    total_timesteps    | 5176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64699    |\n",
      "|    policy_loss        | 0.0688   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | -1.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 775      |\n",
      "|    iterations         | 64800    |\n",
      "|    time_elapsed       | 6688     |\n",
      "|    total_timesteps    | 5184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64799    |\n",
      "|    policy_loss        | 0.02     |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | -0.99    |\n",
      "| time/                 |          |\n",
      "|    fps                | 775      |\n",
      "|    iterations         | 64900    |\n",
      "|    time_elapsed       | 6695     |\n",
      "|    total_timesteps    | 5192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64899    |\n",
      "|    policy_loss        | -0.0712  |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5198336, episode_reward=-0.80 +/- 2.48\n",
      "Episode length: 14753.00 +/- 1471.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+04 |\n",
      "|    mean_reward        | -0.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5198336  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64979    |\n",
      "|    policy_loss        | -0.0469  |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | -0.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 772      |\n",
      "|    iterations         | 65000    |\n",
      "|    time_elapsed       | 6729     |\n",
      "|    total_timesteps    | 5200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64999    |\n",
      "|    policy_loss        | 0.109    |\n",
      "|    value_loss         | 0.0401   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | -0.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 773      |\n",
      "|    iterations         | 65100    |\n",
      "|    time_elapsed       | 6735     |\n",
      "|    total_timesteps    | 5208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65099    |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0069   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | -0.67    |\n",
      "| time/                 |          |\n",
      "|    fps                | 773      |\n",
      "|    iterations         | 65200    |\n",
      "|    time_elapsed       | 6742     |\n",
      "|    total_timesteps    | 5216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65199    |\n",
      "|    policy_loss        | 0.00509  |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5223328, episode_reward=2.80 +/- 1.33\n",
      "Episode length: 12917.60 +/- 1525.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.29e+04 |\n",
      "|    mean_reward        | 2.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5223328  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65291    |\n",
      "|    policy_loss        | -0.0999  |\n",
      "|    value_loss         | 0.0319   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | -0.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 771      |\n",
      "|    iterations         | 65300    |\n",
      "|    time_elapsed       | 6772     |\n",
      "|    total_timesteps    | 5224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65299    |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 771      |\n",
      "|    iterations         | 65400    |\n",
      "|    time_elapsed       | 6779     |\n",
      "|    total_timesteps    | 5232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65399    |\n",
      "|    policy_loss        | 0.00822  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 772      |\n",
      "|    iterations         | 65500    |\n",
      "|    time_elapsed       | 6786     |\n",
      "|    total_timesteps    | 5240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65499    |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | -0.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 772      |\n",
      "|    iterations         | 65600    |\n",
      "|    time_elapsed       | 6792     |\n",
      "|    total_timesteps    | 5248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65599    |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5248320, episode_reward=6.00 +/- 3.63\n",
      "Episode length: 13508.00 +/- 2042.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.35e+04 |\n",
      "|    mean_reward        | 6        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5248320  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65603    |\n",
      "|    policy_loss        | 0.0495   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 770      |\n",
      "|    iterations         | 65700    |\n",
      "|    time_elapsed       | 6824     |\n",
      "|    total_timesteps    | 5256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65699    |\n",
      "|    policy_loss        | 0.0926   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.3      |\n",
      "| time/                 |          |\n",
      "|    fps                | 770      |\n",
      "|    iterations         | 65800    |\n",
      "|    time_elapsed       | 6830     |\n",
      "|    total_timesteps    | 5264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65799    |\n",
      "|    policy_loss        | 0.00158  |\n",
      "|    value_loss         | 0.00532  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 771      |\n",
      "|    iterations         | 65900    |\n",
      "|    time_elapsed       | 6837     |\n",
      "|    total_timesteps    | 5272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65899    |\n",
      "|    policy_loss        | -0.0848  |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5273312, episode_reward=4.00 +/- 4.56\n",
      "Episode length: 13308.00 +/- 2619.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.33e+04 |\n",
      "|    mean_reward        | 4        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5273312  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65916    |\n",
      "|    policy_loss        | -0.0287  |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.47     |\n",
      "| time/                 |          |\n",
      "|    fps                | 768      |\n",
      "|    iterations         | 66000    |\n",
      "|    time_elapsed       | 6868     |\n",
      "|    total_timesteps    | 5280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65999    |\n",
      "|    policy_loss        | -0.0274  |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.68     |\n",
      "| time/                 |          |\n",
      "|    fps                | 769      |\n",
      "|    iterations         | 66100    |\n",
      "|    time_elapsed       | 6874     |\n",
      "|    total_timesteps    | 5288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | -0.0619  |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.78     |\n",
      "| time/                 |          |\n",
      "|    fps                | 769      |\n",
      "|    iterations         | 66200    |\n",
      "|    time_elapsed       | 6881     |\n",
      "|    total_timesteps    | 5296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66199    |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5298304, episode_reward=2.00 +/- 4.15\n",
      "Episode length: 13940.60 +/- 1086.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.39e+04 |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5298304  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66228    |\n",
      "|    policy_loss        | 0.017    |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.84     |\n",
      "| time/                 |          |\n",
      "|    fps                | 767      |\n",
      "|    iterations         | 66300    |\n",
      "|    time_elapsed       | 6913     |\n",
      "|    total_timesteps    | 5304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66299    |\n",
      "|    policy_loss        | -0.0564  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 0.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 767      |\n",
      "|    iterations         | 66400    |\n",
      "|    time_elapsed       | 6920     |\n",
      "|    total_timesteps    | 5312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66399    |\n",
      "|    policy_loss        | -0.00375 |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 1.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 768      |\n",
      "|    iterations         | 66500    |\n",
      "|    time_elapsed       | 6926     |\n",
      "|    total_timesteps    | 5320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | -0.0976  |\n",
      "|    value_loss         | 0.0393   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5323296, episode_reward=0.40 +/- 2.87\n",
      "Episode length: 14451.60 +/- 949.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.45e+04 |\n",
      "|    mean_reward        | 0.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5323296  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66541    |\n",
      "|    policy_loss        | -0.075   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 1.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 765      |\n",
      "|    iterations         | 66600    |\n",
      "|    time_elapsed       | 6959     |\n",
      "|    total_timesteps    | 5328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66599    |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 1.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 765      |\n",
      "|    iterations         | 66700    |\n",
      "|    time_elapsed       | 6966     |\n",
      "|    total_timesteps    | 5336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66699    |\n",
      "|    policy_loss        | 0.0232   |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 1.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 766      |\n",
      "|    iterations         | 66800    |\n",
      "|    time_elapsed       | 6972     |\n",
      "|    total_timesteps    | 5344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5348288, episode_reward=1.40 +/- 2.06\n",
      "Episode length: 13020.80 +/- 494.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.3e+04  |\n",
      "|    mean_reward        | 1.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5348288  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66853    |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 1.82     |\n",
      "| time/                 |          |\n",
      "|    fps                | 764      |\n",
      "|    iterations         | 66900    |\n",
      "|    time_elapsed       | 7003     |\n",
      "|    total_timesteps    | 5352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66899    |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 764      |\n",
      "|    iterations         | 67000    |\n",
      "|    time_elapsed       | 7009     |\n",
      "|    total_timesteps    | 5360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66999    |\n",
      "|    policy_loss        | -0.00419 |\n",
      "|    value_loss         | 0.00794  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 765      |\n",
      "|    iterations         | 67100    |\n",
      "|    time_elapsed       | 7016     |\n",
      "|    total_timesteps    | 5368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67099    |\n",
      "|    policy_loss        | -0.0528  |\n",
      "|    value_loss         | 0.0196   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5373280, episode_reward=2.00 +/- 4.82\n",
      "Episode length: 14823.60 +/- 2360.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.48e+04 |\n",
      "|    mean_reward        | 2        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5373280  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67165    |\n",
      "|    policy_loss        | 0.0403   |\n",
      "|    value_loss         | 0.00877  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 762      |\n",
      "|    iterations         | 67200    |\n",
      "|    time_elapsed       | 7050     |\n",
      "|    total_timesteps    | 5376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67199    |\n",
      "|    policy_loss        | 0.0486   |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.47     |\n",
      "| time/                 |          |\n",
      "|    fps                | 762      |\n",
      "|    iterations         | 67300    |\n",
      "|    time_elapsed       | 7056     |\n",
      "|    total_timesteps    | 5384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.573    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67299    |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 763      |\n",
      "|    iterations         | 67400    |\n",
      "|    time_elapsed       | 7063     |\n",
      "|    total_timesteps    | 5392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67399    |\n",
      "|    policy_loss        | -0.0462  |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5398272, episode_reward=8.00 +/- 5.76\n",
      "Episode length: 12073.60 +/- 1507.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.21e+04 |\n",
      "|    mean_reward        | 8        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5398272  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67478    |\n",
      "|    policy_loss        | -0.0578  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 67500    |\n",
      "|    time_elapsed       | 7092     |\n",
      "|    total_timesteps    | 5400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67499    |\n",
      "|    policy_loss        | 0.0341   |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.68     |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 67600    |\n",
      "|    time_elapsed       | 7098     |\n",
      "|    total_timesteps    | 5408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67599    |\n",
      "|    policy_loss        | 0.0752   |\n",
      "|    value_loss         | 0.0246   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.87     |\n",
      "| time/                 |          |\n",
      "|    fps                | 762      |\n",
      "|    iterations         | 67700    |\n",
      "|    time_elapsed       | 7105     |\n",
      "|    total_timesteps    | 5416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67699    |\n",
      "|    policy_loss        | 0.043    |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5423264, episode_reward=3.40 +/- 1.62\n",
      "Episode length: 13590.80 +/- 1208.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.36e+04 |\n",
      "|    mean_reward        | 3.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5423264  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67790    |\n",
      "|    policy_loss        | -0.0596  |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 759      |\n",
      "|    iterations         | 67800    |\n",
      "|    time_elapsed       | 7136     |\n",
      "|    total_timesteps    | 5424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | 0.0948   |\n",
      "|    value_loss         | 0.0482   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 2.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 760      |\n",
      "|    iterations         | 67900    |\n",
      "|    time_elapsed       | 7143     |\n",
      "|    total_timesteps    | 5432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67899    |\n",
      "|    policy_loss        | -0.0805  |\n",
      "|    value_loss         | 0.0382   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 2.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 760      |\n",
      "|    iterations         | 68000    |\n",
      "|    time_elapsed       | 7150     |\n",
      "|    total_timesteps    | 5440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67999    |\n",
      "|    policy_loss        | -0.0716  |\n",
      "|    value_loss         | 0.0267   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.13     |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 68100    |\n",
      "|    time_elapsed       | 7156     |\n",
      "|    total_timesteps    | 5448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68099    |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5448256, episode_reward=4.60 +/- 3.38\n",
      "Episode length: 13384.00 +/- 1896.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 4.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5448256  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68103    |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.031    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 759      |\n",
      "|    iterations         | 68200    |\n",
      "|    time_elapsed       | 7187     |\n",
      "|    total_timesteps    | 5456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68199    |\n",
      "|    policy_loss        | 0.00657  |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.25     |\n",
      "| time/                 |          |\n",
      "|    fps                | 759      |\n",
      "|    iterations         | 68300    |\n",
      "|    time_elapsed       | 7194     |\n",
      "|    total_timesteps    | 5464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68299    |\n",
      "|    policy_loss        | 0.0625   |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.35     |\n",
      "| time/                 |          |\n",
      "|    fps                | 759      |\n",
      "|    iterations         | 68400    |\n",
      "|    time_elapsed       | 7201     |\n",
      "|    total_timesteps    | 5472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68399    |\n",
      "|    policy_loss        | 0.149    |\n",
      "|    value_loss         | 0.0383   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5473248, episode_reward=4.20 +/- 1.17\n",
      "Episode length: 13414.80 +/- 1285.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.34e+04 |\n",
      "|    mean_reward        | 4.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5473248  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68415    |\n",
      "|    policy_loss        | 0.0183   |\n",
      "|    value_loss         | 0.0198   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 757      |\n",
      "|    iterations         | 68500    |\n",
      "|    time_elapsed       | 7232     |\n",
      "|    total_timesteps    | 5480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68499    |\n",
      "|    policy_loss        | -0.0341  |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 3.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 758      |\n",
      "|    iterations         | 68600    |\n",
      "|    time_elapsed       | 7238     |\n",
      "|    total_timesteps    | 5488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68599    |\n",
      "|    policy_loss        | -0.02    |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34e+04 |\n",
      "|    ep_rew_mean        | 3.76     |\n",
      "| time/                 |          |\n",
      "|    fps                | 758      |\n",
      "|    iterations         | 68700    |\n",
      "|    time_elapsed       | 7245     |\n",
      "|    total_timesteps    | 5496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68699    |\n",
      "|    policy_loss        | 0.0375   |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5498240, episode_reward=8.40 +/- 2.94\n",
      "Episode length: 11902.80 +/- 1583.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+04 |\n",
      "|    mean_reward        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5498240  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68727    |\n",
      "|    policy_loss        | 0.0386   |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 3.71     |\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 68800    |\n",
      "|    time_elapsed       | 7273     |\n",
      "|    total_timesteps    | 5504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68799    |\n",
      "|    policy_loss        | 0.0718   |\n",
      "|    value_loss         | 0.0609   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 3.76     |\n",
      "| time/                 |          |\n",
      "|    fps                | 757      |\n",
      "|    iterations         | 68900    |\n",
      "|    time_elapsed       | 7280     |\n",
      "|    total_timesteps    | 5512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.658    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68899    |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33e+04 |\n",
      "|    ep_rew_mean        | 3.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 757      |\n",
      "|    iterations         | 69000    |\n",
      "|    time_elapsed       | 7287     |\n",
      "|    total_timesteps    | 5520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68999    |\n",
      "|    policy_loss        | 0.0496   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5523232, episode_reward=10.00 +/- 4.98\n",
      "Episode length: 10692.60 +/- 1516.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+04 |\n",
      "|    mean_reward        | 10       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5523232  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69040    |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.32e+04 |\n",
      "|    ep_rew_mean        | 4.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 755      |\n",
      "|    iterations         | 69100    |\n",
      "|    time_elapsed       | 7313     |\n",
      "|    total_timesteps    | 5528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69099    |\n",
      "|    policy_loss        | -0.00435 |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.31e+04 |\n",
      "|    ep_rew_mean        | 4.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 69200    |\n",
      "|    time_elapsed       | 7320     |\n",
      "|    total_timesteps    | 5536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69199    |\n",
      "|    policy_loss        | 0.00963  |\n",
      "|    value_loss         | 0.0246   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.31e+04 |\n",
      "|    ep_rew_mean        | 4.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 69300    |\n",
      "|    time_elapsed       | 7326     |\n",
      "|    total_timesteps    | 5544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69299    |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5548224, episode_reward=6.60 +/- 4.72\n",
      "Episode length: 11705.40 +/- 805.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+04 |\n",
      "|    mean_reward        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5548224  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.555    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69352    |\n",
      "|    policy_loss        | -0.0284  |\n",
      "|    value_loss         | 0.0635   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 4.89     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 69400    |\n",
      "|    time_elapsed       | 7355     |\n",
      "|    total_timesteps    | 5552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69399    |\n",
      "|    policy_loss        | -0.00796 |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+04  |\n",
      "|    ep_rew_mean        | 5.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 755      |\n",
      "|    iterations         | 69500    |\n",
      "|    time_elapsed       | 7361     |\n",
      "|    total_timesteps    | 5560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 5.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 755      |\n",
      "|    iterations         | 69600    |\n",
      "|    time_elapsed       | 7368     |\n",
      "|    total_timesteps    | 5568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69599    |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5573216, episode_reward=14.60 +/- 2.58\n",
      "Episode length: 8543.20 +/- 951.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.54e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5573216  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69665    |\n",
      "|    policy_loss        | -0.087   |\n",
      "|    value_loss         | 0.063    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.28e+04 |\n",
      "|    ep_rew_mean        | 5.75     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 69700    |\n",
      "|    time_elapsed       | 7390     |\n",
      "|    total_timesteps    | 5576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69699    |\n",
      "|    policy_loss        | 0.0463   |\n",
      "|    value_loss         | 0.0543   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+04 |\n",
      "|    ep_rew_mean        | 5.89     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 69800    |\n",
      "|    time_elapsed       | 7397     |\n",
      "|    total_timesteps    | 5584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69799    |\n",
      "|    policy_loss        | -0.0324  |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.25e+04 |\n",
      "|    ep_rew_mean        | 6.31     |\n",
      "| time/                 |          |\n",
      "|    fps                | 755      |\n",
      "|    iterations         | 69900    |\n",
      "|    time_elapsed       | 7404     |\n",
      "|    total_timesteps    | 5592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69899    |\n",
      "|    policy_loss        | 0.0966   |\n",
      "|    value_loss         | 0.0546   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5598208, episode_reward=12.60 +/- 6.15\n",
      "Episode length: 9881.80 +/- 2725.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.88e+03 |\n",
      "|    mean_reward        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5598208  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69977    |\n",
      "|    policy_loss        | -0.00177 |\n",
      "|    value_loss         | 0.0408   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.23e+04 |\n",
      "|    ep_rew_mean        | 6.67     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70000    |\n",
      "|    time_elapsed       | 7428     |\n",
      "|    total_timesteps    | 5600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69999    |\n",
      "|    policy_loss        | -0.0615  |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+04 |\n",
      "|    ep_rew_mean        | 6.93     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 70100    |\n",
      "|    time_elapsed       | 7435     |\n",
      "|    total_timesteps    | 5608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | -0.00743 |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+04 |\n",
      "|    ep_rew_mean        | 7.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 70200    |\n",
      "|    time_elapsed       | 7441     |\n",
      "|    total_timesteps    | 5616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70199    |\n",
      "|    policy_loss        | -0.0113  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5623200, episode_reward=8.20 +/- 4.35\n",
      "Episode length: 10462.40 +/- 1704.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+04 |\n",
      "|    mean_reward        | 8.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5623200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70289    |\n",
      "|    policy_loss        | 0.0454   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+04 |\n",
      "|    ep_rew_mean        | 7.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70300    |\n",
      "|    time_elapsed       | 7467     |\n",
      "|    total_timesteps    | 5624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.67     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70299    |\n",
      "|    policy_loss        | -0.0488  |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+04 |\n",
      "|    ep_rew_mean        | 7.75     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70400    |\n",
      "|    time_elapsed       | 7474     |\n",
      "|    total_timesteps    | 5632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70399    |\n",
      "|    policy_loss        | 0.0831   |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+04 |\n",
      "|    ep_rew_mean        | 7.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70500    |\n",
      "|    time_elapsed       | 7481     |\n",
      "|    total_timesteps    | 5640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70499    |\n",
      "|    policy_loss        | -0.0475  |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15e+04 |\n",
      "|    ep_rew_mean        | 8.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 70600    |\n",
      "|    time_elapsed       | 7487     |\n",
      "|    total_timesteps    | 5648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70599    |\n",
      "|    policy_loss        | -0.0398  |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5648192, episode_reward=16.40 +/- 2.73\n",
      "Episode length: 8329.80 +/- 1271.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.33e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5648192  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70602    |\n",
      "|    policy_loss        | 0.0269   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14e+04 |\n",
      "|    ep_rew_mean        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70700    |\n",
      "|    time_elapsed       | 7509     |\n",
      "|    total_timesteps    | 5656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.744    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70699    |\n",
      "|    policy_loss        | 0.109    |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11e+04 |\n",
      "|    ep_rew_mean        | 8.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70800    |\n",
      "|    time_elapsed       | 7516     |\n",
      "|    total_timesteps    | 5664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70799    |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1e+04  |\n",
      "|    ep_rew_mean        | 9.36     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 70900    |\n",
      "|    time_elapsed       | 7522     |\n",
      "|    total_timesteps    | 5672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70899    |\n",
      "|    policy_loss        | 0.00284  |\n",
      "|    value_loss         | 0.0239   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5673184, episode_reward=15.00 +/- 2.76\n",
      "Episode length: 8637.20 +/- 1027.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.64e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5673184  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70914    |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09e+04 |\n",
      "|    ep_rew_mean        | 9.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71000    |\n",
      "|    time_elapsed       | 7545     |\n",
      "|    total_timesteps    | 5680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70999    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07e+04 |\n",
      "|    ep_rew_mean        | 9.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 71100    |\n",
      "|    time_elapsed       | 7551     |\n",
      "|    total_timesteps    | 5688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71099    |\n",
      "|    policy_loss        | 0.0313   |\n",
      "|    value_loss         | 0.00829  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06e+04 |\n",
      "|    ep_rew_mean        | 10.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 71200    |\n",
      "|    time_elapsed       | 7558     |\n",
      "|    total_timesteps    | 5696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71199    |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5698176, episode_reward=16.60 +/- 5.31\n",
      "Episode length: 7590.80 +/- 1928.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.59e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5698176  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71227    |\n",
      "|    policy_loss        | 0.0618   |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03e+04 |\n",
      "|    ep_rew_mean        | 10.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71300    |\n",
      "|    time_elapsed       | 7579     |\n",
      "|    total_timesteps    | 5704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71299    |\n",
      "|    policy_loss        | -0.0979  |\n",
      "|    value_loss         | 0.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02e+04 |\n",
      "|    ep_rew_mean        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71400    |\n",
      "|    time_elapsed       | 7585     |\n",
      "|    total_timesteps    | 5712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71399    |\n",
      "|    policy_loss        | 0.0876   |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+04    |\n",
      "|    ep_rew_mean        | 11.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 753      |\n",
      "|    iterations         | 71500    |\n",
      "|    time_elapsed       | 7592     |\n",
      "|    total_timesteps    | 5720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71499    |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.00654  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5723168, episode_reward=12.60 +/- 5.28\n",
      "Episode length: 9909.00 +/- 2378.55\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 9.91e+03  |\n",
      "|    mean_reward        | 12.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 5723168   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0.941     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71539     |\n",
      "|    policy_loss        | -0.000127 |\n",
      "|    value_loss         | 0.00696   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.89e+03 |\n",
      "|    ep_rew_mean        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71600    |\n",
      "|    time_elapsed       | 7616     |\n",
      "|    total_timesteps    | 5728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71599    |\n",
      "|    policy_loss        | -0.0494  |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.6e+03  |\n",
      "|    ep_rew_mean        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71700    |\n",
      "|    time_elapsed       | 7623     |\n",
      "|    total_timesteps    | 5736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71699    |\n",
      "|    policy_loss        | -0.0316  |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.43e+03 |\n",
      "|    ep_rew_mean        | 13.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 71800    |\n",
      "|    time_elapsed       | 7629     |\n",
      "|    total_timesteps    | 5744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71799    |\n",
      "|    policy_loss        | -0.0487  |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5748160, episode_reward=15.20 +/- 6.68\n",
      "Episode length: 8620.60 +/- 3376.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.62e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5748160  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71851    |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.36e+03 |\n",
      "|    ep_rew_mean        | 13.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 71900    |\n",
      "|    time_elapsed       | 7651     |\n",
      "|    total_timesteps    | 5752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71899    |\n",
      "|    policy_loss        | 0.0445   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.23e+03 |\n",
      "|    ep_rew_mean        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 72000    |\n",
      "|    time_elapsed       | 7658     |\n",
      "|    total_timesteps    | 5760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71999    |\n",
      "|    policy_loss        | 0.00855  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.09e+03 |\n",
      "|    ep_rew_mean        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 72100    |\n",
      "|    time_elapsed       | 7664     |\n",
      "|    total_timesteps    | 5768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72099    |\n",
      "|    policy_loss        | -0.00542 |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5773152, episode_reward=18.60 +/- 1.36\n",
      "Episode length: 7372.80 +/- 775.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.37e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5773152  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72164    |\n",
      "|    policy_loss        | 0.0409   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.08e+03 |\n",
      "|    ep_rew_mean        | 14       |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 72200    |\n",
      "|    time_elapsed       | 7684     |\n",
      "|    total_timesteps    | 5776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72199    |\n",
      "|    policy_loss        | 0.0553   |\n",
      "|    value_loss         | 0.028    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.98e+03 |\n",
      "|    ep_rew_mean        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 72300    |\n",
      "|    time_elapsed       | 7691     |\n",
      "|    total_timesteps    | 5784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72299    |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.85e+03 |\n",
      "|    ep_rew_mean        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 72400    |\n",
      "|    time_elapsed       | 7697     |\n",
      "|    total_timesteps    | 5792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72399    |\n",
      "|    policy_loss        | 0.00303  |\n",
      "|    value_loss         | 0.00451  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5798144, episode_reward=16.80 +/- 2.14\n",
      "Episode length: 8400.20 +/- 408.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.4e+03  |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5798144  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72476    |\n",
      "|    policy_loss        | 0.0261   |\n",
      "|    value_loss         | 0.00962  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.8e+03  |\n",
      "|    ep_rew_mean        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 72500    |\n",
      "|    time_elapsed       | 7719     |\n",
      "|    total_timesteps    | 5800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72499    |\n",
      "|    policy_loss        | 0.0753   |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | 14.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 72600    |\n",
      "|    time_elapsed       | 7725     |\n",
      "|    total_timesteps    | 5808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72599    |\n",
      "|    policy_loss        | 0.00888  |\n",
      "|    value_loss         | 0.00501  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | 15       |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 72700    |\n",
      "|    time_elapsed       | 7732     |\n",
      "|    total_timesteps    | 5816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72699    |\n",
      "|    policy_loss        | 0.0519   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5823136, episode_reward=17.40 +/- 2.33\n",
      "Episode length: 7981.60 +/- 1277.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.98e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5823136  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72789    |\n",
      "|    policy_loss        | 0.00323  |\n",
      "|    value_loss         | 0.00445  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 15.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 72800    |\n",
      "|    time_elapsed       | 7753     |\n",
      "|    total_timesteps    | 5824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72799    |\n",
      "|    policy_loss        | -0.0203  |\n",
      "|    value_loss         | 0.00283  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 15.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 72900    |\n",
      "|    time_elapsed       | 7759     |\n",
      "|    total_timesteps    | 5832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72899    |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73000    |\n",
      "|    time_elapsed       | 7766     |\n",
      "|    total_timesteps    | 5840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72999    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 752      |\n",
      "|    iterations         | 73100    |\n",
      "|    time_elapsed       | 7773     |\n",
      "|    total_timesteps    | 5848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73099    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5848128, episode_reward=18.40 +/- 1.62\n",
      "Episode length: 8575.80 +/- 832.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.58e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5848128  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73101    |\n",
      "|    policy_loss        | -0.0953  |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 15.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73200    |\n",
      "|    time_elapsed       | 7795     |\n",
      "|    total_timesteps    | 5856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73199    |\n",
      "|    policy_loss        | -0.0597  |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 15.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73300    |\n",
      "|    time_elapsed       | 7802     |\n",
      "|    total_timesteps    | 5864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73299    |\n",
      "|    policy_loss        | -0.0029  |\n",
      "|    value_loss         | 0.00733  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73400    |\n",
      "|    time_elapsed       | 7809     |\n",
      "|    total_timesteps    | 5872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73399    |\n",
      "|    policy_loss        | 0.00502  |\n",
      "|    value_loss         | 0.00993  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5873120, episode_reward=15.60 +/- 2.80\n",
      "Episode length: 9381.80 +/- 1337.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.38e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5873120  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73413    |\n",
      "|    policy_loss        | 0.0281   |\n",
      "|    value_loss         | 0.00785  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 73500    |\n",
      "|    time_elapsed       | 7833     |\n",
      "|    total_timesteps    | 5880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73499    |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 16.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73600    |\n",
      "|    time_elapsed       | 7840     |\n",
      "|    total_timesteps    | 5888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73599    |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.00226  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 751      |\n",
      "|    iterations         | 73700    |\n",
      "|    time_elapsed       | 7846     |\n",
      "|    total_timesteps    | 5896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73699    |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.00547  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5898112, episode_reward=17.00 +/- 2.28\n",
      "Episode length: 8900.80 +/- 1178.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.9e+03  |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5898112  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73726    |\n",
      "|    policy_loss        | -0.00979 |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 16.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 73800    |\n",
      "|    time_elapsed       | 7869     |\n",
      "|    total_timesteps    | 5904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73799    |\n",
      "|    policy_loss        | -0.0581  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 73900    |\n",
      "|    time_elapsed       | 7876     |\n",
      "|    total_timesteps    | 5912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73899    |\n",
      "|    policy_loss        | 0.0251   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74000    |\n",
      "|    time_elapsed       | 7883     |\n",
      "|    total_timesteps    | 5920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73999    |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.00708  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5923104, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 7235.40 +/- 1063.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.24e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5923104  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74038    |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 16.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74100    |\n",
      "|    time_elapsed       | 7903     |\n",
      "|    total_timesteps    | 5928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74099    |\n",
      "|    policy_loss        | -0.00274 |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74200    |\n",
      "|    time_elapsed       | 7909     |\n",
      "|    total_timesteps    | 5936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74199    |\n",
      "|    policy_loss        | 0.00464  |\n",
      "|    value_loss         | 0.00759  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74300    |\n",
      "|    time_elapsed       | 7916     |\n",
      "|    total_timesteps    | 5944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74299    |\n",
      "|    policy_loss        | 0.00944  |\n",
      "|    value_loss         | 0.00636  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5948096, episode_reward=17.20 +/- 2.40\n",
      "Episode length: 9204.60 +/- 1382.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.2e+03  |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5948096  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74351    |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.00245  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 16.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 74400    |\n",
      "|    time_elapsed       | 7940     |\n",
      "|    total_timesteps    | 5952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.00322  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 74500    |\n",
      "|    time_elapsed       | 7946     |\n",
      "|    total_timesteps    | 5960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74499    |\n",
      "|    policy_loss        | -0.0775  |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74600    |\n",
      "|    time_elapsed       | 7953     |\n",
      "|    total_timesteps    | 5968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74599    |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.00651  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5973088, episode_reward=17.20 +/- 2.48\n",
      "Episode length: 7724.20 +/- 1041.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.72e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5973088  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74663    |\n",
      "|    policy_loss        | 0.0508   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 74700    |\n",
      "|    time_elapsed       | 7974     |\n",
      "|    total_timesteps    | 5976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74699    |\n",
      "|    policy_loss        | 0.0511   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 74800    |\n",
      "|    time_elapsed       | 7981     |\n",
      "|    total_timesteps    | 5984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74799    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 74900    |\n",
      "|    time_elapsed       | 7987     |\n",
      "|    total_timesteps    | 5992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74899    |\n",
      "|    policy_loss        | -0.00314 |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5998080, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 7993.20 +/- 615.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.99e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5998080  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74975    |\n",
      "|    policy_loss        | -0.0303  |\n",
      "|    value_loss         | 0.00669  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75000    |\n",
      "|    time_elapsed       | 8009     |\n",
      "|    total_timesteps    | 6000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74999    |\n",
      "|    policy_loss        | -0.00902 |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75100    |\n",
      "|    time_elapsed       | 8015     |\n",
      "|    total_timesteps    | 6008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75099    |\n",
      "|    policy_loss        | 0.00455  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75200    |\n",
      "|    time_elapsed       | 8022     |\n",
      "|    total_timesteps    | 6016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75199    |\n",
      "|    policy_loss        | 0.0022   |\n",
      "|    value_loss         | 0.00611  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6023072, episode_reward=17.20 +/- 1.94\n",
      "Episode length: 8867.20 +/- 411.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.87e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6023072  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75288    |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 75300    |\n",
      "|    time_elapsed       | 8045     |\n",
      "|    total_timesteps    | 6024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75299    |\n",
      "|    policy_loss        | -0.0834  |\n",
      "|    value_loss         | 0.042    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75400    |\n",
      "|    time_elapsed       | 8052     |\n",
      "|    total_timesteps    | 6032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75399    |\n",
      "|    policy_loss        | -0.00535 |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75500    |\n",
      "|    time_elapsed       | 8058     |\n",
      "|    total_timesteps    | 6040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75499    |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 16.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75600    |\n",
      "|    time_elapsed       | 8065     |\n",
      "|    total_timesteps    | 6048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75599    |\n",
      "|    policy_loss        | -0.0541  |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6048064, episode_reward=19.60 +/- 0.49\n",
      "Episode length: 7338.60 +/- 766.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.34e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6048064  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75600    |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.00836  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 75700    |\n",
      "|    time_elapsed       | 8085     |\n",
      "|    total_timesteps    | 6056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75699    |\n",
      "|    policy_loss        | 0.0042   |\n",
      "|    value_loss         | 0.00999  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75800    |\n",
      "|    time_elapsed       | 8092     |\n",
      "|    total_timesteps    | 6064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75799    |\n",
      "|    policy_loss        | 0.029    |\n",
      "|    value_loss         | 0.00243  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 75900    |\n",
      "|    time_elapsed       | 8099     |\n",
      "|    total_timesteps    | 6072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75899    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.00972  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6073056, episode_reward=19.80 +/- 0.40\n",
      "Episode length: 7704.00 +/- 787.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.7e+03  |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6073056  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75913    |\n",
      "|    policy_loss        | 0.0604   |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76000    |\n",
      "|    time_elapsed       | 8120     |\n",
      "|    total_timesteps    | 6080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75999    |\n",
      "|    policy_loss        | 0.00603  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 76100    |\n",
      "|    time_elapsed       | 8126     |\n",
      "|    total_timesteps    | 6088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76099    |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 76200    |\n",
      "|    time_elapsed       | 8133     |\n",
      "|    total_timesteps    | 6096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76199    |\n",
      "|    policy_loss        | -0.0189  |\n",
      "|    value_loss         | 0.00784  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6098048, episode_reward=18.60 +/- 1.50\n",
      "Episode length: 8430.20 +/- 195.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.43e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6098048  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76225    |\n",
      "|    policy_loss        | -0.0334  |\n",
      "|    value_loss         | 0.00345  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76300    |\n",
      "|    time_elapsed       | 8155     |\n",
      "|    total_timesteps    | 6104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76299    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76400    |\n",
      "|    time_elapsed       | 8162     |\n",
      "|    total_timesteps    | 6112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76399    |\n",
      "|    policy_loss        | -0.0338  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 749      |\n",
      "|    iterations         | 76500    |\n",
      "|    time_elapsed       | 8168     |\n",
      "|    total_timesteps    | 6120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76499    |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6123040, episode_reward=17.60 +/- 1.02\n",
      "Episode length: 8317.80 +/- 1194.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.32e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6123040  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76537    |\n",
      "|    policy_loss        | 0.0248   |\n",
      "|    value_loss         | 0.00743  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76600    |\n",
      "|    time_elapsed       | 8190     |\n",
      "|    total_timesteps    | 6128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76599    |\n",
      "|    policy_loss        | -0.0284  |\n",
      "|    value_loss         | 0.00379  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.63e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76700    |\n",
      "|    time_elapsed       | 8197     |\n",
      "|    total_timesteps    | 6136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76699    |\n",
      "|    policy_loss        | 0.00615  |\n",
      "|    value_loss         | 0.00799  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 76800    |\n",
      "|    time_elapsed       | 8204     |\n",
      "|    total_timesteps    | 6144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76799    |\n",
      "|    policy_loss        | 0.0483   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6148032, episode_reward=15.60 +/- 4.92\n",
      "Episode length: 8543.00 +/- 1807.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.54e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6148032  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76850    |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.00454  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 76900    |\n",
      "|    time_elapsed       | 8226     |\n",
      "|    total_timesteps    | 6152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76899    |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    value_loss         | 0.00737  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 77000    |\n",
      "|    time_elapsed       | 8232     |\n",
      "|    total_timesteps    | 6160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76999    |\n",
      "|    policy_loss        | 0.0276   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 77100    |\n",
      "|    time_elapsed       | 8239     |\n",
      "|    total_timesteps    | 6168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77099    |\n",
      "|    policy_loss        | 0.00447  |\n",
      "|    value_loss         | 0.0024   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6173024, episode_reward=17.60 +/- 1.96\n",
      "Episode length: 7877.00 +/- 1316.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.88e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6173024  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77162    |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 77200    |\n",
      "|    time_elapsed       | 8260     |\n",
      "|    total_timesteps    | 6176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77199    |\n",
      "|    policy_loss        | 0.0259   |\n",
      "|    value_loss         | 0.00495  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 77300    |\n",
      "|    time_elapsed       | 8267     |\n",
      "|    total_timesteps    | 6184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77299    |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.00685  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 748      |\n",
      "|    iterations         | 77400    |\n",
      "|    time_elapsed       | 8273     |\n",
      "|    total_timesteps    | 6192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77399    |\n",
      "|    policy_loss        | 0.00958  |\n",
      "|    value_loss         | 0.00634  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6198016, episode_reward=16.60 +/- 3.14\n",
      "Episode length: 9401.20 +/- 1349.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.4e+03  |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6198016  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77475    |\n",
      "|    policy_loss        | -0.0321  |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 77500    |\n",
      "|    time_elapsed       | 8297     |\n",
      "|    total_timesteps    | 6200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77499    |\n",
      "|    policy_loss        | -0.0347  |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 77600    |\n",
      "|    time_elapsed       | 8304     |\n",
      "|    total_timesteps    | 6208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | 0.0109   |\n",
      "|    value_loss         | 0.00409  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 77700    |\n",
      "|    time_elapsed       | 8311     |\n",
      "|    total_timesteps    | 6216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77699    |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.00431  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6223008, episode_reward=13.80 +/- 4.66\n",
      "Episode length: 9246.40 +/- 2719.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.25e+03 |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6223008  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77787    |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 77800    |\n",
      "|    time_elapsed       | 8334     |\n",
      "|    total_timesteps    | 6224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77799    |\n",
      "|    policy_loss        | -0.0811  |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 77900    |\n",
      "|    time_elapsed       | 8341     |\n",
      "|    total_timesteps    | 6232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77899    |\n",
      "|    policy_loss        | -0.0307  |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 78000    |\n",
      "|    time_elapsed       | 8348     |\n",
      "|    total_timesteps    | 6240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77999    |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.00735  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6248000, episode_reward=17.80 +/- 2.14\n",
      "Episode length: 8275.60 +/- 1284.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.28e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | 0.0442   |\n",
      "|    value_loss         | 0.0098   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.39e+03 |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 746      |\n",
      "|    iterations      | 78100    |\n",
      "|    time_elapsed    | 8369     |\n",
      "|    total_timesteps | 6248000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 78200    |\n",
      "|    time_elapsed       | 8376     |\n",
      "|    total_timesteps    | 6256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78199    |\n",
      "|    policy_loss        | 0.0568   |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 78300    |\n",
      "|    time_elapsed       | 8383     |\n",
      "|    total_timesteps    | 6264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | 0.0354   |\n",
      "|    value_loss         | 0.00311  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 78400    |\n",
      "|    time_elapsed       | 8389     |\n",
      "|    total_timesteps    | 6272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78399    |\n",
      "|    policy_loss        | 0.0194   |\n",
      "|    value_loss         | 0.0057   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6272992, episode_reward=18.40 +/- 1.02\n",
      "Episode length: 8327.40 +/- 996.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.33e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6272992  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78412    |\n",
      "|    policy_loss        | 0.0286   |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 78500    |\n",
      "|    time_elapsed       | 8411     |\n",
      "|    total_timesteps    | 6280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78499    |\n",
      "|    policy_loss        | 0.0726   |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 78600    |\n",
      "|    time_elapsed       | 8418     |\n",
      "|    total_timesteps    | 6288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78599    |\n",
      "|    policy_loss        | -0.0477  |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 78700    |\n",
      "|    time_elapsed       | 8424     |\n",
      "|    total_timesteps    | 6296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78699    |\n",
      "|    policy_loss        | -0.0135  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6297984, episode_reward=18.00 +/- 1.79\n",
      "Episode length: 7666.00 +/- 1134.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.67e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6297984  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78724    |\n",
      "|    policy_loss        | -0.00483 |\n",
      "|    value_loss         | 0.00288  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 78800    |\n",
      "|    time_elapsed       | 8445     |\n",
      "|    total_timesteps    | 6304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78799    |\n",
      "|    policy_loss        | -0.0642  |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 78900    |\n",
      "|    time_elapsed       | 8452     |\n",
      "|    total_timesteps    | 6312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78899    |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.00521  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 747      |\n",
      "|    iterations         | 79000    |\n",
      "|    time_elapsed       | 8458     |\n",
      "|    total_timesteps    | 6320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78999    |\n",
      "|    policy_loss        | 0.0187   |\n",
      "|    value_loss         | 0.00591  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6322976, episode_reward=16.60 +/- 2.58\n",
      "Episode length: 8489.20 +/- 1692.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.49e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6322976  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79037    |\n",
      "|    policy_loss        | -0.0491  |\n",
      "|    value_loss         | 0.00807  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79100    |\n",
      "|    time_elapsed       | 8480     |\n",
      "|    total_timesteps    | 6328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79099    |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.0044   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79200    |\n",
      "|    time_elapsed       | 8487     |\n",
      "|    total_timesteps    | 6336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79199    |\n",
      "|    policy_loss        | 0.00498  |\n",
      "|    value_loss         | 0.00674  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79300    |\n",
      "|    time_elapsed       | 8494     |\n",
      "|    total_timesteps    | 6344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | -0.00595 |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6347968, episode_reward=16.60 +/- 4.13\n",
      "Episode length: 8647.80 +/- 1400.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.65e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6347968  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79349    |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.00488  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 79400    |\n",
      "|    time_elapsed       | 8516     |\n",
      "|    total_timesteps    | 6352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79399    |\n",
      "|    policy_loss        | 0.00424  |\n",
      "|    value_loss         | 0.00854  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79500    |\n",
      "|    time_elapsed       | 8523     |\n",
      "|    total_timesteps    | 6360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79499    |\n",
      "|    policy_loss        | 0.0611   |\n",
      "|    value_loss         | 0.00661  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79600    |\n",
      "|    time_elapsed       | 8530     |\n",
      "|    total_timesteps    | 6368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79599    |\n",
      "|    policy_loss        | 0.00567  |\n",
      "|    value_loss         | 0.0351   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6372960, episode_reward=17.00 +/- 1.90\n",
      "Episode length: 9038.80 +/- 1178.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.04e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6372960  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79661    |\n",
      "|    policy_loss        | -0.0313  |\n",
      "|    value_loss         | 0.00733  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 79700    |\n",
      "|    time_elapsed       | 8553     |\n",
      "|    total_timesteps    | 6376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79699    |\n",
      "|    policy_loss        | -0.00164 |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 79800    |\n",
      "|    time_elapsed       | 8559     |\n",
      "|    total_timesteps    | 6384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79799    |\n",
      "|    policy_loss        | 0.0289   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 79900    |\n",
      "|    time_elapsed       | 8566     |\n",
      "|    total_timesteps    | 6392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79899    |\n",
      "|    policy_loss        | -0.039   |\n",
      "|    value_loss         | 0.00917  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6397952, episode_reward=19.00 +/- 1.26\n",
      "Episode length: 7584.60 +/- 718.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.58e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6397952  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79974    |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 80000    |\n",
      "|    time_elapsed       | 8587     |\n",
      "|    total_timesteps    | 6400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79999    |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 80100    |\n",
      "|    time_elapsed       | 8593     |\n",
      "|    total_timesteps    | 6408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80099    |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 746      |\n",
      "|    iterations         | 80200    |\n",
      "|    time_elapsed       | 8600     |\n",
      "|    total_timesteps    | 6416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80199    |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.00727  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6422944, episode_reward=12.00 +/- 6.23\n",
      "Episode length: 11036.40 +/- 2444.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+04  |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6422944  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80286    |\n",
      "|    policy_loss        | -0.0073  |\n",
      "|    value_loss         | 0.00812  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 80300    |\n",
      "|    time_elapsed       | 8627     |\n",
      "|    total_timesteps    | 6424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80299    |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 80400    |\n",
      "|    time_elapsed       | 8633     |\n",
      "|    total_timesteps    | 6432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80399    |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 80500    |\n",
      "|    time_elapsed       | 8640     |\n",
      "|    total_timesteps    | 6440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80499    |\n",
      "|    policy_loss        | -0.0491  |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6447936, episode_reward=16.20 +/- 7.11\n",
      "Episode length: 9668.00 +/- 2984.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.67e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6447936  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80599    |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.45e+03 |\n",
      "|    ep_rew_mean     | 17.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 744      |\n",
      "|    iterations      | 80600    |\n",
      "|    time_elapsed    | 8664     |\n",
      "|    total_timesteps | 6448000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 80700    |\n",
      "|    time_elapsed       | 8671     |\n",
      "|    total_timesteps    | 6456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80699    |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.00433  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 80800    |\n",
      "|    time_elapsed       | 8677     |\n",
      "|    total_timesteps    | 6464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80799    |\n",
      "|    policy_loss        | 0.0275   |\n",
      "|    value_loss         | 0.00774  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 745      |\n",
      "|    iterations         | 80900    |\n",
      "|    time_elapsed       | 8684     |\n",
      "|    total_timesteps    | 6472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80899    |\n",
      "|    policy_loss        | 0.00557  |\n",
      "|    value_loss         | 0.00247  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6472928, episode_reward=16.60 +/- 2.24\n",
      "Episode length: 9383.80 +/- 1420.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.38e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6472928  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80911    |\n",
      "|    policy_loss        | 5.4e-05  |\n",
      "|    value_loss         | 0.0038   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81000    |\n",
      "|    time_elapsed       | 8708     |\n",
      "|    total_timesteps    | 6480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80999    |\n",
      "|    policy_loss        | 0.0183   |\n",
      "|    value_loss         | 0.00244  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81100    |\n",
      "|    time_elapsed       | 8714     |\n",
      "|    total_timesteps    | 6488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81099    |\n",
      "|    policy_loss        | 0.0288   |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81200    |\n",
      "|    time_elapsed       | 8721     |\n",
      "|    total_timesteps    | 6496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81199    |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0091   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6497920, episode_reward=18.40 +/- 1.85\n",
      "Episode length: 7251.00 +/- 685.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.25e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6497920  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81223    |\n",
      "|    policy_loss        | 0.0226   |\n",
      "|    value_loss         | 0.0163   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81300    |\n",
      "|    time_elapsed       | 8741     |\n",
      "|    total_timesteps    | 6504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81299    |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81400    |\n",
      "|    time_elapsed       | 8747     |\n",
      "|    total_timesteps    | 6512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81399    |\n",
      "|    policy_loss        | 0.0716   |\n",
      "|    value_loss         | 0.0774   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81500    |\n",
      "|    time_elapsed       | 8754     |\n",
      "|    total_timesteps    | 6520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81499    |\n",
      "|    policy_loss        | 0.0281   |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6522912, episode_reward=14.80 +/- 2.40\n",
      "Episode length: 9116.60 +/- 636.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.12e+03 |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6522912  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81536    |\n",
      "|    policy_loss        | -0.00796 |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 81600    |\n",
      "|    time_elapsed       | 8777     |\n",
      "|    total_timesteps    | 6528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81599    |\n",
      "|    policy_loss        | 0.0771   |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81700    |\n",
      "|    time_elapsed       | 8784     |\n",
      "|    total_timesteps    | 6536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81699    |\n",
      "|    policy_loss        | 0.0231   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 744      |\n",
      "|    iterations         | 81800    |\n",
      "|    time_elapsed       | 8790     |\n",
      "|    total_timesteps    | 6544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81799    |\n",
      "|    policy_loss        | 0.0363   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6547904, episode_reward=16.40 +/- 2.73\n",
      "Episode length: 9831.40 +/- 1219.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.83e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6547904  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81848    |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 81900    |\n",
      "|    time_elapsed       | 8815     |\n",
      "|    total_timesteps    | 6552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81899    |\n",
      "|    policy_loss        | 0.0346   |\n",
      "|    value_loss         | 0.00879  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82000    |\n",
      "|    time_elapsed       | 8822     |\n",
      "|    total_timesteps    | 6560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81999    |\n",
      "|    policy_loss        | 0.048    |\n",
      "|    value_loss         | 0.0082   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82100    |\n",
      "|    time_elapsed       | 8828     |\n",
      "|    total_timesteps    | 6568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82099    |\n",
      "|    policy_loss        | 0.0217   |\n",
      "|    value_loss         | 0.00507  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6572896, episode_reward=18.40 +/- 1.02\n",
      "Episode length: 8015.00 +/- 477.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.02e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6572896  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82161    |\n",
      "|    policy_loss        | -0.0113  |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82200    |\n",
      "|    time_elapsed       | 8849     |\n",
      "|    total_timesteps    | 6576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82199    |\n",
      "|    policy_loss        | 0.0332   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82300    |\n",
      "|    time_elapsed       | 8856     |\n",
      "|    total_timesteps    | 6584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82299    |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.00436  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82400    |\n",
      "|    time_elapsed       | 8863     |\n",
      "|    total_timesteps    | 6592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82399    |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.00508  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6597888, episode_reward=18.00 +/- 2.61\n",
      "Episode length: 7408.00 +/- 1010.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.41e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6597888  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82473    |\n",
      "|    policy_loss        | -0.00281 |\n",
      "|    value_loss         | 0.00888  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 82500    |\n",
      "|    time_elapsed       | 8883     |\n",
      "|    total_timesteps    | 6600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82499    |\n",
      "|    policy_loss        | 0.00372  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82600    |\n",
      "|    time_elapsed       | 8889     |\n",
      "|    total_timesteps    | 6608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82599    |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82700    |\n",
      "|    time_elapsed       | 8896     |\n",
      "|    total_timesteps    | 6616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82699    |\n",
      "|    policy_loss        | -0.00184 |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6622880, episode_reward=16.80 +/- 2.14\n",
      "Episode length: 8717.20 +/- 1030.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.72e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6622880  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82785    |\n",
      "|    policy_loss        | -0.00461 |\n",
      "|    value_loss         | 0.00539  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 82800    |\n",
      "|    time_elapsed       | 8919     |\n",
      "|    total_timesteps    | 6624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82799    |\n",
      "|    policy_loss        | 0.0476   |\n",
      "|    value_loss         | 0.00819  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.11e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 82900    |\n",
      "|    time_elapsed       | 8925     |\n",
      "|    total_timesteps    | 6632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82899    |\n",
      "|    policy_loss        | -0.0023  |\n",
      "|    value_loss         | 0.00288  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.08e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 83000    |\n",
      "|    time_elapsed       | 8932     |\n",
      "|    total_timesteps    | 6640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82999    |\n",
      "|    policy_loss        | 0.065    |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6647872, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 7638.80 +/- 847.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.64e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6647872  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83098    |\n",
      "|    policy_loss        | 0.0543   |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83100    |\n",
      "|    time_elapsed       | 8952     |\n",
      "|    total_timesteps    | 6648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83099    |\n",
      "|    policy_loss        | 0.00529  |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.05e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83200    |\n",
      "|    time_elapsed       | 8959     |\n",
      "|    total_timesteps    | 6656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83199    |\n",
      "|    policy_loss        | -0.00706 |\n",
      "|    value_loss         | 0.00639  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8e+03    |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 83300    |\n",
      "|    time_elapsed       | 8966     |\n",
      "|    total_timesteps    | 6664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83299    |\n",
      "|    policy_loss        | 0.00267  |\n",
      "|    value_loss         | 0.00775  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 83400    |\n",
      "|    time_elapsed       | 8972     |\n",
      "|    total_timesteps    | 6672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83399    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.021    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6672864, episode_reward=17.80 +/- 2.23\n",
      "Episode length: 8562.60 +/- 766.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.56e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6672864  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83410    |\n",
      "|    policy_loss        | -0.0784  |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83500    |\n",
      "|    time_elapsed       | 8994     |\n",
      "|    total_timesteps    | 6680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83499    |\n",
      "|    policy_loss        | -0.0374  |\n",
      "|    value_loss         | 0.00771  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83600    |\n",
      "|    time_elapsed       | 9001     |\n",
      "|    total_timesteps    | 6688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83599    |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.17e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 743      |\n",
      "|    iterations         | 83700    |\n",
      "|    time_elapsed       | 9008     |\n",
      "|    total_timesteps    | 6696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83699    |\n",
      "|    policy_loss        | -0.0458  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6697856, episode_reward=16.20 +/- 3.19\n",
      "Episode length: 8847.80 +/- 748.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.85e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6697856  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83723    |\n",
      "|    policy_loss        | -0.0277  |\n",
      "|    value_loss         | 0.00875  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.17e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83800    |\n",
      "|    time_elapsed       | 9031     |\n",
      "|    total_timesteps    | 6704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83799    |\n",
      "|    policy_loss        | -0.0329  |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 83900    |\n",
      "|    time_elapsed       | 9037     |\n",
      "|    total_timesteps    | 6712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83899    |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84000    |\n",
      "|    time_elapsed       | 9044     |\n",
      "|    total_timesteps    | 6720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83999    |\n",
      "|    policy_loss        | 0.000177 |\n",
      "|    value_loss         | 0.00439  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6722848, episode_reward=15.00 +/- 2.61\n",
      "Episode length: 9221.00 +/- 1624.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.22e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6722848  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84035    |\n",
      "|    policy_loss        | -0.0978  |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 84100    |\n",
      "|    time_elapsed       | 9068     |\n",
      "|    total_timesteps    | 6728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84099    |\n",
      "|    policy_loss        | 0.0431   |\n",
      "|    value_loss         | 0.00698  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84200    |\n",
      "|    time_elapsed       | 9074     |\n",
      "|    total_timesteps    | 6736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84199    |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.00232  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84300    |\n",
      "|    time_elapsed       | 9081     |\n",
      "|    total_timesteps    | 6744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84299    |\n",
      "|    policy_loss        | -0.00957 |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6747840, episode_reward=16.40 +/- 1.20\n",
      "Episode length: 8508.00 +/- 612.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.51e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6747840  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84347    |\n",
      "|    policy_loss        | -0.0207  |\n",
      "|    value_loss         | 0.00666  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 84400    |\n",
      "|    time_elapsed       | 9103     |\n",
      "|    total_timesteps    | 6752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84399    |\n",
      "|    policy_loss        | -0.00303 |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84500    |\n",
      "|    time_elapsed       | 9110     |\n",
      "|    total_timesteps    | 6760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84499    |\n",
      "|    policy_loss        | -0.0298  |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84600    |\n",
      "|    time_elapsed       | 9116     |\n",
      "|    total_timesteps    | 6768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84599    |\n",
      "|    policy_loss        | 0.0456   |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6772832, episode_reward=18.20 +/- 3.60\n",
      "Episode length: 8694.00 +/- 1918.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.69e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6772832  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84660    |\n",
      "|    policy_loss        | -0.0211  |\n",
      "|    value_loss         | 0.00368  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 84700    |\n",
      "|    time_elapsed       | 9139     |\n",
      "|    total_timesteps    | 6776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.584    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84699    |\n",
      "|    policy_loss        | -0.0975  |\n",
      "|    value_loss         | 0.0636   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 84800    |\n",
      "|    time_elapsed       | 9145     |\n",
      "|    total_timesteps    | 6784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84799    |\n",
      "|    policy_loss        | 0.0336   |\n",
      "|    value_loss         | 0.00286  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 742      |\n",
      "|    iterations         | 84900    |\n",
      "|    time_elapsed       | 9152     |\n",
      "|    total_timesteps    | 6792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84899    |\n",
      "|    policy_loss        | -0.00429 |\n",
      "|    value_loss         | 0.00539  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6797824, episode_reward=14.80 +/- 2.48\n",
      "Episode length: 9481.80 +/- 1187.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.48e+03 |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6797824  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84972    |\n",
      "|    policy_loss        | 0.0571   |\n",
      "|    value_loss         | 0.0143   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85000    |\n",
      "|    time_elapsed       | 9176     |\n",
      "|    total_timesteps    | 6800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84999    |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85100    |\n",
      "|    time_elapsed       | 9182     |\n",
      "|    total_timesteps    | 6808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85099    |\n",
      "|    policy_loss        | -0.00195 |\n",
      "|    value_loss         | 0.00206  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85200    |\n",
      "|    time_elapsed       | 9189     |\n",
      "|    total_timesteps    | 6816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85199    |\n",
      "|    policy_loss        | -0.00463 |\n",
      "|    value_loss         | 0.00458  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6822816, episode_reward=18.80 +/- 0.75\n",
      "Episode length: 8132.40 +/- 362.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6822816  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85285    |\n",
      "|    policy_loss        | -0.00505 |\n",
      "|    value_loss         | 0.00824  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 85300    |\n",
      "|    time_elapsed       | 9210     |\n",
      "|    total_timesteps    | 6824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85299    |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85400    |\n",
      "|    time_elapsed       | 9217     |\n",
      "|    total_timesteps    | 6832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85399    |\n",
      "|    policy_loss        | 0.0426   |\n",
      "|    value_loss         | 0.00929  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85500    |\n",
      "|    time_elapsed       | 9223     |\n",
      "|    total_timesteps    | 6840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85499    |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    value_loss         | 0.0211   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6847808, episode_reward=17.20 +/- 2.23\n",
      "Episode length: 8451.20 +/- 1261.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.45e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6847808  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85597    |\n",
      "|    policy_loss        | 0.0296   |\n",
      "|    value_loss         | 0.00387  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 85600    |\n",
      "|    time_elapsed       | 9245     |\n",
      "|    total_timesteps    | 6848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85599    |\n",
      "|    policy_loss        | -0.0068  |\n",
      "|    value_loss         | 0.00354  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85700    |\n",
      "|    time_elapsed       | 9252     |\n",
      "|    total_timesteps    | 6856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85699    |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 0.00919  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85800    |\n",
      "|    time_elapsed       | 9258     |\n",
      "|    total_timesteps    | 6864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85799    |\n",
      "|    policy_loss        | -0.0624  |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 85900    |\n",
      "|    time_elapsed       | 9265     |\n",
      "|    total_timesteps    | 6872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.745    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85899    |\n",
      "|    policy_loss        | -0.0566  |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6872800, episode_reward=18.80 +/- 0.75\n",
      "Episode length: 7302.40 +/- 1229.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.3e+03  |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6872800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85909    |\n",
      "|    policy_loss        | -0.00442 |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86000    |\n",
      "|    time_elapsed       | 9284     |\n",
      "|    total_timesteps    | 6880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85999    |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.00644  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86100    |\n",
      "|    time_elapsed       | 9291     |\n",
      "|    total_timesteps    | 6888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86099    |\n",
      "|    policy_loss        | -0.00576 |\n",
      "|    value_loss         | 0.00339  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86200    |\n",
      "|    time_elapsed       | 9297     |\n",
      "|    total_timesteps    | 6896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86199    |\n",
      "|    policy_loss        | -0.086   |\n",
      "|    value_loss         | 0.044    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6897792, episode_reward=15.20 +/- 2.32\n",
      "Episode length: 9219.80 +/- 1210.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.22e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6897792  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86222    |\n",
      "|    policy_loss        | -0.00873 |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 17       |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 86300    |\n",
      "|    time_elapsed       | 9320     |\n",
      "|    total_timesteps    | 6904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86299    |\n",
      "|    policy_loss        | -0.00681 |\n",
      "|    value_loss         | 0.00703  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86400    |\n",
      "|    time_elapsed       | 9326     |\n",
      "|    total_timesteps    | 6912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86399    |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.00588  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86500    |\n",
      "|    time_elapsed       | 9333     |\n",
      "|    total_timesteps    | 6920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86499    |\n",
      "|    policy_loss        | -0.0493  |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6922784, episode_reward=18.80 +/- 0.40\n",
      "Episode length: 7677.40 +/- 772.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.68e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6922784  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86534    |\n",
      "|    policy_loss        | -0.0497  |\n",
      "|    value_loss         | 0.0216   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 86600    |\n",
      "|    time_elapsed       | 9353     |\n",
      "|    total_timesteps    | 6928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86599    |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.00853  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86700    |\n",
      "|    time_elapsed       | 9360     |\n",
      "|    total_timesteps    | 6936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86699    |\n",
      "|    policy_loss        | -0.00397 |\n",
      "|    value_loss         | 0.00501  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 86800    |\n",
      "|    time_elapsed       | 9366     |\n",
      "|    total_timesteps    | 6944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86799    |\n",
      "|    policy_loss        | -0.00806 |\n",
      "|    value_loss         | 0.00552  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6947776, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 7432.60 +/- 761.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.43e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6947776  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86847    |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    value_loss         | 0.0996   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 86900    |\n",
      "|    time_elapsed       | 9386     |\n",
      "|    total_timesteps    | 6952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86899    |\n",
      "|    policy_loss        | 0.00132  |\n",
      "|    value_loss         | 0.0154   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87000    |\n",
      "|    time_elapsed       | 9393     |\n",
      "|    total_timesteps    | 6960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86999    |\n",
      "|    policy_loss        | -0.00304 |\n",
      "|    value_loss         | 0.00447  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 87100    |\n",
      "|    time_elapsed       | 9399     |\n",
      "|    total_timesteps    | 6968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87099    |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6972768, episode_reward=17.20 +/- 1.72\n",
      "Episode length: 7985.20 +/- 1021.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.99e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6972768  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87159    |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.00645  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87200    |\n",
      "|    time_elapsed       | 9421     |\n",
      "|    total_timesteps    | 6976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87199    |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.00404  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87300    |\n",
      "|    time_elapsed       | 9427     |\n",
      "|    total_timesteps    | 6984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87299    |\n",
      "|    policy_loss        | 0.0367   |\n",
      "|    value_loss         | 0.00999  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 741      |\n",
      "|    iterations         | 87400    |\n",
      "|    time_elapsed       | 9434     |\n",
      "|    total_timesteps    | 6992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87399    |\n",
      "|    policy_loss        | -0.00171 |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6997760, episode_reward=17.80 +/- 1.72\n",
      "Episode length: 8242.40 +/- 1239.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.24e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6997760  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87471    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.00407  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87500    |\n",
      "|    time_elapsed       | 9455     |\n",
      "|    total_timesteps    | 7000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87499    |\n",
      "|    policy_loss        | 0.0412   |\n",
      "|    value_loss         | 0.0071   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87600    |\n",
      "|    time_elapsed       | 9462     |\n",
      "|    total_timesteps    | 7008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87599    |\n",
      "|    policy_loss        | 0.0466   |\n",
      "|    value_loss         | 0.00687  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87700    |\n",
      "|    time_elapsed       | 9469     |\n",
      "|    total_timesteps    | 7016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87699    |\n",
      "|    policy_loss        | 0.0441   |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7022752, episode_reward=18.60 +/- 2.33\n",
      "Episode length: 7657.00 +/- 980.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.66e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7022752  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87784    |\n",
      "|    policy_loss        | -0.00941 |\n",
      "|    value_loss         | 0.00412  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87800    |\n",
      "|    time_elapsed       | 9489     |\n",
      "|    total_timesteps    | 7024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87799    |\n",
      "|    policy_loss        | 0.0945   |\n",
      "|    value_loss         | 0.0343   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 87900    |\n",
      "|    time_elapsed       | 9496     |\n",
      "|    total_timesteps    | 7032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87899    |\n",
      "|    policy_loss        | -0.0071  |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88000    |\n",
      "|    time_elapsed       | 9502     |\n",
      "|    total_timesteps    | 7040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87999    |\n",
      "|    policy_loss        | 0.00139  |\n",
      "|    value_loss         | 0.00817  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7047744, episode_reward=17.00 +/- 1.90\n",
      "Episode length: 9516.60 +/- 990.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.52e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7047744  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88096    |\n",
      "|    policy_loss        | -0.0013  |\n",
      "|    value_loss         | 0.00252  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 88100    |\n",
      "|    time_elapsed       | 9526     |\n",
      "|    total_timesteps    | 7048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88099    |\n",
      "|    policy_loss        | -0.0506  |\n",
      "|    value_loss         | 0.00925  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88200    |\n",
      "|    time_elapsed       | 9533     |\n",
      "|    total_timesteps    | 7056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88199    |\n",
      "|    policy_loss        | -0.045   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88300    |\n",
      "|    time_elapsed       | 9539     |\n",
      "|    total_timesteps    | 7064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88299    |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.00316  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88400    |\n",
      "|    time_elapsed       | 9546     |\n",
      "|    total_timesteps    | 7072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88399    |\n",
      "|    policy_loss        | 0.00929  |\n",
      "|    value_loss         | 0.00248  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7072736, episode_reward=19.40 +/- 0.49\n",
      "Episode length: 7904.40 +/- 682.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.9e+03  |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7072736  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88409    |\n",
      "|    policy_loss        | -0.0772  |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 88500    |\n",
      "|    time_elapsed       | 9567     |\n",
      "|    total_timesteps    | 7080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88499    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.0195   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88600    |\n",
      "|    time_elapsed       | 9574     |\n",
      "|    total_timesteps    | 7088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88599    |\n",
      "|    policy_loss        | -0.00931 |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88700    |\n",
      "|    time_elapsed       | 9580     |\n",
      "|    total_timesteps    | 7096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88699    |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.00618  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7097728, episode_reward=19.60 +/- 0.49\n",
      "Episode length: 7946.80 +/- 521.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.95e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7097728  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88721    |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 88800    |\n",
      "|    time_elapsed       | 9601     |\n",
      "|    total_timesteps    | 7104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88799    |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.00834  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 88900    |\n",
      "|    time_elapsed       | 9608     |\n",
      "|    total_timesteps    | 7112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88899    |\n",
      "|    policy_loss        | -0.00332 |\n",
      "|    value_loss         | 0.00418  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 89000    |\n",
      "|    time_elapsed       | 9615     |\n",
      "|    total_timesteps    | 7120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88999    |\n",
      "|    policy_loss        | -0.00375 |\n",
      "|    value_loss         | 0.00399  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7122720, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 7826.60 +/- 503.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.83e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7122720  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89033    |\n",
      "|    policy_loss        | 0.00988  |\n",
      "|    value_loss         | 0.00769  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 89100    |\n",
      "|    time_elapsed       | 9635     |\n",
      "|    total_timesteps    | 7128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89099    |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.00422  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 89200    |\n",
      "|    time_elapsed       | 9642     |\n",
      "|    total_timesteps    | 7136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89199    |\n",
      "|    policy_loss        | -0.0613  |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 89300    |\n",
      "|    time_elapsed       | 9649     |\n",
      "|    total_timesteps    | 7144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89299    |\n",
      "|    policy_loss        | -0.0569  |\n",
      "|    value_loss         | 0.00829  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7147712, episode_reward=18.00 +/- 1.79\n",
      "Episode length: 7756.80 +/- 733.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.76e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7147712  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89346    |\n",
      "|    policy_loss        | 0.0482   |\n",
      "|    value_loss         | 0.00381  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 89400    |\n",
      "|    time_elapsed       | 9669     |\n",
      "|    total_timesteps    | 7152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89399    |\n",
      "|    policy_loss        | 0.00781  |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 89500    |\n",
      "|    time_elapsed       | 9676     |\n",
      "|    total_timesteps    | 7160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89499    |\n",
      "|    policy_loss        | 0.023    |\n",
      "|    value_loss         | 0.0184   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 89600    |\n",
      "|    time_elapsed       | 9683     |\n",
      "|    total_timesteps    | 7168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89599    |\n",
      "|    policy_loss        | 0.046    |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7172704, episode_reward=19.80 +/- 1.47\n",
      "Episode length: 7665.40 +/- 773.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.67e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7172704  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89658    |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.00894  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 89700    |\n",
      "|    time_elapsed       | 9703     |\n",
      "|    total_timesteps    | 7176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89699    |\n",
      "|    policy_loss        | -0.00778 |\n",
      "|    value_loss         | 0.00446  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 89800    |\n",
      "|    time_elapsed       | 9710     |\n",
      "|    total_timesteps    | 7184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89799    |\n",
      "|    policy_loss        | -0.00487 |\n",
      "|    value_loss         | 0.00697  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 89900    |\n",
      "|    time_elapsed       | 9716     |\n",
      "|    total_timesteps    | 7192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89899    |\n",
      "|    policy_loss        | -0.0206  |\n",
      "|    value_loss         | 0.00655  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7197696, episode_reward=19.00 +/- 0.89\n",
      "Episode length: 7222.60 +/- 961.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.22e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7197696  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89971    |\n",
      "|    policy_loss        | 0.00754  |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90000    |\n",
      "|    time_elapsed       | 9736     |\n",
      "|    total_timesteps    | 7200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89999    |\n",
      "|    policy_loss        | 0.0645   |\n",
      "|    value_loss         | 0.00766  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90100    |\n",
      "|    time_elapsed       | 9743     |\n",
      "|    total_timesteps    | 7208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90099    |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.00591  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 90200    |\n",
      "|    time_elapsed       | 9749     |\n",
      "|    total_timesteps    | 7216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90199    |\n",
      "|    policy_loss        | -0.0227  |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7222688, episode_reward=19.80 +/- 0.40\n",
      "Episode length: 7631.60 +/- 1027.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.63e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7222688  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90283    |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90300    |\n",
      "|    time_elapsed       | 9770     |\n",
      "|    total_timesteps    | 7224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90299    |\n",
      "|    policy_loss        | -0.0128  |\n",
      "|    value_loss         | 0.00624  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90400    |\n",
      "|    time_elapsed       | 9777     |\n",
      "|    total_timesteps    | 7232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90399    |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 90500    |\n",
      "|    time_elapsed       | 9783     |\n",
      "|    total_timesteps    | 7240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90499    |\n",
      "|    policy_loss        | 0.00734  |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7247680, episode_reward=17.80 +/- 2.48\n",
      "Episode length: 8377.60 +/- 1475.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.38e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7247680  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90595    |\n",
      "|    policy_loss        | -0.0201  |\n",
      "|    value_loss         | 0.0041   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90600    |\n",
      "|    time_elapsed       | 9805     |\n",
      "|    total_timesteps    | 7248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90599    |\n",
      "|    policy_loss        | 0.0744   |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90700    |\n",
      "|    time_elapsed       | 9812     |\n",
      "|    total_timesteps    | 7256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90699    |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.00601  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 90800    |\n",
      "|    time_elapsed       | 9818     |\n",
      "|    total_timesteps    | 7264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90799    |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    value_loss         | 0.00966  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 740      |\n",
      "|    iterations         | 90900    |\n",
      "|    time_elapsed       | 9825     |\n",
      "|    total_timesteps    | 7272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90899    |\n",
      "|    policy_loss        | 0.00713  |\n",
      "|    value_loss         | 0.00172  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7272672, episode_reward=18.40 +/- 2.58\n",
      "Episode length: 8546.80 +/- 1258.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.55e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7272672  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90908    |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.00615  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91000    |\n",
      "|    time_elapsed       | 9847     |\n",
      "|    total_timesteps    | 7280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90999    |\n",
      "|    policy_loss        | -0.00826 |\n",
      "|    value_loss         | 0.00639  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91100    |\n",
      "|    time_elapsed       | 9853     |\n",
      "|    total_timesteps    | 7288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91099    |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91200    |\n",
      "|    time_elapsed       | 9860     |\n",
      "|    total_timesteps    | 7296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91199    |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.00159  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7297664, episode_reward=18.60 +/- 1.36\n",
      "Episode length: 7627.60 +/- 549.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.63e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7297664  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91220    |\n",
      "|    policy_loss        | -0.0107  |\n",
      "|    value_loss         | 0.00387  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91300    |\n",
      "|    time_elapsed       | 9880     |\n",
      "|    total_timesteps    | 7304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91299    |\n",
      "|    policy_loss        | 0.0206   |\n",
      "|    value_loss         | 0.00543  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91400    |\n",
      "|    time_elapsed       | 9887     |\n",
      "|    total_timesteps    | 7312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91399    |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91500    |\n",
      "|    time_elapsed       | 9894     |\n",
      "|    total_timesteps    | 7320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91499    |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.00253  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7322656, episode_reward=19.60 +/- 1.02\n",
      "Episode length: 8255.40 +/- 676.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.26e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7322656  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91533    |\n",
      "|    policy_loss        | 0.0009   |\n",
      "|    value_loss         | 0.00754  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91600    |\n",
      "|    time_elapsed       | 9915     |\n",
      "|    total_timesteps    | 7328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91599    |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.00776  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91700    |\n",
      "|    time_elapsed       | 9922     |\n",
      "|    total_timesteps    | 7336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91699    |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.00538  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 91800    |\n",
      "|    time_elapsed       | 9928     |\n",
      "|    total_timesteps    | 7344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91799    |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7347648, episode_reward=17.40 +/- 2.42\n",
      "Episode length: 8979.60 +/- 724.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.98e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7347648  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91845    |\n",
      "|    policy_loss        | -0.0293  |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 91900    |\n",
      "|    time_elapsed       | 9951     |\n",
      "|    total_timesteps    | 7352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91899    |\n",
      "|    policy_loss        | 0.0568   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 92000    |\n",
      "|    time_elapsed       | 9958     |\n",
      "|    total_timesteps    | 7360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91999    |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 92100    |\n",
      "|    time_elapsed       | 9965     |\n",
      "|    total_timesteps    | 7368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92099    |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.00513  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7372640, episode_reward=17.80 +/- 3.54\n",
      "Episode length: 8351.00 +/- 1042.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.35e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7372640  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92157    |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.00567  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92200    |\n",
      "|    time_elapsed       | 9986     |\n",
      "|    total_timesteps    | 7376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92199    |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.00467  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92300    |\n",
      "|    time_elapsed       | 9993     |\n",
      "|    total_timesteps    | 7384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92299    |\n",
      "|    policy_loss        | -0.0644  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 92400    |\n",
      "|    time_elapsed       | 10000    |\n",
      "|    total_timesteps    | 7392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92399    |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7397632, episode_reward=18.00 +/- 1.10\n",
      "Episode length: 7743.40 +/- 962.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.74e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7397632  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92470    |\n",
      "|    policy_loss        | -0.00141 |\n",
      "|    value_loss         | 0.0088   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92500    |\n",
      "|    time_elapsed       | 10020    |\n",
      "|    total_timesteps    | 7400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92499    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.00336  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92600    |\n",
      "|    time_elapsed       | 10027    |\n",
      "|    total_timesteps    | 7408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92599    |\n",
      "|    policy_loss        | -0.0227  |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 92700    |\n",
      "|    time_elapsed       | 10033    |\n",
      "|    total_timesteps    | 7416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92699    |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7422624, episode_reward=18.60 +/- 1.02\n",
      "Episode length: 8159.80 +/- 849.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.16e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7422624  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92782    |\n",
      "|    policy_loss        | 0.00433  |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92800    |\n",
      "|    time_elapsed       | 10055    |\n",
      "|    total_timesteps    | 7424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92799    |\n",
      "|    policy_loss        | 0.00941  |\n",
      "|    value_loss         | 0.00345  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 92900    |\n",
      "|    time_elapsed       | 10061    |\n",
      "|    total_timesteps    | 7432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92899    |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.0078   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93000    |\n",
      "|    time_elapsed       | 10068    |\n",
      "|    total_timesteps    | 7440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92999    |\n",
      "|    policy_loss        | 0.0611   |\n",
      "|    value_loss         | 0.00524  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7447616, episode_reward=18.60 +/- 2.15\n",
      "Episode length: 7952.60 +/- 993.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.95e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7447616  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93095    |\n",
      "|    policy_loss        | -0.0548  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93100    |\n",
      "|    time_elapsed       | 10089    |\n",
      "|    total_timesteps    | 7448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93099    |\n",
      "|    policy_loss        | 0.0201   |\n",
      "|    value_loss         | 0.00826  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93200    |\n",
      "|    time_elapsed       | 10096    |\n",
      "|    total_timesteps    | 7456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93199    |\n",
      "|    policy_loss        | 0.000561 |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93300    |\n",
      "|    time_elapsed       | 10102    |\n",
      "|    total_timesteps    | 7464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93299    |\n",
      "|    policy_loss        | -0.00605 |\n",
      "|    value_loss         | 0.00917  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 739      |\n",
      "|    iterations         | 93400    |\n",
      "|    time_elapsed       | 10109    |\n",
      "|    total_timesteps    | 7472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93399    |\n",
      "|    policy_loss        | -0.0209  |\n",
      "|    value_loss         | 0.0043   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7472608, episode_reward=15.20 +/- 5.11\n",
      "Episode length: 9139.20 +/- 2594.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.14e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7472608  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93407    |\n",
      "|    policy_loss        | 0.0024   |\n",
      "|    value_loss         | 0.00564  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93500    |\n",
      "|    time_elapsed       | 10132    |\n",
      "|    total_timesteps    | 7480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93499    |\n",
      "|    policy_loss        | 0.00599  |\n",
      "|    value_loss         | 0.00319  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93600    |\n",
      "|    time_elapsed       | 10139    |\n",
      "|    total_timesteps    | 7488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93599    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.00594  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93700    |\n",
      "|    time_elapsed       | 10145    |\n",
      "|    total_timesteps    | 7496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93699    |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7497600, episode_reward=20.00 +/- 1.10\n",
      "Episode length: 7309.80 +/- 683.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.31e+03 |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7497600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93719    |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.00337  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93800    |\n",
      "|    time_elapsed       | 10165    |\n",
      "|    total_timesteps    | 7504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93799    |\n",
      "|    policy_loss        | 0.0147   |\n",
      "|    value_loss         | 0.00658  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 93900    |\n",
      "|    time_elapsed       | 10172    |\n",
      "|    total_timesteps    | 7512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93899    |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.00315  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94000    |\n",
      "|    time_elapsed       | 10179    |\n",
      "|    total_timesteps    | 7520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93999    |\n",
      "|    policy_loss        | 0.0251   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7522592, episode_reward=19.00 +/- 1.26\n",
      "Episode length: 7861.20 +/- 924.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.86e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7522592  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94032    |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94100    |\n",
      "|    time_elapsed       | 10199    |\n",
      "|    total_timesteps    | 7528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94099    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.00141  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94200    |\n",
      "|    time_elapsed       | 10206    |\n",
      "|    total_timesteps    | 7536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94199    |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.00169  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94300    |\n",
      "|    time_elapsed       | 10212    |\n",
      "|    total_timesteps    | 7544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94299    |\n",
      "|    policy_loss        | -0.0457  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7547584, episode_reward=15.00 +/- 4.05\n",
      "Episode length: 10255.60 +/- 1558.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7547584  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94344    |\n",
      "|    policy_loss        | -0.0585  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 94400    |\n",
      "|    time_elapsed       | 10237    |\n",
      "|    total_timesteps    | 7552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94399    |\n",
      "|    policy_loss        | 0.00553  |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 94500    |\n",
      "|    time_elapsed       | 10244    |\n",
      "|    total_timesteps    | 7560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94499    |\n",
      "|    policy_loss        | -0.00858 |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94600    |\n",
      "|    time_elapsed       | 10251    |\n",
      "|    total_timesteps    | 7568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.745    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94599    |\n",
      "|    policy_loss        | 0.0323   |\n",
      "|    value_loss         | 0.0276   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7572576, episode_reward=18.20 +/- 1.72\n",
      "Episode length: 8580.40 +/- 1153.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.58e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7572576  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94657    |\n",
      "|    policy_loss        | -0.0431  |\n",
      "|    value_loss         | 0.0137   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 94700    |\n",
      "|    time_elapsed       | 10273    |\n",
      "|    total_timesteps    | 7576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94699    |\n",
      "|    policy_loss        | 0.00603  |\n",
      "|    value_loss         | 0.0148   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 94800    |\n",
      "|    time_elapsed       | 10279    |\n",
      "|    total_timesteps    | 7584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94799    |\n",
      "|    policy_loss        | -0.0232  |\n",
      "|    value_loss         | 0.00602  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 738      |\n",
      "|    iterations         | 94900    |\n",
      "|    time_elapsed       | 10286    |\n",
      "|    total_timesteps    | 7592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94899    |\n",
      "|    policy_loss        | -0.00938 |\n",
      "|    value_loss         | 0.00311  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7597568, episode_reward=19.20 +/- 1.60\n",
      "Episode length: 7662.00 +/- 712.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.66e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7597568  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94969    |\n",
      "|    policy_loss        | -0.0525  |\n",
      "|    value_loss         | 0.00929  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95000    |\n",
      "|    time_elapsed       | 10306    |\n",
      "|    total_timesteps    | 7600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94999    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.00951  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95100    |\n",
      "|    time_elapsed       | 10313    |\n",
      "|    total_timesteps    | 7608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95099    |\n",
      "|    policy_loss        | 0.0285   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95200    |\n",
      "|    time_elapsed       | 10320    |\n",
      "|    total_timesteps    | 7616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95199    |\n",
      "|    policy_loss        | -0.0507  |\n",
      "|    value_loss         | 0.00849  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7622560, episode_reward=16.00 +/- 4.52\n",
      "Episode length: 9040.40 +/- 2868.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.04e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7622560  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95281    |\n",
      "|    policy_loss        | 0.0238   |\n",
      "|    value_loss         | 0.00375  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95300    |\n",
      "|    time_elapsed       | 10343    |\n",
      "|    total_timesteps    | 7624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95299    |\n",
      "|    policy_loss        | 0.00562  |\n",
      "|    value_loss         | 0.00318  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95400    |\n",
      "|    time_elapsed       | 10349    |\n",
      "|    total_timesteps    | 7632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95399    |\n",
      "|    policy_loss        | 0.00579  |\n",
      "|    value_loss         | 0.0023   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95500    |\n",
      "|    time_elapsed       | 10356    |\n",
      "|    total_timesteps    | 7640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95499    |\n",
      "|    policy_loss        | 0.0763   |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7647552, episode_reward=16.80 +/- 3.54\n",
      "Episode length: 8665.60 +/- 1538.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.67e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7647552  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95594    |\n",
      "|    policy_loss        | 0.0406   |\n",
      "|    value_loss         | 0.00766  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 95600    |\n",
      "|    time_elapsed       | 10378    |\n",
      "|    total_timesteps    | 7648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95599    |\n",
      "|    policy_loss        | 0.0213   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95700    |\n",
      "|    time_elapsed       | 10385    |\n",
      "|    total_timesteps    | 7656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95699    |\n",
      "|    policy_loss        | -0.0174  |\n",
      "|    value_loss         | 0.00676  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95800    |\n",
      "|    time_elapsed       | 10391    |\n",
      "|    total_timesteps    | 7664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95799    |\n",
      "|    policy_loss        | 0.00941  |\n",
      "|    value_loss         | 0.00547  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 95900    |\n",
      "|    time_elapsed       | 10398    |\n",
      "|    total_timesteps    | 7672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95899    |\n",
      "|    policy_loss        | -0.0183  |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7672544, episode_reward=18.20 +/- 1.94\n",
      "Episode length: 7767.00 +/- 982.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.77e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7672544  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95906    |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96000    |\n",
      "|    time_elapsed       | 10419    |\n",
      "|    total_timesteps    | 7680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95999    |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.00591  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96100    |\n",
      "|    time_elapsed       | 10425    |\n",
      "|    total_timesteps    | 7688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96099    |\n",
      "|    policy_loss        | -0.0273  |\n",
      "|    value_loss         | 0.00892  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96200    |\n",
      "|    time_elapsed       | 10432    |\n",
      "|    total_timesteps    | 7696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96199    |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7697536, episode_reward=18.00 +/- 1.79\n",
      "Episode length: 8529.00 +/- 1173.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.53e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7697536  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96219    |\n",
      "|    policy_loss        | 0.0348   |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 96300    |\n",
      "|    time_elapsed       | 10454    |\n",
      "|    total_timesteps    | 7704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96299    |\n",
      "|    policy_loss        | -0.00239 |\n",
      "|    value_loss         | 0.00882  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96400    |\n",
      "|    time_elapsed       | 10460    |\n",
      "|    total_timesteps    | 7712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96399    |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.00573  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96500    |\n",
      "|    time_elapsed       | 10467    |\n",
      "|    total_timesteps    | 7720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96499    |\n",
      "|    policy_loss        | -0.0112  |\n",
      "|    value_loss         | 0.00176  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7722528, episode_reward=17.40 +/- 1.62\n",
      "Episode length: 9130.60 +/- 584.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.13e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7722528  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96531    |\n",
      "|    policy_loss        | -0.00502 |\n",
      "|    value_loss         | 0.0038   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 96600    |\n",
      "|    time_elapsed       | 10490    |\n",
      "|    total_timesteps    | 7728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96599    |\n",
      "|    policy_loss        | 0.0123   |\n",
      "|    value_loss         | 0.00307  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 96700    |\n",
      "|    time_elapsed       | 10497    |\n",
      "|    total_timesteps    | 7736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96699    |\n",
      "|    policy_loss        | -0.0433  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 96800    |\n",
      "|    time_elapsed       | 10503    |\n",
      "|    total_timesteps    | 7744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96799    |\n",
      "|    policy_loss        | -0.0587  |\n",
      "|    value_loss         | 0.00785  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7747520, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 7893.80 +/- 820.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.89e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7747520  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96843    |\n",
      "|    policy_loss        | -0.0057  |\n",
      "|    value_loss         | 0.00845  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 96900    |\n",
      "|    time_elapsed       | 10524    |\n",
      "|    total_timesteps    | 7752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96899    |\n",
      "|    policy_loss        | 0.00713  |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97000    |\n",
      "|    time_elapsed       | 10531    |\n",
      "|    total_timesteps    | 7760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96999    |\n",
      "|    policy_loss        | -0.00581 |\n",
      "|    value_loss         | 0.00751  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 97100    |\n",
      "|    time_elapsed       | 10538    |\n",
      "|    total_timesteps    | 7768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97099    |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.00879  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7772512, episode_reward=19.60 +/- 1.02\n",
      "Episode length: 7194.60 +/- 658.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.19e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7772512  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97156    |\n",
      "|    policy_loss        | 0.000185 |\n",
      "|    value_loss         | 0.00316  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.65e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97200    |\n",
      "|    time_elapsed       | 10557    |\n",
      "|    total_timesteps    | 7776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97199    |\n",
      "|    policy_loss        | -0.00295 |\n",
      "|    value_loss         | 0.00139  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97300    |\n",
      "|    time_elapsed       | 10564    |\n",
      "|    total_timesteps    | 7784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97299    |\n",
      "|    policy_loss        | 0.00996  |\n",
      "|    value_loss         | 0.00647  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 97400    |\n",
      "|    time_elapsed       | 10570    |\n",
      "|    total_timesteps    | 7792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97399    |\n",
      "|    policy_loss        | -0.0362  |\n",
      "|    value_loss         | 0.00966  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7797504, episode_reward=17.20 +/- 1.83\n",
      "Episode length: 8909.40 +/- 1067.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.91e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7797504  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97468    |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.00735  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97500    |\n",
      "|    time_elapsed       | 10593    |\n",
      "|    total_timesteps    | 7800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97499    |\n",
      "|    policy_loss        | 0.0194   |\n",
      "|    value_loss         | 0.00801  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97600    |\n",
      "|    time_elapsed       | 10600    |\n",
      "|    total_timesteps    | 7808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97599    |\n",
      "|    policy_loss        | 0.0162   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97700    |\n",
      "|    time_elapsed       | 10606    |\n",
      "|    total_timesteps    | 7816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97699    |\n",
      "|    policy_loss        | -0.0639  |\n",
      "|    value_loss         | 0.00735  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7822496, episode_reward=17.80 +/- 2.14\n",
      "Episode length: 8130.00 +/- 1589.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7822496  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97781    |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.00527  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97800    |\n",
      "|    time_elapsed       | 10628    |\n",
      "|    total_timesteps    | 7824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97799    |\n",
      "|    policy_loss        | 0.0386   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 97900    |\n",
      "|    time_elapsed       | 10634    |\n",
      "|    total_timesteps    | 7832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97899    |\n",
      "|    policy_loss        | 0.0614   |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98000    |\n",
      "|    time_elapsed       | 10641    |\n",
      "|    total_timesteps    | 7840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97999    |\n",
      "|    policy_loss        | -0.0318  |\n",
      "|    value_loss         | 0.0176   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7847488, episode_reward=18.60 +/- 1.50\n",
      "Episode length: 7604.80 +/- 1465.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.6e+03  |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7847488  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98093    |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.00377  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98100    |\n",
      "|    time_elapsed       | 10661    |\n",
      "|    total_timesteps    | 7848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98099    |\n",
      "|    policy_loss        | -0.0125  |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98200    |\n",
      "|    time_elapsed       | 10667    |\n",
      "|    total_timesteps    | 7856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98199    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.00551  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98300    |\n",
      "|    time_elapsed       | 10674    |\n",
      "|    total_timesteps    | 7864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98299    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.00546  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 737      |\n",
      "|    iterations         | 98400    |\n",
      "|    time_elapsed       | 10680    |\n",
      "|    total_timesteps    | 7872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98399    |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7872480, episode_reward=15.80 +/- 1.17\n",
      "Episode length: 9818.60 +/- 1242.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.82e+03 |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7872480  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98405    |\n",
      "|    policy_loss        | 0.00819  |\n",
      "|    value_loss         | 0.00686  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.68e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98500    |\n",
      "|    time_elapsed       | 10704    |\n",
      "|    total_timesteps    | 7880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98499    |\n",
      "|    policy_loss        | -0.0227  |\n",
      "|    value_loss         | 0.00673  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.65e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98600    |\n",
      "|    time_elapsed       | 10711    |\n",
      "|    total_timesteps    | 7888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98599    |\n",
      "|    policy_loss        | 0.0456   |\n",
      "|    value_loss         | 0.0076   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.63e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98700    |\n",
      "|    time_elapsed       | 10718    |\n",
      "|    total_timesteps    | 7896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98699    |\n",
      "|    policy_loss        | -0.00352 |\n",
      "|    value_loss         | 0.00653  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7897472, episode_reward=17.40 +/- 1.85\n",
      "Episode length: 8551.20 +/- 1042.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.55e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7897472  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98718    |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 98800    |\n",
      "|    time_elapsed       | 10740    |\n",
      "|    total_timesteps    | 7904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98799    |\n",
      "|    policy_loss        | -0.0296  |\n",
      "|    value_loss         | 0.00851  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 98900    |\n",
      "|    time_elapsed       | 10746    |\n",
      "|    total_timesteps    | 7912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98899    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.00712  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 99000    |\n",
      "|    time_elapsed       | 10753    |\n",
      "|    total_timesteps    | 7920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98999    |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7922464, episode_reward=14.60 +/- 4.50\n",
      "Episode length: 10219.20 +/- 2323.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7922464  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99030    |\n",
      "|    policy_loss        | 0.00233  |\n",
      "|    value_loss         | 0.00272  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99100    |\n",
      "|    time_elapsed       | 10778    |\n",
      "|    total_timesteps    | 7928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99099    |\n",
      "|    policy_loss        | -0.0292  |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99200    |\n",
      "|    time_elapsed       | 10785    |\n",
      "|    total_timesteps    | 7936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99199    |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.00956  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 99300    |\n",
      "|    time_elapsed       | 10791    |\n",
      "|    total_timesteps    | 7944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99299    |\n",
      "|    policy_loss        | 0.0829   |\n",
      "|    value_loss         | 0.0193   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7947456, episode_reward=18.40 +/- 1.36\n",
      "Episode length: 7537.80 +/- 411.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.54e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7947456  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.474    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99343    |\n",
      "|    policy_loss        | -0.051   |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99400    |\n",
      "|    time_elapsed       | 10812    |\n",
      "|    total_timesteps    | 7952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99399    |\n",
      "|    policy_loss        | -0.00286 |\n",
      "|    value_loss         | 0.0159   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99500    |\n",
      "|    time_elapsed       | 10818    |\n",
      "|    total_timesteps    | 7960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99499    |\n",
      "|    policy_loss        | -0.0459  |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 736      |\n",
      "|    iterations         | 99600    |\n",
      "|    time_elapsed       | 10825    |\n",
      "|    total_timesteps    | 7968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99599    |\n",
      "|    policy_loss        | -0.057   |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7972448, episode_reward=18.20 +/- 2.23\n",
      "Episode length: 8602.20 +/- 1236.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.6e+03  |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7972448  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99655    |\n",
      "|    policy_loss        | -0.0665  |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99700    |\n",
      "|    time_elapsed       | 10847    |\n",
      "|    total_timesteps    | 7976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99699    |\n",
      "|    policy_loss        | -0.065   |\n",
      "|    value_loss         | 0.00893  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99800    |\n",
      "|    time_elapsed       | 10854    |\n",
      "|    total_timesteps    | 7984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99799    |\n",
      "|    policy_loss        | 0.00317  |\n",
      "|    value_loss         | 0.00498  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 99900    |\n",
      "|    time_elapsed       | 10860    |\n",
      "|    total_timesteps    | 7992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99899    |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.00399  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7997440, episode_reward=17.60 +/- 3.88\n",
      "Episode length: 9133.40 +/- 1532.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.13e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7997440  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99967    |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.00871  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100000   |\n",
      "|    time_elapsed       | 10883    |\n",
      "|    total_timesteps    | 8000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99999    |\n",
      "|    policy_loss        | 0.00666  |\n",
      "|    value_loss         | 0.00396  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100100   |\n",
      "|    time_elapsed       | 10890    |\n",
      "|    total_timesteps    | 8008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100099   |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 0.00687  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100200   |\n",
      "|    time_elapsed       | 10897    |\n",
      "|    total_timesteps    | 8016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100199   |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.00404  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8022432, episode_reward=19.00 +/- 1.55\n",
      "Episode length: 7926.40 +/- 1166.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.93e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8022432  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100280   |\n",
      "|    policy_loss        | 0.0181   |\n",
      "|    value_loss         | 0.00276  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.67e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 100300   |\n",
      "|    time_elapsed       | 10918    |\n",
      "|    total_timesteps    | 8024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100299   |\n",
      "|    policy_loss        | 0.0098   |\n",
      "|    value_loss         | 0.0027   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100400   |\n",
      "|    time_elapsed       | 10924    |\n",
      "|    total_timesteps    | 8032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100399   |\n",
      "|    policy_loss        | -0.0501  |\n",
      "|    value_loss         | 0.00747  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100500   |\n",
      "|    time_elapsed       | 10931    |\n",
      "|    total_timesteps    | 8040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100499   |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.00635  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8047424, episode_reward=19.60 +/- 1.02\n",
      "Episode length: 7879.20 +/- 489.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.88e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8047424  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100592   |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.71e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 100600   |\n",
      "|    time_elapsed       | 10952    |\n",
      "|    total_timesteps    | 8048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100599   |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100700   |\n",
      "|    time_elapsed       | 10958    |\n",
      "|    total_timesteps    | 8056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100699   |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.00508  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100800   |\n",
      "|    time_elapsed       | 10965    |\n",
      "|    total_timesteps    | 8064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100799   |\n",
      "|    policy_loss        | -0.0364  |\n",
      "|    value_loss         | 0.00402  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 100900   |\n",
      "|    time_elapsed       | 10971    |\n",
      "|    total_timesteps    | 8072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100899   |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.00856  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8072416, episode_reward=19.00 +/- 1.41\n",
      "Episode length: 7716.20 +/- 823.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.72e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8072416  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100905   |\n",
      "|    policy_loss        | -0.0253  |\n",
      "|    value_loss         | 0.00671  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101000   |\n",
      "|    time_elapsed       | 10992    |\n",
      "|    total_timesteps    | 8080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 100999   |\n",
      "|    policy_loss        | -0.0225  |\n",
      "|    value_loss         | 0.00226  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101100   |\n",
      "|    time_elapsed       | 10999    |\n",
      "|    total_timesteps    | 8088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101099   |\n",
      "|    policy_loss        | 0.0327   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101200   |\n",
      "|    time_elapsed       | 11005    |\n",
      "|    total_timesteps    | 8096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101199   |\n",
      "|    policy_loss        | 0.0044   |\n",
      "|    value_loss         | 0.00201  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8097408, episode_reward=18.80 +/- 1.47\n",
      "Episode length: 8276.60 +/- 1015.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.28e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8097408  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101217   |\n",
      "|    policy_loss        | 0.000933 |\n",
      "|    value_loss         | 0.00176  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 101300   |\n",
      "|    time_elapsed       | 11027    |\n",
      "|    total_timesteps    | 8104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101299   |\n",
      "|    policy_loss        | 0.000397 |\n",
      "|    value_loss         | 0.00208  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 8.61e+03  |\n",
      "|    ep_rew_mean        | 18.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 735       |\n",
      "|    iterations         | 101400    |\n",
      "|    time_elapsed       | 11033     |\n",
      "|    total_timesteps    | 8112000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0.933     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 101399    |\n",
      "|    policy_loss        | -0.000493 |\n",
      "|    value_loss         | 0.00641   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101500   |\n",
      "|    time_elapsed       | 11040    |\n",
      "|    total_timesteps    | 8120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101499   |\n",
      "|    policy_loss        | -0.00882 |\n",
      "|    value_loss         | 0.00589  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8122400, episode_reward=18.20 +/- 1.60\n",
      "Episode length: 8441.80 +/- 638.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.44e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8122400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101529   |\n",
      "|    policy_loss        | -0.0559  |\n",
      "|    value_loss         | 0.00548  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 101600   |\n",
      "|    time_elapsed       | 11062    |\n",
      "|    total_timesteps    | 8128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101599   |\n",
      "|    policy_loss        | 0.0133   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101700   |\n",
      "|    time_elapsed       | 11069    |\n",
      "|    total_timesteps    | 8136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101699   |\n",
      "|    policy_loss        | 0.00676  |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 101800   |\n",
      "|    time_elapsed       | 11075    |\n",
      "|    total_timesteps    | 8144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101799   |\n",
      "|    policy_loss        | -0.0389  |\n",
      "|    value_loss         | 0.00772  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8147392, episode_reward=18.20 +/- 0.98\n",
      "Episode length: 7743.80 +/- 1023.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.74e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8147392  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101842   |\n",
      "|    policy_loss        | -0.0461  |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.74e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 101900   |\n",
      "|    time_elapsed       | 11096    |\n",
      "|    total_timesteps    | 8152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101899   |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.00313  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.73e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102000   |\n",
      "|    time_elapsed       | 11103    |\n",
      "|    total_timesteps    | 8160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 101999   |\n",
      "|    policy_loss        | 0.0357   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 8.71e+03  |\n",
      "|    ep_rew_mean        | 18        |\n",
      "| time/                 |           |\n",
      "|    fps                | 735       |\n",
      "|    iterations         | 102100    |\n",
      "|    time_elapsed       | 11109     |\n",
      "|    total_timesteps    | 8168000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0.974     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 102099    |\n",
      "|    policy_loss        | -0.000994 |\n",
      "|    value_loss         | 0.00322   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8172384, episode_reward=18.20 +/- 0.75\n",
      "Episode length: 8087.80 +/- 702.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8172384  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102154   |\n",
      "|    policy_loss        | -0.0306  |\n",
      "|    value_loss         | 0.00731  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.7e+03  |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102200   |\n",
      "|    time_elapsed       | 11130    |\n",
      "|    total_timesteps    | 8176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102199   |\n",
      "|    policy_loss        | 0.0251   |\n",
      "|    value_loss         | 0.00372  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.68e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102300   |\n",
      "|    time_elapsed       | 11137    |\n",
      "|    total_timesteps    | 8184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102299   |\n",
      "|    policy_loss        | 0.0558   |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.65e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 735      |\n",
      "|    iterations         | 102400   |\n",
      "|    time_elapsed       | 11144    |\n",
      "|    total_timesteps    | 8192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102399   |\n",
      "|    policy_loss        | 0.0607   |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8197376, episode_reward=18.60 +/- 1.36\n",
      "Episode length: 8664.20 +/- 1375.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.66e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8197376  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102467   |\n",
      "|    policy_loss        | -0.0198  |\n",
      "|    value_loss         | 0.0262   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102500   |\n",
      "|    time_elapsed       | 11166    |\n",
      "|    total_timesteps    | 8200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102499   |\n",
      "|    policy_loss        | -0.0361  |\n",
      "|    value_loss         | 0.00423  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102600   |\n",
      "|    time_elapsed       | 11173    |\n",
      "|    total_timesteps    | 8208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102599   |\n",
      "|    policy_loss        | 0.0345   |\n",
      "|    value_loss         | 0.00454  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102700   |\n",
      "|    time_elapsed       | 11179    |\n",
      "|    total_timesteps    | 8216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102699   |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.016    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8222368, episode_reward=16.00 +/- 5.69\n",
      "Episode length: 8955.40 +/- 3052.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.96e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8222368  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102779   |\n",
      "|    policy_loss        | -0.00804 |\n",
      "|    value_loss         | 0.00388  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102800   |\n",
      "|    time_elapsed       | 11202    |\n",
      "|    total_timesteps    | 8224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102799   |\n",
      "|    policy_loss        | 0.000926 |\n",
      "|    value_loss         | 0.00484  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 102900   |\n",
      "|    time_elapsed       | 11209    |\n",
      "|    total_timesteps    | 8232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102899   |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.00315  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.58e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103000   |\n",
      "|    time_elapsed       | 11215    |\n",
      "|    total_timesteps    | 8240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 102999   |\n",
      "|    policy_loss        | 0.000365 |\n",
      "|    value_loss         | 0.00191  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8247360, episode_reward=18.00 +/- 1.67\n",
      "Episode length: 7728.80 +/- 1170.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.73e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8247360  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103091   |\n",
      "|    policy_loss        | -0.0031  |\n",
      "|    value_loss         | 0.00495  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.67e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103100   |\n",
      "|    time_elapsed       | 11236    |\n",
      "|    total_timesteps    | 8248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103099   |\n",
      "|    policy_loss        | 0.00516  |\n",
      "|    value_loss         | 0.00637  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.65e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103200   |\n",
      "|    time_elapsed       | 11243    |\n",
      "|    total_timesteps    | 8256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103199   |\n",
      "|    policy_loss        | -0.00669 |\n",
      "|    value_loss         | 0.00908  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103300   |\n",
      "|    time_elapsed       | 11249    |\n",
      "|    total_timesteps    | 8264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103299   |\n",
      "|    policy_loss        | -0.00585 |\n",
      "|    value_loss         | 0.00609  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103400   |\n",
      "|    time_elapsed       | 11256    |\n",
      "|    total_timesteps    | 8272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103399   |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 0.00364  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8272352, episode_reward=19.00 +/- 2.19\n",
      "Episode length: 8451.00 +/- 1308.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.45e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8272352  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103404   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.00561  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103500   |\n",
      "|    time_elapsed       | 11278    |\n",
      "|    total_timesteps    | 8280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103499   |\n",
      "|    policy_loss        | 0.00431  |\n",
      "|    value_loss         | 0.00429  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103600   |\n",
      "|    time_elapsed       | 11284    |\n",
      "|    total_timesteps    | 8288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103599   |\n",
      "|    policy_loss        | 0.0416   |\n",
      "|    value_loss         | 0.00936  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103700   |\n",
      "|    time_elapsed       | 11291    |\n",
      "|    total_timesteps    | 8296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103699   |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8297344, episode_reward=19.40 +/- 0.80\n",
      "Episode length: 8064.80 +/- 809.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.06e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8297344  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103716   |\n",
      "|    policy_loss        | -0.0939  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103800   |\n",
      "|    time_elapsed       | 11312    |\n",
      "|    total_timesteps    | 8304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103799   |\n",
      "|    policy_loss        | -0.0309  |\n",
      "|    value_loss         | 0.00253  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 103900   |\n",
      "|    time_elapsed       | 11319    |\n",
      "|    total_timesteps    | 8312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103899   |\n",
      "|    policy_loss        | -0.0398  |\n",
      "|    value_loss         | 0.0199   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.64e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 104000   |\n",
      "|    time_elapsed       | 11325    |\n",
      "|    total_timesteps    | 8320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 103999   |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.00595  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8322336, episode_reward=19.40 +/- 0.80\n",
      "Episode length: 7597.20 +/- 1099.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.6e+03  |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8322336  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104029   |\n",
      "|    policy_loss        | 0.00201  |\n",
      "|    value_loss         | 0.00684  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.63e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 104100   |\n",
      "|    time_elapsed       | 11346    |\n",
      "|    total_timesteps    | 8328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104099   |\n",
      "|    policy_loss        | -0.0219  |\n",
      "|    value_loss         | 0.00832  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 104200   |\n",
      "|    time_elapsed       | 11353    |\n",
      "|    total_timesteps    | 8336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104199   |\n",
      "|    policy_loss        | 0.0683   |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 104300   |\n",
      "|    time_elapsed       | 11359    |\n",
      "|    total_timesteps    | 8344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104299   |\n",
      "|    policy_loss        | 0.000296 |\n",
      "|    value_loss         | 0.00743  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8347328, episode_reward=19.40 +/- 0.80\n",
      "Episode length: 7907.60 +/- 965.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.91e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8347328  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104341   |\n",
      "|    policy_loss        | 0.00628  |\n",
      "|    value_loss         | 0.00423  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 104400   |\n",
      "|    time_elapsed       | 11380    |\n",
      "|    total_timesteps    | 8352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104399   |\n",
      "|    policy_loss        | 0.0192   |\n",
      "|    value_loss         | 0.00237  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.56e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 104500   |\n",
      "|    time_elapsed       | 11387    |\n",
      "|    total_timesteps    | 8360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104499   |\n",
      "|    policy_loss        | 0.000989 |\n",
      "|    value_loss         | 0.00216  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 104600   |\n",
      "|    time_elapsed       | 11394    |\n",
      "|    total_timesteps    | 8368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104599   |\n",
      "|    policy_loss        | 2.13e-05 |\n",
      "|    value_loss         | 0.00184  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8372320, episode_reward=14.60 +/- 4.22\n",
      "Episode length: 11158.80 +/- 3945.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8372320  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104653   |\n",
      "|    policy_loss        | -0.00889 |\n",
      "|    value_loss         | 0.00442  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 104700   |\n",
      "|    time_elapsed       | 11420    |\n",
      "|    total_timesteps    | 8376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104699   |\n",
      "|    policy_loss        | -0.0207  |\n",
      "|    value_loss         | 0.00176  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 104800   |\n",
      "|    time_elapsed       | 11427    |\n",
      "|    total_timesteps    | 8384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104799   |\n",
      "|    policy_loss        | 0.0106   |\n",
      "|    value_loss         | 0.00328  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 104900   |\n",
      "|    time_elapsed       | 11434    |\n",
      "|    total_timesteps    | 8392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104899   |\n",
      "|    policy_loss        | -0.0032  |\n",
      "|    value_loss         | 0.0197   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8397312, episode_reward=15.60 +/- 2.58\n",
      "Episode length: 10444.00 +/- 1369.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+04 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8397312  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104966   |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.00383  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 105000   |\n",
      "|    time_elapsed       | 11459    |\n",
      "|    total_timesteps    | 8400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 104999   |\n",
      "|    policy_loss        | 0.00126  |\n",
      "|    value_loss         | 0.00432  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105100   |\n",
      "|    time_elapsed       | 11466    |\n",
      "|    total_timesteps    | 8408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105099   |\n",
      "|    policy_loss        | 0.00989  |\n",
      "|    value_loss         | 0.00576  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105200   |\n",
      "|    time_elapsed       | 11473    |\n",
      "|    total_timesteps    | 8416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105199   |\n",
      "|    policy_loss        | -0.00158 |\n",
      "|    value_loss         | 0.00321  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8422304, episode_reward=18.20 +/- 1.33\n",
      "Episode length: 8624.20 +/- 1084.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.62e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8422304  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105278   |\n",
      "|    policy_loss        | 0.00579  |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 105300   |\n",
      "|    time_elapsed       | 11495    |\n",
      "|    total_timesteps    | 8424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105299   |\n",
      "|    policy_loss        | -0.00507 |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105400   |\n",
      "|    time_elapsed       | 11502    |\n",
      "|    total_timesteps    | 8432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105399   |\n",
      "|    policy_loss        | 0.00723  |\n",
      "|    value_loss         | 0.00453  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105500   |\n",
      "|    time_elapsed       | 11508    |\n",
      "|    total_timesteps    | 8440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105499   |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.0037   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8447296, episode_reward=19.80 +/- 1.47\n",
      "Episode length: 8314.80 +/- 643.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.31e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8447296  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105591   |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.00775  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 105600   |\n",
      "|    time_elapsed       | 11530    |\n",
      "|    total_timesteps    | 8448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105599   |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 105700   |\n",
      "|    time_elapsed       | 11537    |\n",
      "|    total_timesteps    | 8456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105699   |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.00425  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105800   |\n",
      "|    time_elapsed       | 11543    |\n",
      "|    total_timesteps    | 8464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105799   |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.00248  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 105900   |\n",
      "|    time_elapsed       | 11550    |\n",
      "|    total_timesteps    | 8472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105899   |\n",
      "|    policy_loss        | -0.0408  |\n",
      "|    value_loss         | 0.00645  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8472288, episode_reward=18.00 +/- 1.55\n",
      "Episode length: 9258.60 +/- 687.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.26e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8472288  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.592    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105903   |\n",
      "|    policy_loss        | -0.0678  |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106000   |\n",
      "|    time_elapsed       | 11573    |\n",
      "|    total_timesteps    | 8480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 105999   |\n",
      "|    policy_loss        | 0.0205   |\n",
      "|    value_loss         | 0.0059   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106100   |\n",
      "|    time_elapsed       | 11580    |\n",
      "|    total_timesteps    | 8488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106099   |\n",
      "|    policy_loss        | 0.0386   |\n",
      "|    value_loss         | 0.00611  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 106200   |\n",
      "|    time_elapsed       | 11586    |\n",
      "|    total_timesteps    | 8496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106199   |\n",
      "|    policy_loss        | -0.0477  |\n",
      "|    value_loss         | 0.007    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8497280, episode_reward=16.20 +/- 2.93\n",
      "Episode length: 9702.00 +/- 2084.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.7e+03  |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8497280  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106215   |\n",
      "|    policy_loss        | 0.0241   |\n",
      "|    value_loss         | 0.00211  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106300   |\n",
      "|    time_elapsed       | 11611    |\n",
      "|    total_timesteps    | 8504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106299   |\n",
      "|    policy_loss        | -0.00395 |\n",
      "|    value_loss         | 0.00486  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106400   |\n",
      "|    time_elapsed       | 11617    |\n",
      "|    total_timesteps    | 8512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106399   |\n",
      "|    policy_loss        | 0.0351   |\n",
      "|    value_loss         | 0.00673  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106500   |\n",
      "|    time_elapsed       | 11624    |\n",
      "|    total_timesteps    | 8520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106499   |\n",
      "|    policy_loss        | -0.0155  |\n",
      "|    value_loss         | 0.00478  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8522272, episode_reward=18.40 +/- 3.38\n",
      "Episode length: 8570.60 +/- 2127.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.57e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8522272  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106528   |\n",
      "|    policy_loss        | 0.0198   |\n",
      "|    value_loss         | 0.00314  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106600   |\n",
      "|    time_elapsed       | 11646    |\n",
      "|    total_timesteps    | 8528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106599   |\n",
      "|    policy_loss        | -0.0905  |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106700   |\n",
      "|    time_elapsed       | 11653    |\n",
      "|    total_timesteps    | 8536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106699   |\n",
      "|    policy_loss        | -0.00372 |\n",
      "|    value_loss         | 0.003    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106800   |\n",
      "|    time_elapsed       | 11659    |\n",
      "|    total_timesteps    | 8544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106799   |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8547264, episode_reward=17.00 +/- 3.58\n",
      "Episode length: 8839.80 +/- 1907.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.84e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8547264  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106840   |\n",
      "|    policy_loss        | 0.00302  |\n",
      "|    value_loss         | 0.00274  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 106900   |\n",
      "|    time_elapsed       | 11682    |\n",
      "|    total_timesteps    | 8552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106899   |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107000   |\n",
      "|    time_elapsed       | 11688    |\n",
      "|    total_timesteps    | 8560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 106999   |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.00566  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107100   |\n",
      "|    time_elapsed       | 11695    |\n",
      "|    total_timesteps    | 8568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107099   |\n",
      "|    policy_loss        | -0.0219  |\n",
      "|    value_loss         | 0.00733  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8572256, episode_reward=19.20 +/- 0.98\n",
      "Episode length: 8772.40 +/- 569.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.77e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8572256  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107153   |\n",
      "|    policy_loss        | 0.0176   |\n",
      "|    value_loss         | 0.00632  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.53e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 107200   |\n",
      "|    time_elapsed       | 11718    |\n",
      "|    total_timesteps    | 8576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107199   |\n",
      "|    policy_loss        | -0.0264  |\n",
      "|    value_loss         | 0.00439  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107300   |\n",
      "|    time_elapsed       | 11724    |\n",
      "|    total_timesteps    | 8584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107299   |\n",
      "|    policy_loss        | -0.019   |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107400   |\n",
      "|    time_elapsed       | 11731    |\n",
      "|    total_timesteps    | 8592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107399   |\n",
      "|    policy_loss        | -0.00775 |\n",
      "|    value_loss         | 0.00556  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8597248, episode_reward=19.80 +/- 0.98\n",
      "Episode length: 7064.20 +/- 860.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.06e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8597248  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107465   |\n",
      "|    policy_loss        | 0.00547  |\n",
      "|    value_loss         | 0.00738  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 107500   |\n",
      "|    time_elapsed       | 11750    |\n",
      "|    total_timesteps    | 8600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107499   |\n",
      "|    policy_loss        | -0.0475  |\n",
      "|    value_loss         | 0.00329  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107600   |\n",
      "|    time_elapsed       | 11757    |\n",
      "|    total_timesteps    | 8608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107599   |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107700   |\n",
      "|    time_elapsed       | 11764    |\n",
      "|    total_timesteps    | 8616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107699   |\n",
      "|    policy_loss        | -0.0219  |\n",
      "|    value_loss         | 0.00509  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8622240, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 7928.00 +/- 869.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.93e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8622240  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107777   |\n",
      "|    policy_loss        | -0.00553 |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 107800   |\n",
      "|    time_elapsed       | 11785    |\n",
      "|    total_timesteps    | 8624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107799   |\n",
      "|    policy_loss        | -0.04    |\n",
      "|    value_loss         | 0.00499  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.54e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 107900   |\n",
      "|    time_elapsed       | 11791    |\n",
      "|    total_timesteps    | 8632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107899   |\n",
      "|    policy_loss        | 0.0605   |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108000   |\n",
      "|    time_elapsed       | 11798    |\n",
      "|    total_timesteps    | 8640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 107999   |\n",
      "|    policy_loss        | 0.0493   |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8647232, episode_reward=18.40 +/- 1.36\n",
      "Episode length: 8200.80 +/- 1025.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.2e+03  |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8647232  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108090   |\n",
      "|    policy_loss        | -0.00578 |\n",
      "|    value_loss         | 0.00514  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 108100   |\n",
      "|    time_elapsed       | 11820    |\n",
      "|    total_timesteps    | 8648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108099   |\n",
      "|    policy_loss        | 0.0623   |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 108200   |\n",
      "|    time_elapsed       | 11826    |\n",
      "|    total_timesteps    | 8656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108199   |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.00333  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108300   |\n",
      "|    time_elapsed       | 11833    |\n",
      "|    total_timesteps    | 8664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108299   |\n",
      "|    policy_loss        | 0.013    |\n",
      "|    value_loss         | 0.00272  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108400   |\n",
      "|    time_elapsed       | 11839    |\n",
      "|    total_timesteps    | 8672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108399   |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.00314  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8672224, episode_reward=19.80 +/- 0.75\n",
      "Episode length: 7416.00 +/- 805.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.42e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8672224  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108402   |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.00217  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 108500   |\n",
      "|    time_elapsed       | 11859    |\n",
      "|    total_timesteps    | 8680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108499   |\n",
      "|    policy_loss        | -0.0264  |\n",
      "|    value_loss         | 0.00846  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108600   |\n",
      "|    time_elapsed       | 11866    |\n",
      "|    total_timesteps    | 8688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108599   |\n",
      "|    policy_loss        | -0.00975 |\n",
      "|    value_loss         | 0.00627  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108700   |\n",
      "|    time_elapsed       | 11872    |\n",
      "|    total_timesteps    | 8696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108699   |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8697216, episode_reward=19.60 +/- 0.49\n",
      "Episode length: 7692.40 +/- 1037.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.69e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8697216  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108715   |\n",
      "|    policy_loss        | -0.0173  |\n",
      "|    value_loss         | 0.00288  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 108800   |\n",
      "|    time_elapsed       | 11893    |\n",
      "|    total_timesteps    | 8704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108799   |\n",
      "|    policy_loss        | -0.05    |\n",
      "|    value_loss         | 0.00681  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 108900   |\n",
      "|    time_elapsed       | 11900    |\n",
      "|    total_timesteps    | 8712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108899   |\n",
      "|    policy_loss        | 0.0318   |\n",
      "|    value_loss         | 0.00923  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 109000   |\n",
      "|    time_elapsed       | 11906    |\n",
      "|    total_timesteps    | 8720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 108999   |\n",
      "|    policy_loss        | -0.0177  |\n",
      "|    value_loss         | 0.00335  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8722208, episode_reward=19.60 +/- 0.49\n",
      "Episode length: 7735.20 +/- 1144.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.74e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8722208  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109027   |\n",
      "|    policy_loss        | 0.0054   |\n",
      "|    value_loss         | 0.00254  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109100   |\n",
      "|    time_elapsed       | 11927    |\n",
      "|    total_timesteps    | 8728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109099   |\n",
      "|    policy_loss        | -0.00735 |\n",
      "|    value_loss         | 0.00545  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 109200   |\n",
      "|    time_elapsed       | 11934    |\n",
      "|    total_timesteps    | 8736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109199   |\n",
      "|    policy_loss        | -0.00747 |\n",
      "|    value_loss         | 0.00559  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 109300   |\n",
      "|    time_elapsed       | 11940    |\n",
      "|    total_timesteps    | 8744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109299   |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.00394  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8747200, episode_reward=18.80 +/- 1.17\n",
      "Episode length: 8744.20 +/- 1022.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.74e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8747200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109339   |\n",
      "|    policy_loss        | 0.0243   |\n",
      "|    value_loss         | 0.00723  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109400   |\n",
      "|    time_elapsed       | 11963    |\n",
      "|    total_timesteps    | 8752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109399   |\n",
      "|    policy_loss        | 0.0889   |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109500   |\n",
      "|    time_elapsed       | 11969    |\n",
      "|    total_timesteps    | 8760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109499   |\n",
      "|    policy_loss        | -0.0136  |\n",
      "|    value_loss         | 0.00449  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 109600   |\n",
      "|    time_elapsed       | 11976    |\n",
      "|    total_timesteps    | 8768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109599   |\n",
      "|    policy_loss        | -0.0398  |\n",
      "|    value_loss         | 0.00283  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8772192, episode_reward=19.60 +/- 0.49\n",
      "Episode length: 8258.20 +/- 891.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.26e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8772192  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109652   |\n",
      "|    policy_loss        | -0.0297  |\n",
      "|    value_loss         | 0.00301  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109700   |\n",
      "|    time_elapsed       | 11998    |\n",
      "|    total_timesteps    | 8776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109699   |\n",
      "|    policy_loss        | -0.0201  |\n",
      "|    value_loss         | 0.00343  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109800   |\n",
      "|    time_elapsed       | 12004    |\n",
      "|    total_timesteps    | 8784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109799   |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 0.00221  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 109900   |\n",
      "|    time_elapsed       | 12011    |\n",
      "|    total_timesteps    | 8792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109899   |\n",
      "|    policy_loss        | 0.0187   |\n",
      "|    value_loss         | 0.00515  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8797184, episode_reward=16.20 +/- 2.99\n",
      "Episode length: 9556.00 +/- 2954.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.56e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8797184  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109964   |\n",
      "|    policy_loss        | -0.0753  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110000   |\n",
      "|    time_elapsed       | 12035    |\n",
      "|    total_timesteps    | 8800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 109999   |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    value_loss         | 0.00404  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110100   |\n",
      "|    time_elapsed       | 12041    |\n",
      "|    total_timesteps    | 8808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110099   |\n",
      "|    policy_loss        | 0.0337   |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110200   |\n",
      "|    time_elapsed       | 12048    |\n",
      "|    total_timesteps    | 8816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110199   |\n",
      "|    policy_loss        | 0.00194  |\n",
      "|    value_loss         | 0.00983  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8822176, episode_reward=18.20 +/- 1.94\n",
      "Episode length: 8607.20 +/- 1067.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.61e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8822176  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110277   |\n",
      "|    policy_loss        | 0.0213   |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.35e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110300   |\n",
      "|    time_elapsed       | 12070    |\n",
      "|    total_timesteps    | 8824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110299   |\n",
      "|    policy_loss        | -0.0302  |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110400   |\n",
      "|    time_elapsed       | 12077    |\n",
      "|    total_timesteps    | 8832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110399   |\n",
      "|    policy_loss        | 0.0531   |\n",
      "|    value_loss         | 0.0169   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110500   |\n",
      "|    time_elapsed       | 12083    |\n",
      "|    total_timesteps    | 8840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110499   |\n",
      "|    policy_loss        | 0.00231  |\n",
      "|    value_loss         | 0.00798  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8847168, episode_reward=17.60 +/- 2.73\n",
      "Episode length: 8839.60 +/- 897.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.84e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8847168  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110589   |\n",
      "|    policy_loss        | -0.0762  |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 110600   |\n",
      "|    time_elapsed       | 12106    |\n",
      "|    total_timesteps    | 8848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110599   |\n",
      "|    policy_loss        | -0.0156  |\n",
      "|    value_loss         | 0.00551  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110700   |\n",
      "|    time_elapsed       | 12113    |\n",
      "|    total_timesteps    | 8856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110699   |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.00626  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.55e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110800   |\n",
      "|    time_elapsed       | 12119    |\n",
      "|    total_timesteps    | 8864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110799   |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.00641  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 110900   |\n",
      "|    time_elapsed       | 12126    |\n",
      "|    total_timesteps    | 8872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110899   |\n",
      "|    policy_loss        | 0.00886  |\n",
      "|    value_loss         | 0.007    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8872160, episode_reward=16.40 +/- 1.96\n",
      "Episode length: 8749.20 +/- 1445.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.75e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8872160  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110901   |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.6e+03  |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 111000   |\n",
      "|    time_elapsed       | 12148    |\n",
      "|    total_timesteps    | 8880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 110999   |\n",
      "|    policy_loss        | 0.00993  |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 111100   |\n",
      "|    time_elapsed       | 12155    |\n",
      "|    total_timesteps    | 8888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111099   |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.00593  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.52e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 111200   |\n",
      "|    time_elapsed       | 12162    |\n",
      "|    total_timesteps    | 8896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111199   |\n",
      "|    policy_loss        | -0.0571  |\n",
      "|    value_loss         | 0.00969  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8897152, episode_reward=18.00 +/- 1.26\n",
      "Episode length: 8130.20 +/- 770.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.13e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8897152  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111214   |\n",
      "|    policy_loss        | -0.0493  |\n",
      "|    value_loss         | 0.0123   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 111300   |\n",
      "|    time_elapsed       | 12183    |\n",
      "|    total_timesteps    | 8904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111299   |\n",
      "|    policy_loss        | -0.0527  |\n",
      "|    value_loss         | 0.00789  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.63e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 111400   |\n",
      "|    time_elapsed       | 12190    |\n",
      "|    total_timesteps    | 8912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111399   |\n",
      "|    policy_loss        | -0.0401  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.62e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 111500   |\n",
      "|    time_elapsed       | 12196    |\n",
      "|    total_timesteps    | 8920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111499   |\n",
      "|    policy_loss        | -0.00969 |\n",
      "|    value_loss         | 0.0141   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8922144, episode_reward=18.00 +/- 1.41\n",
      "Episode length: 8022.20 +/- 446.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.02e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8922144  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111526   |\n",
      "|    policy_loss        | 0.0386   |\n",
      "|    value_loss         | 0.00788  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.65e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 111600   |\n",
      "|    time_elapsed       | 12218    |\n",
      "|    total_timesteps    | 8928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111599   |\n",
      "|    policy_loss        | 0.0109   |\n",
      "|    value_loss         | 0.0042   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.72e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 111700   |\n",
      "|    time_elapsed       | 12224    |\n",
      "|    total_timesteps    | 8936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111699   |\n",
      "|    policy_loss        | 0.00895  |\n",
      "|    value_loss         | 0.0048   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.72e+03 |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 111800   |\n",
      "|    time_elapsed       | 12231    |\n",
      "|    total_timesteps    | 8944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111799   |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.00927  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8947136, episode_reward=19.20 +/- 0.75\n",
      "Episode length: 6885.20 +/- 513.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.89e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8947136  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111839   |\n",
      "|    policy_loss        | 0.0394   |\n",
      "|    value_loss         | 0.00561  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.88e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 111900   |\n",
      "|    time_elapsed       | 12250    |\n",
      "|    total_timesteps    | 8952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111899   |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.00503  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.9e+03  |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 112000   |\n",
      "|    time_elapsed       | 12257    |\n",
      "|    total_timesteps    | 8960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 111999   |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.00686  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.91e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 112100   |\n",
      "|    time_elapsed       | 12263    |\n",
      "|    total_timesteps    | 8968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112099   |\n",
      "|    policy_loss        | -0.0206  |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8972128, episode_reward=19.40 +/- 1.20\n",
      "Episode length: 7652.40 +/- 915.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.65e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8972128  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112151   |\n",
      "|    policy_loss        | 0.00591  |\n",
      "|    value_loss         | 0.00788  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.89e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112200   |\n",
      "|    time_elapsed       | 12284    |\n",
      "|    total_timesteps    | 8976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112199   |\n",
      "|    policy_loss        | 0.0136   |\n",
      "|    value_loss         | 0.00612  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.89e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112300   |\n",
      "|    time_elapsed       | 12290    |\n",
      "|    total_timesteps    | 8984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112299   |\n",
      "|    policy_loss        | 0.0156   |\n",
      "|    value_loss         | 0.00381  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.81e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 112400   |\n",
      "|    time_elapsed       | 12297    |\n",
      "|    total_timesteps    | 8992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112399   |\n",
      "|    policy_loss        | 0.0183   |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8997120, episode_reward=19.80 +/- 0.40\n",
      "Episode length: 7715.80 +/- 757.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.72e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8997120  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112463   |\n",
      "|    policy_loss        | -0.0024  |\n",
      "|    value_loss         | 0.00535  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.86e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112500   |\n",
      "|    time_elapsed       | 12318    |\n",
      "|    total_timesteps    | 9000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112499   |\n",
      "|    policy_loss        | -0.00368 |\n",
      "|    value_loss         | 0.00216  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.87e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112600   |\n",
      "|    time_elapsed       | 12324    |\n",
      "|    total_timesteps    | 9008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112599   |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.00873  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.86e+03 |\n",
      "|    ep_rew_mean        | 17.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 112700   |\n",
      "|    time_elapsed       | 12331    |\n",
      "|    total_timesteps    | 9016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112699   |\n",
      "|    policy_loss        | -0.0144  |\n",
      "|    value_loss         | 0.00659  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9022112, episode_reward=17.60 +/- 1.36\n",
      "Episode length: 8568.00 +/- 1297.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.57e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9022112  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112776   |\n",
      "|    policy_loss        | -0.00973 |\n",
      "|    value_loss         | 0.00612  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.84e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112800   |\n",
      "|    time_elapsed       | 12353    |\n",
      "|    total_timesteps    | 9024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112799   |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.81e+03 |\n",
      "|    ep_rew_mean        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 112900   |\n",
      "|    time_elapsed       | 12360    |\n",
      "|    total_timesteps    | 9032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112899   |\n",
      "|    policy_loss        | 0.0193   |\n",
      "|    value_loss         | 0.0095   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.74e+03 |\n",
      "|    ep_rew_mean        | 17.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113000   |\n",
      "|    time_elapsed       | 12366    |\n",
      "|    total_timesteps    | 9040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 112999   |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.0061   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9047104, episode_reward=19.00 +/- 1.67\n",
      "Episode length: 8089.00 +/- 1158.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.09e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9047104  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113088   |\n",
      "|    policy_loss        | -0.00182 |\n",
      "|    value_loss         | 0.00239  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.69e+03 |\n",
      "|    ep_rew_mean        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113100   |\n",
      "|    time_elapsed       | 12388    |\n",
      "|    total_timesteps    | 9048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113099   |\n",
      "|    policy_loss        | 0.00561  |\n",
      "|    value_loss         | 0.00627  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.66e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113200   |\n",
      "|    time_elapsed       | 12394    |\n",
      "|    total_timesteps    | 9056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113199   |\n",
      "|    policy_loss        | -0.00551 |\n",
      "|    value_loss         | 0.00166  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.67e+03 |\n",
      "|    ep_rew_mean        | 17.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113300   |\n",
      "|    time_elapsed       | 12401    |\n",
      "|    total_timesteps    | 9064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113299   |\n",
      "|    policy_loss        | -0.00293 |\n",
      "|    value_loss         | 0.00484  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 113400   |\n",
      "|    time_elapsed       | 12407    |\n",
      "|    total_timesteps    | 9072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113399   |\n",
      "|    policy_loss        | 0.0365   |\n",
      "|    value_loss         | 0.00835  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9072096, episode_reward=19.40 +/- 1.36\n",
      "Episode length: 8344.00 +/- 303.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.34e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9072096  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113401   |\n",
      "|    policy_loss        | -0.0606  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.59e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113500   |\n",
      "|    time_elapsed       | 12429    |\n",
      "|    total_timesteps    | 9080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113499   |\n",
      "|    policy_loss        | -0.00652 |\n",
      "|    value_loss         | 0.00246  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.57e+03 |\n",
      "|    ep_rew_mean        | 18.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113600   |\n",
      "|    time_elapsed       | 12436    |\n",
      "|    total_timesteps    | 9088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113599   |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.00263  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.48e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 731      |\n",
      "|    iterations         | 113700   |\n",
      "|    time_elapsed       | 12442    |\n",
      "|    total_timesteps    | 9096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113699   |\n",
      "|    policy_loss        | 0.0127   |\n",
      "|    value_loss         | 0.00606  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9097088, episode_reward=18.00 +/- 3.03\n",
      "Episode length: 8365.00 +/- 1462.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.36e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9097088  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113713   |\n",
      "|    policy_loss        | 0.0196   |\n",
      "|    value_loss         | 0.00531  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.51e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113800   |\n",
      "|    time_elapsed       | 12464    |\n",
      "|    total_timesteps    | 9104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113799   |\n",
      "|    policy_loss        | -0.0101  |\n",
      "|    value_loss         | 0.00247  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 113900   |\n",
      "|    time_elapsed       | 12471    |\n",
      "|    total_timesteps    | 9112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113899   |\n",
      "|    policy_loss        | 0.0198   |\n",
      "|    value_loss         | 0.00171  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114000   |\n",
      "|    time_elapsed       | 12477    |\n",
      "|    total_timesteps    | 9120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 113999   |\n",
      "|    policy_loss        | 0.028    |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9122080, episode_reward=18.60 +/- 1.50\n",
      "Episode length: 8380.80 +/- 1036.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.38e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9122080  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114025   |\n",
      "|    policy_loss        | -0.032   |\n",
      "|    value_loss         | 0.00352  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114100   |\n",
      "|    time_elapsed       | 12499    |\n",
      "|    total_timesteps    | 9128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114099   |\n",
      "|    policy_loss        | -0.00316 |\n",
      "|    value_loss         | 0.00592  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114200   |\n",
      "|    time_elapsed       | 12506    |\n",
      "|    total_timesteps    | 9136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114199   |\n",
      "|    policy_loss        | -0.0388  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114300   |\n",
      "|    time_elapsed       | 12512    |\n",
      "|    total_timesteps    | 9144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114299   |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.00141  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9147072, episode_reward=17.80 +/- 2.04\n",
      "Episode length: 7853.20 +/- 1305.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.85e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9147072  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114338   |\n",
      "|    policy_loss        | 0.0414   |\n",
      "|    value_loss         | 0.00366  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114400   |\n",
      "|    time_elapsed       | 12533    |\n",
      "|    total_timesteps    | 9152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114399   |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114500   |\n",
      "|    time_elapsed       | 12540    |\n",
      "|    total_timesteps    | 9160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114499   |\n",
      "|    policy_loss        | 0.0422   |\n",
      "|    value_loss         | 0.007    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114600   |\n",
      "|    time_elapsed       | 12547    |\n",
      "|    total_timesteps    | 9168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114599   |\n",
      "|    policy_loss        | -0.00764 |\n",
      "|    value_loss         | 0.00293  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9172064, episode_reward=18.20 +/- 3.19\n",
      "Episode length: 9016.20 +/- 1863.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.02e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9172064  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114650   |\n",
      "|    policy_loss        | 0.00705  |\n",
      "|    value_loss         | 0.00315  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 114700   |\n",
      "|    time_elapsed       | 12569    |\n",
      "|    total_timesteps    | 9176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114699   |\n",
      "|    policy_loss        | 0.028    |\n",
      "|    value_loss         | 0.00537  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114800   |\n",
      "|    time_elapsed       | 12576    |\n",
      "|    total_timesteps    | 9184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114799   |\n",
      "|    policy_loss        | -0.00266 |\n",
      "|    value_loss         | 0.00313  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 114900   |\n",
      "|    time_elapsed       | 12583    |\n",
      "|    total_timesteps    | 9192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114899   |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.00489  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9197056, episode_reward=19.40 +/- 1.02\n",
      "Episode length: 7029.00 +/- 556.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.03e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9197056  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114963   |\n",
      "|    policy_loss        | 0.0175   |\n",
      "|    value_loss         | 0.00219  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115000   |\n",
      "|    time_elapsed       | 12602    |\n",
      "|    total_timesteps    | 9200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 114999   |\n",
      "|    policy_loss        | -0.0328  |\n",
      "|    value_loss         | 0.00502  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115100   |\n",
      "|    time_elapsed       | 12609    |\n",
      "|    total_timesteps    | 9208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115099   |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115200   |\n",
      "|    time_elapsed       | 12615    |\n",
      "|    total_timesteps    | 9216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115199   |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    value_loss         | 0.00667  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9222048, episode_reward=17.80 +/- 1.47\n",
      "Episode length: 8342.40 +/- 996.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.34e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9222048  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115275   |\n",
      "|    policy_loss        | -0.00391 |\n",
      "|    value_loss         | 0.00543  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 115300   |\n",
      "|    time_elapsed       | 12637    |\n",
      "|    total_timesteps    | 9224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115299   |\n",
      "|    policy_loss        | 0.0299   |\n",
      "|    value_loss         | 0.0095   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115400   |\n",
      "|    time_elapsed       | 12644    |\n",
      "|    total_timesteps    | 9232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115399   |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115500   |\n",
      "|    time_elapsed       | 12650    |\n",
      "|    total_timesteps    | 9240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115499   |\n",
      "|    policy_loss        | 0.00672  |\n",
      "|    value_loss         | 0.00182  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9247040, episode_reward=18.40 +/- 2.06\n",
      "Episode length: 9030.40 +/- 859.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.03e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9247040  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115587   |\n",
      "|    policy_loss        | -0.0544  |\n",
      "|    value_loss         | 0.00441  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 115600   |\n",
      "|    time_elapsed       | 12673    |\n",
      "|    total_timesteps    | 9248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115599   |\n",
      "|    policy_loss        | -0.00073 |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 115700   |\n",
      "|    time_elapsed       | 12680    |\n",
      "|    total_timesteps    | 9256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115699   |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.00226  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.44e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115800   |\n",
      "|    time_elapsed       | 12687    |\n",
      "|    total_timesteps    | 9264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115799   |\n",
      "|    policy_loss        | 0.0139   |\n",
      "|    value_loss         | 0.00544  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 115900   |\n",
      "|    time_elapsed       | 12693    |\n",
      "|    total_timesteps    | 9272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115899   |\n",
      "|    policy_loss        | -0.0365  |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9272032, episode_reward=18.00 +/- 1.67\n",
      "Episode length: 7766.40 +/- 721.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.77e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9272032  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115900   |\n",
      "|    policy_loss        | -0.0434  |\n",
      "|    value_loss         | 0.00896  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116000   |\n",
      "|    time_elapsed       | 12714    |\n",
      "|    total_timesteps    | 9280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 115999   |\n",
      "|    policy_loss        | 0.00244  |\n",
      "|    value_loss         | 0.00333  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 116100   |\n",
      "|    time_elapsed       | 12721    |\n",
      "|    total_timesteps    | 9288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116099   |\n",
      "|    policy_loss        | 0.0329   |\n",
      "|    value_loss         | 0.00791  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 116200   |\n",
      "|    time_elapsed       | 12727    |\n",
      "|    total_timesteps    | 9296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116199   |\n",
      "|    policy_loss        | 0.0348   |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9297024, episode_reward=16.20 +/- 1.72\n",
      "Episode length: 8370.80 +/- 1084.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.37e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9297024  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116212   |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 0.00931  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116300   |\n",
      "|    time_elapsed       | 12749    |\n",
      "|    total_timesteps    | 9304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116299   |\n",
      "|    policy_loss        | -0.0428  |\n",
      "|    value_loss         | 0.00487  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116400   |\n",
      "|    time_elapsed       | 12756    |\n",
      "|    total_timesteps    | 9312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116399   |\n",
      "|    policy_loss        | -0.00573 |\n",
      "|    value_loss         | 0.00147  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 116500   |\n",
      "|    time_elapsed       | 12762    |\n",
      "|    total_timesteps    | 9320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116499   |\n",
      "|    policy_loss        | -0.0605  |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9322016, episode_reward=19.40 +/- 1.20\n",
      "Episode length: 8226.80 +/- 482.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.23e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9322016  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116525   |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.00418  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116600   |\n",
      "|    time_elapsed       | 12784    |\n",
      "|    total_timesteps    | 9328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116599   |\n",
      "|    policy_loss        | 0.0216   |\n",
      "|    value_loss         | 0.00229  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116700   |\n",
      "|    time_elapsed       | 12791    |\n",
      "|    total_timesteps    | 9336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116699   |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.00753  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.37e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 116800   |\n",
      "|    time_elapsed       | 12797    |\n",
      "|    total_timesteps    | 9344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116799   |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.00239  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9347008, episode_reward=18.20 +/- 1.72\n",
      "Episode length: 8413.00 +/- 807.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.41e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9347008  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116837   |\n",
      "|    policy_loss        | -0.00403 |\n",
      "|    value_loss         | 0.00256  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 116900   |\n",
      "|    time_elapsed       | 12819    |\n",
      "|    total_timesteps    | 9352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116899   |\n",
      "|    policy_loss        | 0.0666   |\n",
      "|    value_loss         | 0.00773  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117000   |\n",
      "|    time_elapsed       | 12826    |\n",
      "|    total_timesteps    | 9360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 116999   |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    value_loss         | 0.00392  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 730      |\n",
      "|    iterations         | 117100   |\n",
      "|    time_elapsed       | 12832    |\n",
      "|    total_timesteps    | 9368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117099   |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    value_loss         | 0.00827  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9372000, episode_reward=19.60 +/- 1.36\n",
      "Episode length: 7721.60 +/- 1259.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.72e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9372000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117149   |\n",
      "|    policy_loss        | -0.0088  |\n",
      "|    value_loss         | 0.00711  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117200   |\n",
      "|    time_elapsed       | 12853    |\n",
      "|    total_timesteps    | 9376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117199   |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.022    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.3e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117300   |\n",
      "|    time_elapsed       | 12860    |\n",
      "|    total_timesteps    | 9384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117299   |\n",
      "|    policy_loss        | 0.00073  |\n",
      "|    value_loss         | 0.00298  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117400   |\n",
      "|    time_elapsed       | 12866    |\n",
      "|    total_timesteps    | 9392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117399   |\n",
      "|    policy_loss        | 0.000285 |\n",
      "|    value_loss         | 0.00262  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9396992, episode_reward=17.80 +/- 1.94\n",
      "Episode length: 10032.20 +/- 1636.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+04    |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9396992  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117462   |\n",
      "|    policy_loss        | -0.0584  |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117500   |\n",
      "|    time_elapsed       | 12891    |\n",
      "|    total_timesteps    | 9400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117499   |\n",
      "|    policy_loss        | 0.0598   |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117600   |\n",
      "|    time_elapsed       | 12898    |\n",
      "|    total_timesteps    | 9408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117599   |\n",
      "|    policy_loss        | -0.0502  |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117700   |\n",
      "|    time_elapsed       | 12904    |\n",
      "|    total_timesteps    | 9416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117699   |\n",
      "|    policy_loss        | 0.0195   |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9421984, episode_reward=17.80 +/- 2.99\n",
      "Episode length: 8208.40 +/- 2109.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.21e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9421984  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117774   |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.00528  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117800   |\n",
      "|    time_elapsed       | 12926    |\n",
      "|    total_timesteps    | 9424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117799   |\n",
      "|    policy_loss        | 0.00513  |\n",
      "|    value_loss         | 0.00605  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.17e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 117900   |\n",
      "|    time_elapsed       | 12933    |\n",
      "|    total_timesteps    | 9432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117899   |\n",
      "|    policy_loss        | 0.0252   |\n",
      "|    value_loss         | 0.00306  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118000   |\n",
      "|    time_elapsed       | 12939    |\n",
      "|    total_timesteps    | 9440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 117999   |\n",
      "|    policy_loss        | -0.0014  |\n",
      "|    value_loss         | 0.0046   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9446976, episode_reward=19.20 +/- 0.75\n",
      "Episode length: 7707.40 +/- 1197.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.71e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9446976  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118087   |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.00336  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.26e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 118100   |\n",
      "|    time_elapsed       | 12960    |\n",
      "|    total_timesteps    | 9448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118099   |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.00596  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118200   |\n",
      "|    time_elapsed       | 12966    |\n",
      "|    total_timesteps    | 9456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118199   |\n",
      "|    policy_loss        | 0.0208   |\n",
      "|    value_loss         | 0.00895  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118300   |\n",
      "|    time_elapsed       | 12973    |\n",
      "|    total_timesteps    | 9464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118299   |\n",
      "|    policy_loss        | 0.0274   |\n",
      "|    value_loss         | 0.0037   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9471968, episode_reward=17.00 +/- 2.10\n",
      "Episode length: 9351.40 +/- 730.51\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 9.35e+03  |\n",
      "|    mean_reward        | 17        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 9471968   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | 0.99      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 118399    |\n",
      "|    policy_loss        | -0.000512 |\n",
      "|    value_loss         | 0.00219   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23e+03 |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 728      |\n",
      "|    iterations      | 118400   |\n",
      "|    time_elapsed    | 12997    |\n",
      "|    total_timesteps | 9472000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118500   |\n",
      "|    time_elapsed       | 13003    |\n",
      "|    total_timesteps    | 9480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118499   |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118600   |\n",
      "|    time_elapsed       | 13010    |\n",
      "|    total_timesteps    | 9488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118599   |\n",
      "|    policy_loss        | 0.00586  |\n",
      "|    value_loss         | 0.00804  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.29e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 118700   |\n",
      "|    time_elapsed       | 13017    |\n",
      "|    total_timesteps    | 9496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118699   |\n",
      "|    policy_loss        | -0.0099  |\n",
      "|    value_loss         | 0.00653  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9496960, episode_reward=15.60 +/- 2.73\n",
      "Episode length: 10310.40 +/- 2041.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+04 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9496960  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118711   |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.00487  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 118800   |\n",
      "|    time_elapsed       | 13042    |\n",
      "|    total_timesteps    | 9504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118799   |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.00523  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 118900   |\n",
      "|    time_elapsed       | 13049    |\n",
      "|    total_timesteps    | 9512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118899   |\n",
      "|    policy_loss        | 0.00431  |\n",
      "|    value_loss         | 0.00644  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 119000   |\n",
      "|    time_elapsed       | 13055    |\n",
      "|    total_timesteps    | 9520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 118999   |\n",
      "|    policy_loss        | -0.00588 |\n",
      "|    value_loss         | 0.00982  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9521952, episode_reward=20.00 +/- 0.63\n",
      "Episode length: 7203.40 +/- 834.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.2e+03  |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9521952  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119024   |\n",
      "|    policy_loss        | 0.0377   |\n",
      "|    value_loss         | 0.005    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119100   |\n",
      "|    time_elapsed       | 13075    |\n",
      "|    total_timesteps    | 9528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119099   |\n",
      "|    policy_loss        | -0.0663  |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119200   |\n",
      "|    time_elapsed       | 13081    |\n",
      "|    total_timesteps    | 9536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119199   |\n",
      "|    policy_loss        | 1.62e-05 |\n",
      "|    value_loss         | 0.00477  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 119300   |\n",
      "|    time_elapsed       | 13088    |\n",
      "|    total_timesteps    | 9544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119299   |\n",
      "|    policy_loss        | 0.0084   |\n",
      "|    value_loss         | 0.00276  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9546944, episode_reward=18.20 +/- 1.47\n",
      "Episode length: 7672.20 +/- 1084.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.67e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9546944  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119336   |\n",
      "|    policy_loss        | -0.0225  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.41e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119400   |\n",
      "|    time_elapsed       | 13109    |\n",
      "|    total_timesteps    | 9552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119399   |\n",
      "|    policy_loss        | 0.00726  |\n",
      "|    value_loss         | 0.0053   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.5e+03  |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119500   |\n",
      "|    time_elapsed       | 13115    |\n",
      "|    total_timesteps    | 9560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119499   |\n",
      "|    policy_loss        | -0.0124  |\n",
      "|    value_loss         | 0.00232  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.47e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 119600   |\n",
      "|    time_elapsed       | 13122    |\n",
      "|    total_timesteps    | 9568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119599   |\n",
      "|    policy_loss        | 0.00166  |\n",
      "|    value_loss         | 0.00446  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9571936, episode_reward=19.00 +/- 1.10\n",
      "Episode length: 8033.60 +/- 1029.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.03e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9571936  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119649   |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.00325  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119700   |\n",
      "|    time_elapsed       | 13143    |\n",
      "|    total_timesteps    | 9576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119699   |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.00323  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 119800   |\n",
      "|    time_elapsed       | 13150    |\n",
      "|    total_timesteps    | 9584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119799   |\n",
      "|    policy_loss        | 0.0272   |\n",
      "|    value_loss         | 0.00232  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.4e+03  |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 729      |\n",
      "|    iterations         | 119900   |\n",
      "|    time_elapsed       | 13157    |\n",
      "|    total_timesteps    | 9592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119899   |\n",
      "|    policy_loss        | 0.00711  |\n",
      "|    value_loss         | 0.00328  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9596928, episode_reward=19.00 +/- 0.89\n",
      "Episode length: 8137.80 +/- 1521.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.14e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9596928  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119961   |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.00219  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.36e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120000   |\n",
      "|    time_elapsed       | 13178    |\n",
      "|    total_timesteps    | 9600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 119999   |\n",
      "|    policy_loss        | 0.025    |\n",
      "|    value_loss         | 0.000942 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.45e+03 |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120100   |\n",
      "|    time_elapsed       | 13184    |\n",
      "|    total_timesteps    | 9608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120099   |\n",
      "|    policy_loss        | 0.01     |\n",
      "|    value_loss         | 0.0061   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.43e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120200   |\n",
      "|    time_elapsed       | 13191    |\n",
      "|    total_timesteps    | 9616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120199   |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.0088   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9621920, episode_reward=18.80 +/- 2.93\n",
      "Episode length: 8207.80 +/- 2166.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.21e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9621920  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120273   |\n",
      "|    policy_loss        | 0.0356   |\n",
      "|    value_loss         | 0.00435  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.42e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120300   |\n",
      "|    time_elapsed       | 13213    |\n",
      "|    total_timesteps    | 9624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120299   |\n",
      "|    policy_loss        | 0.0163   |\n",
      "|    value_loss         | 0.00302  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120400   |\n",
      "|    time_elapsed       | 13219    |\n",
      "|    total_timesteps    | 9632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120399   |\n",
      "|    policy_loss        | -0.00322 |\n",
      "|    value_loss         | 0.00936  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.39e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120500   |\n",
      "|    time_elapsed       | 13226    |\n",
      "|    total_timesteps    | 9640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120499   |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.00668  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9646912, episode_reward=18.80 +/- 1.47\n",
      "Episode length: 7865.60 +/- 751.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.87e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9646912  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120586   |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    value_loss         | 0.00982  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120600   |\n",
      "|    time_elapsed       | 13247    |\n",
      "|    total_timesteps    | 9648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120599   |\n",
      "|    policy_loss        | -0.024   |\n",
      "|    value_loss         | 0.00242  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120700   |\n",
      "|    time_elapsed       | 13253    |\n",
      "|    total_timesteps    | 9656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120699   |\n",
      "|    policy_loss        | 0.0435   |\n",
      "|    value_loss         | 0.00767  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120800   |\n",
      "|    time_elapsed       | 13260    |\n",
      "|    total_timesteps    | 9664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120799   |\n",
      "|    policy_loss        | 0.000622 |\n",
      "|    value_loss         | 0.0084   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9671904, episode_reward=18.40 +/- 1.20\n",
      "Episode length: 8834.00 +/- 622.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.83e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9671904  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120898   |\n",
      "|    policy_loss        | 0.00303  |\n",
      "|    value_loss         | 0.00373  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.22e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 120900   |\n",
      "|    time_elapsed       | 13283    |\n",
      "|    total_timesteps    | 9672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120899   |\n",
      "|    policy_loss        | -0.00178 |\n",
      "|    value_loss         | 0.00444  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.2e+03  |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121000   |\n",
      "|    time_elapsed       | 13289    |\n",
      "|    total_timesteps    | 9680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 120999   |\n",
      "|    policy_loss        | -0.0228  |\n",
      "|    value_loss         | 0.00349  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121100   |\n",
      "|    time_elapsed       | 13296    |\n",
      "|    total_timesteps    | 9688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121099   |\n",
      "|    policy_loss        | -0.00083 |\n",
      "|    value_loss         | 0.00314  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121200   |\n",
      "|    time_elapsed       | 13302    |\n",
      "|    total_timesteps    | 9696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121199   |\n",
      "|    policy_loss        | -0.0127  |\n",
      "|    value_loss         | 0.00634  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9696896, episode_reward=18.00 +/- 2.53\n",
      "Episode length: 9243.20 +/- 1679.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.24e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9696896  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121211   |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.00526  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.19e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121300   |\n",
      "|    time_elapsed       | 13326    |\n",
      "|    total_timesteps    | 9704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121299   |\n",
      "|    policy_loss        | 0.0086   |\n",
      "|    value_loss         | 0.00248  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121400   |\n",
      "|    time_elapsed       | 13332    |\n",
      "|    total_timesteps    | 9712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121399   |\n",
      "|    policy_loss        | -0.0229  |\n",
      "|    value_loss         | 0.00718  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121500   |\n",
      "|    time_elapsed       | 13339    |\n",
      "|    total_timesteps    | 9720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121499   |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 0.0045   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9721888, episode_reward=19.40 +/- 0.80\n",
      "Episode length: 8450.40 +/- 633.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.45e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9721888  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121523   |\n",
      "|    policy_loss        | -0.0478  |\n",
      "|    value_loss         | 0.00567  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121600   |\n",
      "|    time_elapsed       | 13361    |\n",
      "|    total_timesteps    | 9728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121599   |\n",
      "|    policy_loss        | -0.0228  |\n",
      "|    value_loss         | 0.0024   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.21e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121700   |\n",
      "|    time_elapsed       | 13368    |\n",
      "|    total_timesteps    | 9736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121699   |\n",
      "|    policy_loss        | -0.0245  |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.16e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 121800   |\n",
      "|    time_elapsed       | 13374    |\n",
      "|    total_timesteps    | 9744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121799   |\n",
      "|    policy_loss        | -0.0344  |\n",
      "|    value_loss         | 0.00334  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9746880, episode_reward=18.60 +/- 1.85\n",
      "Episode length: 8393.80 +/- 1764.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.39e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9746880  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121835   |\n",
      "|    policy_loss        | -0.00289 |\n",
      "|    value_loss         | 0.0041   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.06e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 121900   |\n",
      "|    time_elapsed       | 13396    |\n",
      "|    total_timesteps    | 9752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121899   |\n",
      "|    policy_loss        | 0.0276   |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.08e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 122000   |\n",
      "|    time_elapsed       | 13403    |\n",
      "|    total_timesteps    | 9760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 121999   |\n",
      "|    policy_loss        | -0.00885 |\n",
      "|    value_loss         | 0.00336  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.06e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 122100   |\n",
      "|    time_elapsed       | 13409    |\n",
      "|    total_timesteps    | 9768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122099   |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.00419  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9771872, episode_reward=18.20 +/- 1.72\n",
      "Episode length: 9374.00 +/- 948.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.37e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9771872  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122148   |\n",
      "|    policy_loss        | 0.0221   |\n",
      "|    value_loss         | 0.00586  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122200   |\n",
      "|    time_elapsed       | 13433    |\n",
      "|    total_timesteps    | 9776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122199   |\n",
      "|    policy_loss        | -0.0472  |\n",
      "|    value_loss         | 0.00402  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.04e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122300   |\n",
      "|    time_elapsed       | 13440    |\n",
      "|    total_timesteps    | 9784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122299   |\n",
      "|    policy_loss        | 0.00959  |\n",
      "|    value_loss         | 0.00498  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.02e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 122400   |\n",
      "|    time_elapsed       | 13446    |\n",
      "|    total_timesteps    | 9792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122399   |\n",
      "|    policy_loss        | 0.0153   |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9796864, episode_reward=19.20 +/- 1.33\n",
      "Episode length: 7477.20 +/- 1098.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.48e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9796864  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122460   |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.0041   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.05e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122500   |\n",
      "|    time_elapsed       | 13466    |\n",
      "|    total_timesteps    | 9800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122499   |\n",
      "|    policy_loss        | -0.0297  |\n",
      "|    value_loss         | 0.00854  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.06e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122600   |\n",
      "|    time_elapsed       | 13473    |\n",
      "|    total_timesteps    | 9808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122599   |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.00516  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.06e+03 |\n",
      "|    ep_rew_mean        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 122700   |\n",
      "|    time_elapsed       | 13480    |\n",
      "|    total_timesteps    | 9816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122699   |\n",
      "|    policy_loss        | -0.0971  |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9821856, episode_reward=18.80 +/- 1.47\n",
      "Episode length: 7782.40 +/- 940.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.78e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9821856  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122773   |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.00869  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122800   |\n",
      "|    time_elapsed       | 13500    |\n",
      "|    total_timesteps    | 9824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122799   |\n",
      "|    policy_loss        | -0.0895  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.1e+03  |\n",
      "|    ep_rew_mean        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 122900   |\n",
      "|    time_elapsed       | 13507    |\n",
      "|    total_timesteps    | 9832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122899   |\n",
      "|    policy_loss        | -0.00224 |\n",
      "|    value_loss         | 0.00427  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.15e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 123000   |\n",
      "|    time_elapsed       | 13514    |\n",
      "|    total_timesteps    | 9840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 122999   |\n",
      "|    policy_loss        | -0.00269 |\n",
      "|    value_loss         | 0.00884  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9846848, episode_reward=15.80 +/- 3.92\n",
      "Episode length: 10169.40 +/- 3184.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+04 |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9846848  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123085   |\n",
      "|    policy_loss        | 0.0449   |\n",
      "|    value_loss         | 0.0047   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.16e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123100   |\n",
      "|    time_elapsed       | 13539    |\n",
      "|    total_timesteps    | 9848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123099   |\n",
      "|    policy_loss        | -0.039   |\n",
      "|    value_loss         | 0.00618  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.23e+03 |\n",
      "|    ep_rew_mean        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123200   |\n",
      "|    time_elapsed       | 13545    |\n",
      "|    total_timesteps    | 9856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123199   |\n",
      "|    policy_loss        | -0.0301  |\n",
      "|    value_loss         | 0.00316  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123300   |\n",
      "|    time_elapsed       | 13552    |\n",
      "|    total_timesteps    | 9864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123299   |\n",
      "|    policy_loss        | -0.0549  |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9871840, episode_reward=17.60 +/- 2.06\n",
      "Episode length: 8668.40 +/- 1245.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.67e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9871840  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123397   |\n",
      "|    policy_loss        | 0.00906  |\n",
      "|    value_loss         | 0.00358  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123400   |\n",
      "|    time_elapsed       | 13574    |\n",
      "|    total_timesteps    | 9872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123399   |\n",
      "|    policy_loss        | 0.0619   |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.24e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123500   |\n",
      "|    time_elapsed       | 13581    |\n",
      "|    total_timesteps    | 9880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123499   |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.00537  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.31e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123600   |\n",
      "|    time_elapsed       | 13587    |\n",
      "|    total_timesteps    | 9888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123599   |\n",
      "|    policy_loss        | -0.0549  |\n",
      "|    value_loss         | 0.00896  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123700   |\n",
      "|    time_elapsed       | 13594    |\n",
      "|    total_timesteps    | 9896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123699   |\n",
      "|    policy_loss        | -0.00855 |\n",
      "|    value_loss         | 0.00792  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9896832, episode_reward=19.40 +/- 1.20\n",
      "Episode length: 7936.00 +/- 871.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.94e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9896832  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123710   |\n",
      "|    policy_loss        | -0.0231  |\n",
      "|    value_loss         | 0.00963  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.27e+03 |\n",
      "|    ep_rew_mean        | 18.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123800   |\n",
      "|    time_elapsed       | 13615    |\n",
      "|    total_timesteps    | 9904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123799   |\n",
      "|    policy_loss        | -0.00531 |\n",
      "|    value_loss         | 0.00272  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 123900   |\n",
      "|    time_elapsed       | 13621    |\n",
      "|    total_timesteps    | 9912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123899   |\n",
      "|    policy_loss        | -0.0172  |\n",
      "|    value_loss         | 0.00561  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.33e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124000   |\n",
      "|    time_elapsed       | 13628    |\n",
      "|    total_timesteps    | 9920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 123999   |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.0052   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9921824, episode_reward=19.00 +/- 1.10\n",
      "Episode length: 7612.40 +/- 897.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.61e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9921824  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124022   |\n",
      "|    policy_loss        | -0.00559 |\n",
      "|    value_loss         | 0.00472  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124100   |\n",
      "|    time_elapsed       | 13648    |\n",
      "|    total_timesteps    | 9928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124099   |\n",
      "|    policy_loss        | -0.0228  |\n",
      "|    value_loss         | 0.00808  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124200   |\n",
      "|    time_elapsed       | 13655    |\n",
      "|    total_timesteps    | 9936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124199   |\n",
      "|    policy_loss        | 0.0355   |\n",
      "|    value_loss         | 0.00582  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124300   |\n",
      "|    time_elapsed       | 13662    |\n",
      "|    total_timesteps    | 9944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124299   |\n",
      "|    policy_loss        | -0.00931 |\n",
      "|    value_loss         | 0.00472  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9946816, episode_reward=17.20 +/- 1.94\n",
      "Episode length: 8137.40 +/- 1342.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.14e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9946816  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124335   |\n",
      "|    policy_loss        | -0.0177  |\n",
      "|    value_loss         | 0.00646  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.28e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124400   |\n",
      "|    time_elapsed       | 13683    |\n",
      "|    total_timesteps    | 9952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124399   |\n",
      "|    policy_loss        | -0.0288  |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.25e+03 |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124500   |\n",
      "|    time_elapsed       | 13690    |\n",
      "|    total_timesteps    | 9960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124499   |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    value_loss         | 0.00424  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124600   |\n",
      "|    time_elapsed       | 13696    |\n",
      "|    total_timesteps    | 9968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124599   |\n",
      "|    policy_loss        | 0.0159   |\n",
      "|    value_loss         | 0.00937  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9971808, episode_reward=17.80 +/- 1.47\n",
      "Episode length: 8412.00 +/- 784.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.41e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9971808  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124647   |\n",
      "|    policy_loss        | 0.0442   |\n",
      "|    value_loss         | 0.00648  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.32e+03 |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124700   |\n",
      "|    time_elapsed       | 13718    |\n",
      "|    total_timesteps    | 9976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124699   |\n",
      "|    policy_loss        | 0.0083   |\n",
      "|    value_loss         | 0.00203  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.46e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124800   |\n",
      "|    time_elapsed       | 13725    |\n",
      "|    total_timesteps    | 9984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124799   |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 124900   |\n",
      "|    time_elapsed       | 13731    |\n",
      "|    total_timesteps    | 9992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124899   |\n",
      "|    policy_loss        | -0.00884 |\n",
      "|    value_loss         | 0.00731  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9996800, episode_reward=17.40 +/- 2.94\n",
      "Episode length: 8456.40 +/- 2190.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 8.46e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9996800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124959   |\n",
      "|    policy_loss        | -0.00198 |\n",
      "|    value_loss         | 0.00262  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.49e+03 |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 727      |\n",
      "|    iterations         | 125000   |\n",
      "|    time_elapsed       | 13753    |\n",
      "|    total_timesteps    | 10000000 |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124999   |\n",
      "|    policy_loss        | -0.00972 |\n",
      "|    value_loss         | 0.00376  |\n",
      "------------------------------------\n",
      "Saving to logs/a2c/PongNoFrameskip-v4_3\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "!python -m rl_zoo3.train --algo a2c  --env PongNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a5396c-0e3f-4931-9854-33804347f4b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        r      l             t\n",
      "385  19.0   6716  13512.943788\n",
      "386  20.0   8340  13558.735385\n",
      "387  14.0  10124  13622.084371\n",
      "388  20.0   8630  13664.284713\n",
      "389  17.0   9670  13726.275796\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the results\n",
    "results = pd.read_csv('logs/a2c/PongNoFrameskip-v4_2/0.monitor.csv', skiprows=2, names=['r', 'l', 't'])\n",
    "# Preview the loaded data\n",
    "print(results.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70c16e7-9b17-47c7-9a27-57aa8b34ac6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Compute a rolling mean with a window size of 50 for the reward\n",
    "results['reward_mean'] = results['r'].rolling(window=50).mean()\n",
    "# Plot the smoothed reward over time\n",
    "plt.plot(results['t'], results['reward_mean'])\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward over Time during Training A2C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84406e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHHCAYAAAB9dxZkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACPbUlEQVR4nOzdd1yV5fvA8c9hHZYsZSogTkTJneLMJDFXlg3NSstcOXKVWT9tZ9m0YbatxJlm5ca9yIELFy4URYaK7M25f38Q5+sRVNCDh3G9X6/z0nM/93nO9TxnXdzPPTRKKYUQQgghhLhrZqYOQAghhBCiqpDESgghhBDCSCSxEkIIIYQwEkmshBBCCCGMRBIrIYQQQggjkcRKCCGEEMJIJLESQgghhDASSayEEEIIIYxEEishhBBCCCORxEqIEmg0Gt566617+pxDhw6lbt269/Q5K6q6devSp08fU4dRYdStW5ehQ4eaOozbKq/3cFX/bNzN901leW9UJ5JYVRJz5sxBo9HQrl07U4dS4dStWxeNRlPirWfPnqYOz2TeeustNBoNV65cMXUoJTp27BhvvfUW586dM3Uo99yWLVtu+p698SZMY968eaV6fapywldaycnJWFtbo9FoOH78eIl1li9fzlNPPUW9evWwtbWlcePGTJ48meTk5BLrZ2dn8/nnn9OuXTscHR2xtramUaNGjB07lpMnT5bj0dw9C1MHIEonNDSUunXrsmfPHk6fPk2DBg1MHVKF0qJFCyZPnlys3MvL6472l5WVhYWFfDzK07Fjx3j77bd54IEHqt2PU5MmTfj9998NyqZNm4a9vT1vvPFGsfpRUVGYmVXfv4N/+OEHdDrdPX3OLl26FHuNXnzxRe6//35GjBihL7O3t7/r57qb75uK8N5YunQpGo0GDw8PQkNDee+994rVGTFiBF5eXjzzzDP4+PgQGRnJ119/zerVq9m/fz82Njb6uleuXKFnz55ERETQp08fnn76aezt7YmKimLRokV8//335Obm3stDLBP55agEoqOj2bVrF8uXL2fkyJGEhoby5ptv3tMYdDodubm5WFtb39PnLa3atWvzzDPPGG1/FfU4ReWilCI7O9vgRwPA3d292Pv1ww8/pFatWiW+j7VabbnGWVFlZGRgZ2eHpaXlPX/uevXqUa9ePYOyUaNGUa9evVt+1+Tn56PT6bCysir1c93N901FeG/Mnz+fXr164evry4IFC0pMrP744w8eeOABg7LWrVszZMgQQkNDefHFF/XlQ4cO5cCBA/zxxx8MGDDA4DHvvvtuiX98VCTV90+gSiQ0NBRnZ2d69+7N448/TmhoqH5bXl4eLi4uPP/888Uel5qairW1NVOmTNGX5eTk8Oabb9KgQQO0Wi3e3t68+uqr5OTkGDxWo9EwduxYQkNDadq0KVqtlrVr1wLwySef0KFDB2rWrImNjQ2tW7fmjz/+KPb8WVlZjB8/nlq1alGjRg369etHbGxsif0JYmNjeeGFF3B3d0er1dK0aVN+/vnnuzltxQwdOhR7e3vOnj1LSEgIdnZ2eHl58c4776CUKnb818eYlpbGhAkTqFu3LlqtFjc3Nx566CH2799v8LilS5fSunVrbGxs9D+SsbGxxWJZsWIFzZo1w9rammbNmvHnn3+WGLNOp+OLL76gadOmWFtb4+7uzsiRI7l27drdn5D/nDhxgscffxwXFxesra1p06YNf//9t0GdossiO3fuZNKkSbi6umJnZ8ejjz7K5cuXi8X81ltv4eXlha2tLd26dePYsWMGfUHmzZvHE088AUC3bt30l1W2bNlisK8dO3Zw//33Y21tTb169fjtt99KdUwZGRlMnjwZb29vtFotjRs35pNPPjF4nZs1a0a3bt2KPVan01G7dm0ef/xxg7LSvA5FfcPWrVtHmzZtsLGx4bvvvitVzLdyYz+aotdjx44djB8/HldXV5ycnBg5ciS5ubkkJyfz3HPP4ezsjLOzM6+++mqx9/jdvrdK8x4uuuR54+t67tw5NBoN8+bN05cVfT7PnDlDr169qFGjBoMHD9Zvu75Vs+jxn3zyCd9//z3169dHq9XStm1b9u7dWyyOpUuXEhAQYBCrMfptXR/HF198oY/j2LFj5ObmMmPGDFq3bo2joyN2dnZ07tyZzZs3F9vPjd83RZfxT58+zdChQ3FycsLR0ZHnn3+ezMxMg8fe7L1hrM/q7cTExLB9+3YGDhzIwIED9Q0BN7oxqQJ49NFHAQwuH+7evZtVq1YxbNiwYkkVFCaSn3zySaliMxVpsaoEQkNDeeyxx7CysmLQoEF8++237N27l7Zt22Jpacmjjz7K8uXL+e677wz+SlqxYgU5OTkMHDgQKPwQ9evXjx07djBixAiaNGlCZGQkn3/+OSdPnmTFihUGz7tp0yaWLFnC2LFjqVWrlv5LaPbs2fTr14/BgweTm5vLokWLeOKJJ1i5ciW9e/fWP37o0KEsWbKEZ599lvbt27N161aD7UUSEhJo3769PplzdXVlzZo1DBs2jNTUVCZMmHDbc5SXl1diXyI7OzuD1oKCggJ69uxJ+/btmTVrFmvXruXNN98kPz+fd95556b7HzVqFH/88Qdjx44lICCAq1evsmPHDo4fP06rVq2Awi+0559/nrZt2zJz5kwSEhKYPXs2O3fu5MCBAzg5OQGwfv16BgwYQEBAADNnzuTq1as8//zz1KlTp9jzjhw5Ur/f8ePHEx0dzddff82BAwfYuXPnXf8lf/ToUTp27Ejt2rV57bXXsLOzY8mSJfTv359ly5bpv/iKjBs3DmdnZ958803OnTvHF198wdixY1m8eLG+zrRp05g1axZ9+/YlJCSEQ4cOERISQnZ2tr5Oly5dGD9+PF9++SWvv/46TZo0AdD/C3D69Gkef/xxhg0bxpAhQ/j5558ZOnQorVu3pmnTpjc9JqUU/fr1Y/PmzQwbNowWLVqwbt06XnnlFWJjY/n8888BeOqpp3jrrbeIj4/Hw8ND//gdO3Zw6dIl/ecGyvY6REVFMWjQIEaOHMnw4cNp3LhxWV+WUhs3bhweHh68/fbb/Pvvv3z//fc4OTmxa9cufHx8+OCDD1i9ejUff/wxzZo147nnnrujY7pRWd7DZZGfn09ISAidOnXik08+wdbW9pb1FyxYQFpaGiNHjkSj0TBr1iwee+wxzp49q49/1apVPPXUUwQGBjJz5kyuXbvGsGHDqF279l3Fer1ffvmF7OxsRowYgVarxcXFhdTUVH788UcGDRrE8OHDSUtL46effiIkJIQ9e/bQokWL2+73ySefxM/Pj5kzZ7J//35+/PFH3Nzc+Oijj277WGN9Vm9n4cKF2NnZ0adPH2xsbKhfvz6hoaF06NDhto+Nj48HoFatWvqyoj/qnn322VLHUOEoUaHt27dPASosLEwppZROp1N16tRRL7/8sr7OunXrFKD++ecfg8f26tVL1atXT3//999/V2ZmZmr79u0G9ebOnasAtXPnTn0ZoMzMzNTRo0eLxZSZmWlwPzc3VzVr1kw9+OCD+rKIiAgFqAkTJhjUHTp0qALUm2++qS8bNmyY8vT0VFeuXDGoO3DgQOXo6Fjs+W7k6+urgBJvM2fO1NcbMmSIAtS4ceP0ZTqdTvXu3VtZWVmpy5cvGxz/9TE6OjqqMWPG3DSG3Nxc5ebmppo1a6aysrL05StXrlSAmjFjhr6sRYsWytPTUyUnJ+vL1q9frwDl6+urL9u+fbsCVGhoqMFzrV27tsTyG7355psKMDiuG3Xv3l0FBgaq7OxsfZlOp1MdOnRQDRs21Jf98ssvClDBwcFKp9PpyydOnKjMzc31xxIfH68sLCxU//79DZ7nrbfeUoAaMmSIvmzp0qUKUJs3by4WV9Frum3bNn1ZYmKi0mq1avLkybc87hUrVihAvffeewbljz/+uNJoNOr06dNKKaWioqIUoL766iuDei+99JKyt7fXv+/K8joUxb127dpbxliSpk2bqq5du5a4zdfX1+DcFb0eISEhBq9HUFCQ0mg0atSoUfqy/Px8VadOHYN93+17q7Tv4c2bN5f4GkdHRytA/fLLL/qyos/na6+9Vuz5hgwZYrDfosfXrFlTJSUl6cv/+uuvYt+FgYGBqk6dOiotLU1ftmXLlmKxloadnZ3B61AUh4ODg0pMTDSom5+fr3JycgzKrl27ptzd3dULL7xgUH7j903RZ/fGeo8++qiqWbOmQdnN3hvG/KzeSmBgoBo8eLD+/uuvv65q1aql8vLybvvYYcOGKXNzc3Xy5EmDYwTUtWvXSvX8FZFcCqzgQkNDcXd311+y0Gg0PPXUUyxatIiCggIAHnzwQWrVqmXwl8i1a9cICwvjqaee0pctXbqUJk2a4O/vz5UrV/S3Bx98EKBYE3XXrl0JCAgoFtP1LUDXrl0jJSWFzp07G1wWK7ps+NJLLxk8dty4cQb3lVIsW7aMvn37opQyiCskJISUlJRil9tK0q5dO8LCwordBg0aVKzu2LFj9f8vaiXLzc1lw4YNN92/k5MTu3fv5tKlSyVu37dvH4mJibz00ksG/SV69+6Nv78/q1atAiAuLo6DBw8yZMgQHB0d9fUeeuihYud66dKlODo68tBDDxmcl9atW2Nvb1/iJYWySEpKYtOmTTz55JOkpaXp93/16lVCQkI4depUscuYI0aMMBip1rlzZwoKCjh//jwAGzduJD8//7ave2kEBATQuXNn/X1XV1caN27M2bNnb/m41atXY25uzvjx4w3KJ0+ejFKKNWvWANCoUSNatGhh8LkpKCjgjz/+oG/fvvr3eVlfBz8/P0JCQsp8vHdi2LBhBq9Hu3btUEoxbNgwfZm5uTlt2rQxOG93894qy3v4TowePbrUdZ966imcnZ3194veL0XHeunSJSIjI3nuuecMOpl37dqVwMDAu461yIABA3B1dTUoMzc3119B0Ol0JCUlkZ+fT5s2bUr1nQaFLeXX69y5M1evXiU1NfW2j70Xn9XDhw8TGRlp8D07aNAgrly5wrp162752AULFvDTTz8xefJkGjZsqC8vOrYaNWqUOo6KRi4FVmAFBQUsWrSIbt26ER0drS9v164dn376KRs3bqRHjx5YWFgwYMAAFixYQE5ODlqtluXLl5OXl2eQWJ06dYrjx48X+wIokpiYaHDfz8+vxHorV67kvffe4+DBgwZ9s67/EJ8/fx4zM7Ni+7hxNOPly5dJTk7m+++/5/vvvy9VXCWpVasWwcHBt61nZmZWrENqo0aNAG457H/WrFkMGTIEb29vWrduTa9evXjuuef0+yr6sirpso+/vz87duwwqHf9F0mRxo0bG3zhnjp1ipSUFNzc3EqMqTTn5VZOnz6NUorp06czffr0mz7H9ZdMfHx8DLYX/agV9cspOr4bX2cXFxeDH8DSuPG5ip7vdn2Azp8/j5eXV7Ev5qLLjEUxQuEP8+uvv05sbCy1a9dmy5YtJCYmFvvclOV1uNnnpjzceI6KEh1vb+9i5deft7t5b5XlPVxWFhYWZbqceKfvx6Kyu4n1ejd7zX/99Vc+/fRTTpw4QV5e3m3r3+hWx+fg4HDHjwXjfFbnz5+PnZ0d9erV4/Tp00BhR/y6desSGhpaYtcPgO3btzNs2DBCQkJ4//33DbYVHVdaWpq++0RlI4lVBbZp0ybi4uJYtGgRixYtKrY9NDSUHj16ADBw4EC+++471qxZQ//+/VmyZAn+/v40b95cX1+n0xEYGMhnn31W4vPd+GV840gmKPxA9OvXjy5dujBnzhw8PT2xtLTkl19+YcGCBWU+xqIh1M888wxDhgwpsc59991X5v0a25NPPknnzp35888/Wb9+PR9//DEfffQRy5cv5+GHHy6X59TpdLi5uRkMVrjezRLksuwfYMqUKTdtYbnxS9fc3LzEeuqGjtHGcC+e66mnnmLatGksXbqUCRMmsGTJEhwdHQ3mPyvr61DS56a83OwclVR+/Xkr7/dWkZvNw1XU2n4jrVZbpqkD7uX78VZKes3nz5/P0KFD6d+/P6+88gpubm6Ym5szc+ZMzpw5U6r93s3xlfe5UUqxcOFCMjIySmypTExMJD09vdh0FIcOHaJfv340a9aMP/74o9g0E/7+/gBERkYatFhXJpJYVWChoaG4ubnxzTffFNu2fPly/vzzT+bOnYuNjQ1dunTB09OTxYsX06lTJzZt2lRsSGr9+vU5dOgQ3bt3v+OJB5ctW4a1tTXr1q0zGOb7yy+/GNTz9fVFp9MRHR1t8Jdt0V81RVxdXalRowYFBQWlanG6WzqdjrNnz+pbqQD9ZHO3GyHk6enJSy+9xEsvvURiYiKtWrXi/fff5+GHH8bX1xco7LhcdGm1SFRUlH570b+nTp0qtv+oqCiD+/Xr12fDhg107NixXH6si1rbLC0tjXbui47v9OnTBn+VX716tVhLU3lNfunr68uGDRtIS0szaLU6ceKEQYxQ2HJw//33s3jxYsaOHcvy5cvp37+/wXu7vF8HU7ibYyrLe7io5ePGSSCvbzUsT9e/H29UUpkx/fHHH9SrV4/ly5cbvNfv9VQ5N1OWz2pJtm7dysWLF3nnnXcMBp1AYavYiBEjWLFihcHUFGfOnKFnz564ubmxevXqEucA69u3LzNnzmT+/PmVNrGSPlYVVFZWFsuXL6dPnz48/vjjxW5jx44lLS1NP4LCzMyMxx9/nH/++Yfff/+d/Px8g8sZUNjqEhsbyw8//FDi82VkZNw2LnNzczQajcFfnOfOnSs2orCoBWTOnDkG5V999VWx/Q0YMIBly5Zx5MiRYs934/BgY/j666/1/1dK8fXXX2NpaUn37t1LrF9QUEBKSopBmZubG15eXvpLoW3atMHNzY25c+caXB5ds2YNx48f1zeJe3p60qJFC3799VeDfYaFhXHs2DGD53jyyScpKCjg3XffLRZTfn7+TWcsLi03NzceeOABvvvuO+Li4optv5Nz3717dywsLPj2228Nyq8/50Xs7OyA4j+6d6tXr14UFBQUe87PP/8cjUZTrIXxqaee4t9//+Xnn3/mypUrJX5uyvN1MIW7OaayvId9fX0xNzdn27ZtBuU3fi+UFy8vL5o1a8Zvv/1Genq6vnzr1q1ERkaW63MXtRhd30K0e/duwsPDy/V5S6ssn9WSFF0GfOWVV4r9Pg0fPpyGDRsatIjGx8fTo0cPzMzMWLdu3U1bRYOCgujZsyc//vhjsd8VgNzcXIMphCoiabGqoP7++2/S0tLo169fidvbt2+Pq6sroaGh+h+Cp556iq+++oo333yTwMDAYn9FPPvssyxZsoRRo0axefNmOnbsSEFBASdOnGDJkiX6uXdupXfv3nz22Wf07NmTp59+msTERL755hsaNGjA4cOH9fVat27NgAED+OKLL7h69ap+uoWi1qHr/4L78MMP2bx5M+3atWP48OEEBASQlJTE/v372bBhA0lJSbc9X7GxscyfP79Yub29Pf3799fft7a2Zu3atQwZMoR27dqxZs0aVq1axeuvv37TD3paWhp16tTh8ccfp3nz5tjb27Nhwwb27t3Lp59+ChS2+nz00Uc8//zzdO3alUGDBumnW6hbty4TJ07U72/mzJn07t2bTp068cILL5CUlMRXX31F06ZNDb78u3btysiRI5k5cyYHDx6kR48eWFpacurUKZYuXcrs2bMN5lq6mc8++6zYsHUzMzNef/11vvnmGzp16kRgYCDDhw+nXr16JCQkEB4ezsWLFzl06NBt9389d3d3Xn75ZT799FP69etHz549OXToEGvWrKFWrVoGr3uLFi0wNzfno48+IiUlBa1Wy4MPPnjTfj+l1bdvX7p168Ybb7zBuXPnaN68OevXr+evv/5iwoQJ1K9f36D+k08+yZQpU5gyZQouLi7FWu+M9TpUJHd7TKV9Dzs6OvLEE0/w1VdfodFoqF+/PitXrrzr/oFl8cEHH/DII4/QsWNHnn/+ea5du8bXX39Ns2bNDGI1tj59+rB8+XIeffRRevfuTXR0NHPnziUgIKBcn7e0yvJZvVFOTg7Lli3joYceuunkpv369WP27NkkJibi5uZGz549OXv2LK+++io7duzQ9zstiuWhhx7S3//tt9/o0aMHjz32GH379qV79+7Y2dlx6tQpFi1aRFxcXMWey8oUQxHF7fXt21dZW1urjIyMm9YZOnSosrS01E9ToNPplLe3d4lDzYvk5uaqjz76SDVt2lRptVrl7OysWrdurd5++22VkpKirwfcdHqBn376STVs2FBptVrl7++vfvnlF/3w4OtlZGSoMWPGKBcXF2Vvb6/69++vH+L+4YcfGtRNSEhQY8aMUd7e3srS0lJ5eHio7t27q++///625+pW0y1cP5x6yJAhys7OTp05c0b16NFD2draKnd3d/Xmm2+qgoICg31y3fDnnJwc9corr6jmzZurGjVqKDs7O9W8eXM1Z86cYrEsXrxYtWzZUmm1WuXi4qIGDx6sLl68WKzesmXLVJMmTZRWq1UBAQFq+fLlxYaUF/n+++9V69atlY2NjapRo4YKDAxUr776qrp06dItz0vRa1LSzdzcXF/vzJkz6rnnnlMeHh7K0tJS1a5dW/Xp00f98ccf+jpFQ7j37t1r8BwlDafPz89X06dPVx4eHsrGxkY9+OCD6vjx46pmzZoG0wAopdQPP/yg6tWrp8zNzQ324+vrq3r37l3smLp27XrTKQmul5aWpiZOnKi8vLyUpaWlatiwofr4448Nhp9fr2PHjgpQL7744k33WZrX4WZxl8adTLdw4+txsyk2it77d3JMN1Pa9/Dly5fVgAEDlK2trXJ2dlYjR45UR44cKXG6hZJiLNpW0nQLH3/8cbG61392iyxatEj5+/srrVarmjVrpv7++281YMAA5e/vf9vjvN7NplsoKQ6dTqc++OAD5evrq7RarWrZsqVauXJliefoxphv9joWve7R0dH6stK+N+72s3q9ZcuWKUD99NNPN61TNKXF7Nmz9cd4s1tJ7/vMzEz1ySefqLZt2yp7e3tlZWWlGjZsqMaNG6efMqWi0ih1j3v5iWrt4MGDtGzZkvnz5+tnVb5Xhg4dyh9//FEh/lqsbpKTk3F2dua9996r8MtRiOqhRYsWuLq6EhYWZupQKhT5rN496WMlyk1WVlaxsi+++AIzMzO6dOligojEvXCz1x1KXtZCiPKUl5dHfn6+QdmWLVs4dOhQtX8/yme1fEgfK1FuZs2aRUREBN26dcPCwoI1a9awZs0aRowYUWxqB1F1LF68mHnz5tGrVy/s7e3ZsWMHCxcupEePHnTs2NHU4YlqJjY2luDgYJ555hm8vLw4ceIEc+fOxcPDo9gEnNWNfFbLhyRWotx06NCBsLAw3n33XdLT0/Hx8eGtt96S5uUq7r777sPCwoJZs2aRmpqq7yRb0or3QpQ3Z2dnWrduzY8//sjly5exs7Ojd+/efPjhh9SsWdPU4ZmUfFbLiSk7eG3dulX16dNHeXp6KkD9+eefBtuXLVumHnroIeXi4qIAdeDAgWL7yMrKUi+99JJycXFRdnZ26rHHHlPx8fEGdc6fP6969eqlbGxslKurq5oyZUqxdYw2b96sWrZsqaysrFT9+vUNOlUKIYQQQpSGSftYZWRk0Lx58xInwCza3qlTp1uu5D1x4kT++ecfli5dytatW7l06RKPPfaYfntBQQG9e/cmNzeXXbt28euvvzJv3jxmzJihrxMdHU3v3r3p1q0bBw8eZMKECbz44ou3XetICCGEEOJ6FWZUoEaj4c8//zSYc6jIuXPn8PPz48CBA7Ro0UJfnpKSgqurKwsWLNDPuXLixAmaNGlCeHg47du3Z82aNfTp04dLly7h7u4OwNy5c5k6dSqXL1/GysqKqVOnsmrVKoMJKgcOHEhycrJ+MWEhhBBCiNup1H2sIiIiyMvLM5jQz9/fHx8fH31iFR4eTmBgoD6pgsJZwUePHs3Ro0dp2bIl4eHhxSYFDAkJYcKECTd97pycHIMZtotWL69Zs2a5LdUhhBBCCONSSpGWloaXl1eZ1qq8mUqdWMXHx2NlZVVsBWx3d3fi4+P1da5Pqoq2F227VZ3U1FSysrJKXEtr5syZvP3228Y6FCGEEEKY0IULF6hTp85d76dSJ1amNG3aNCZNmqS/n5KSgo+PDxcuXMDBwcGEkQkhhBCitFJTU/H29jZYtP1uVOrEysPDg9zcXJKTkw1arRISEvDw8NDX2bNnj8HjEhIS9NuK/i0qu76Og4PDTVd+12q1aLXaYuUODg6SWAkhhBCVjLG68VTqmddbt26NpaUlGzdu1JdFRUURExNDUFAQULhSdmRkpMGin2FhYTg4OBAQEKCvc/0+iuoU7UMIIYQQojRM2mKVnp7O6dOn9fejo6M5ePAgLi4u+Pj4kJSURExMDJcuXQIKkyYobGHy8PDA0dGRYcOGMWnSJFxcXHBwcGDcuHEEBQXRvn17AHr06EFAQADPPvsss2bNIj4+nv/7v/9jzJgx+hanUaNG8fXXX/Pqq6/ywgsvsGnTJpYsWcKqVavu8RkRQgghRKVmykm0ilbbvvFWtFJ30SrdN96uXwW8aIJQZ2dnZWtrqx599FEVFxdn8Dznzp1TDz/8sLKxsVG1atVSkydPLnGC0BYtWigrKytVr169Mk8QmpKSogCVkpJyJ6dCCCGEECZg7N/vCjOPVWWXmpqKo6MjKSkp0sdKCCGEqCSM/ftdqftYCSGEEEJUJJJYCSGEEEIYiSRWQgghhBBGIomVEEIIIYSRSGIlhBBCCGEkklgJIYQQQhiJJFZCCCGEEEYiiZUQQgghhJFIYiWEEOUoIycfmYdZiOpDEishhCgnu85cofnb65m4+OBNk6vwM1c5einlHkcmhCgvklgJIUQ50OkU7686Tr5OseLgJb7fdrZYnTWRcQz64V8e+XonayLjTBClEMLYLEwdgBBCVEWrj8Rx9FIq5mYaCnSKj9aeYM2ReJrXcaRHUw88Ha155Y/DAOTrFGMXHuA7czOCA9xNHLkQ4m7IIsxGIoswCyGK5Bfo6PH5Ns5eyWBicCPiU7NYuOeCQR1Lcw15BYr767pQx9mG5QdisbE0Z8nIIALrOJoociGqH2P/fkuLlRBCGFFadh6L9lzg7JUMXOysGNbZDzsrc54LqsvZyxlsP3WZVYfjSMvJp5a9FV893ZKadlZcychl28nLDPrhX8Z3b8BTbXyIT82mvqsdFubSa0OIykJarIxEWqyEEGuPxPHyooPk5OsA+L/eTXixc71i9VKz81h3JJ7Wvs7Uc7UHChOy53/Zy77z1wzq9ghwZ+4zrTEz05T/AQhRDRn791v+DBJCCCNITMtm6rJIcvJ1uNhZEdLUnWfa+5ZY18HakifaeOuTKoAa1pYsGRnErMfvo5a9Vl++/lgC32w+Xe7xCyGMQy4FCiHEXVJKMX3FEVKy8mjq5cCKMR2xvIPLd2ZmGp5s482jLWuTkZPP+qMJvLrsMJ9tOElgHUceaOxmUH/tkXjeXXmM9/o3o5u/2032KoS4l6TFSggh7tCxS6n0+Wo7A77dxbqjCViYafjkieZ3lFRdz9LcDCdbK55s683T7XxQCl794zDJmbn6OlfTc3ht+WFik7N4/c9IsnIL7vZwhBBGIImVEELcgfScfF4KjeBIbCr7Y5IBGPtgA5p4GreP5Yw+AdRztSMxLYe3/zmmL39v1XGSM/MAiEvJLnGeLCHEvSeJlRBC3IHpK45w7momXo7WzHwskHf7N2NstwZGfx5rS3M+eaI5Zhr480Ase88lsf3UZf48EItGAyO7FHaOn7v1DHEpWUZ/fiFE2UhiJYQQZbQ5KpE/D8Ribqbhy0EtGXS/D8+29y23aRFa+TjzVFtvAD5YfZw3/jwCwJCgurz2sD9tfJ3Jyivg9eWFlwSvpOeUSxxCiNuTxEoIIcogN1/HuysLL8k936Eubeq63JPnHd+9IVYWZhyISSYmKRMPB2sm92iERqPhvUebYWVhxuaoyzR7ax3tP9jI1pOX70lcQghDklgJIUQZ/BZ+jrOXM6hlb8X44Ib37Hk9HW2Y0qMRLnZWdG5Yix+HtKGGtSUA/h4OTO8TAECBTpGvU0xZeohrGbm32qUQohzIBKFGIhOEClH1XUnPodvHW0jLyeejAYE81dbH1CEZOBBzDVsrC8Ys2M/pxHSGdqjLW/2amjosISo0mSBUCCFMICe/gFf/OExaTj6BtR15orW3qUMqpqWPM409avBGryYArIqMQ6f739/OMVczyczNN1V4QlQLklgJIcRtZOcVMOr3CDadSERrYca7/ZtV6CVmOjaoRQ1rCy6n5bA/pnCJnIV7Yujy8WZG/h4BFE5qeilZRhEKYWySWAkhxC1k5RYw/Ld9bI66jLWlGT8PbUsLbydTh3VLVhZmBDdxB2Dpvov8Hn6ON/6MBGD7qSuEn7nKS6H76fDhJmb8dQTpESKE8ciSNkIIcQtfbTrF9lNXsLUy5+ehbWlfr6apQyqVkKYe/HkglsX7LrB4X2GZi50VSRm5DP1lj36h6N/Cz+NoY8nkHo1NGK0QVYe0WAkhxE1k5xWwYE8MAB8/3rzSJFUAXRu54uNii7mZhiaeDox7sAFLRgZhpoGcfB1WFmY8F1S4SPRXm05z4L9LhkKIuyMtVkIIcRN/H7xEcmYedZxt6NnMw9ThlImNlTmbpzxAXoEOa0tzfflzQXX562AsXw5qSeeGrmTmFvBHxEVm/HWUFWM6Yl6B+44JURlIi5UQQpRAKcW8XecAeLa9b6VMOMzNNAZJFcBb/Zqyf/pDdG7oCsDUnv7U0FoQGZvC3nNJpghTiCpFEishhCjBvvPXOBaXitbCjCfbVLypFe6GRvO/JNG1hpbuTdwA2Hn6iqlCEqLKkMRKCCFKUNRa1b9FbZztrEwbTDnr0KAWADsksRLirkliJYQQN4hPyWbtkXgAhnSoa9pg7oGO/yVWhy4kk5qdZ+JohKjcJLESQogbhO4+T4FOcX9dFwK8qv4SVbWdbPCrZYdOwR/7Lpo6HCEqNUmshBDiOsmZufz+73mgerRWFel7nycA76w8xsfrTpg4GiEqL0mshBDiOp+HnSQ5M4/G7jUIaepu6nDumZeDGzG2WwMAvtl8hpMJaSaOSIjKyaSJ1bZt2+jbty9eXl5oNBpWrFhhsF0pxYwZM/D09MTGxobg4GBOnTplUCcpKYnBgwfj4OCAk5MTw4YNIz093aDO4cOH6dy5M9bW1nh7ezNr1qxisSxduhR/f3+sra0JDAxk9erVRj9eIUTFFhWfxvzdhROCvtk3AAvz6vO3p7mZhikhjXn4v/m65mw+beKIhKicTPqtkZGRQfPmzfnmm29K3D5r1iy+/PJL5s6dy+7du7GzsyMkJITs7Gx9ncGDB3P06FHCwsJYuXIl27ZtY8SIEfrtqamp9OjRA19fXyIiIvj444956623+P777/V1du3axaBBgxg2bBgHDhygf//+9O/fnyNHjpTfwQshKhSlFG//c5QCnaJnUw/9SLnqZsx/rVZ/H7pEzNVME0cjRCWkKghA/fnnn/r7Op1OeXh4qI8//lhflpycrLRarVq4cKFSSqljx44pQO3du1dfZ82aNUqj0ajY2FillFJz5sxRzs7OKicnR19n6tSpqnHjxvr7Tz75pOrdu7dBPO3atVMjR44sdfwpKSkKUCkpKaV+jBCi4lgTeUn5Tl2pGr6xWsVczTB1OCb13E+7le/UlWr0/H3qTGKa0ul0pg5JiHJj7N/vCtvOHR0dTXx8PMHBwfoyR0dH2rVrR3h4OADh4eE4OTnRpk0bfZ3g4GDMzMzYvXu3vk6XLl2wsvrfPDQhISFERUVx7do1fZ3rn6eoTtHzlCQnJ4fU1FSDmxCicsrOK+C9VccBGNmlHt4utiaOyLSKWq1WR8bz4Kdbafv+Rv5vRST7Y66hlDJxdEJUbBU2sYqPL5xDxt3dsPOou7u7flt8fDxubm4G2y0sLHBxcTGoU9I+rn+Om9Up2l6SmTNn4ujoqL95e1etmZmFqE5+2HaWi9ey8HS0ZvQD9U0djsnd7+fCRwMCaVvXGStzM66k5zD/3xgem7OL5+ftRaeT5EqIm6mwiVVFN23aNFJSUvS3CxcumDokIcQduJCUyZwtZwB47WF/bK1kbXqAp9r6sHRUByLf7sHvw+7nsZa1sTI3Y0vUZSJirpk6PCEqrAr7DeLhUTgyJSEhAU9PT315QkICLVq00NdJTEw0eFx+fj5JSUn6x3t4eJCQkGBQp+j+7eoUbS+JVqtFq9XewZEJISqK3HwdYxceICuvgPv9XOjX3MvUIVU4WgtzOjd0pXNDVzQaDcv2X+Svg7G0reti6tCEqJAqbIuVn58fHh4ebNy4UV+WmprK7t27CQoKAiAoKIjk5GQiIiL0dTZt2oROp6Ndu3b6Otu2bSMv73/LNISFhdG4cWOcnZ31da5/nqI6Rc8jhKiaPlkfxaELyThYW/DZk80NFicWxT3SojDxXB0ZT16BzsTRCFExmTSxSk9P5+DBgxw8eBAo7LB+8OBBYmJi0Gg0TJgwgffee4+///6byMhInnvuOby8vOjfvz8ATZo0oWfPngwfPpw9e/awc+dOxo4dy8CBA/HyKvwCePrpp7GysmLYsGEcPXqUxYsXM3v2bCZNmqSP4+WXX2bt2rV8+umnnDhxgrfeeot9+/YxduzYe31KhBD3yKYTCXy/7SwAHz/RnDrO1bvDeml0qF+TWvZWJGXkyoLNQtyMUcYW3qHNmzcroNhtyJAhSqnCKRemT5+u3N3dlVarVd27d1dRUVEG+7h69aoaNGiQsre3Vw4ODur5559XaWlpBnUOHTqkOnXqpLRarapdu7b68MMPi8WyZMkS1ahRI2VlZaWaNm2qVq1aVaZjkekWhKg8ouJTVYu31ynfqSvVm38dMXU4lcqMFZHKd+pKNXHRAVOHIoRRGPv3W6OUjJ01htTUVBwdHUlJScHBoeov2ipEZXQ5LYfPN5xk8d4LFOgUzWo7sGx0B7QW5qYOrdKIOJ/EgG/DsbMyJ2L6Q1hbyrkTlZuxf78rbB8rIYQwpsiLKXT7ZAsLdsdQoFP0CHDnx+faSlJVRq18nKnjbENGbgEbjyfe/gFCVDOSWAkhqoWZa46TnpNPUy8HlowM4vvn2uDhaG3qsCodjUZD3/9GT/51MNbE0QhR8UhiJYSo8vaeS2LXmatYmmv47tnW3O8nUwXcjb73FSZWW09eJjM338TRCFGxSGIlhKjyZm84BcDjrevI6D8jaOJZA28XG3LydWw7KaMDhbieJFZCiCot4nwSO05fwcJMw0sPNDB1OFWCRqOhR0DhBMrrj9186S8hqiNJrIQQVdoX/7VWDWhVp9ovrmxMDwUUrq+66UQi+TJZqBB6klgJIaqs/THX2H6qsLVqTDdprTKmNr7OONtakpyZx95zsnagEEUksRJCVFlFfasea1Ubn5rSWmVMFuZmdG9S2GollwOF+B9JrIQQVdLBC8lsPXkZczMNY7s1NHU4VVLR5cCwYwnIXNNCFJLESghRJc3ecBKAR1tKa1V56dLQFWtLMy5ey+J4XJqpwxGiQpDESghR5Ry6kMzmqKLWKulbVV5srMzp1MAVKGy1EkJIYiWEqIK+3FjYt+qRFl7UrWVn4miqth5NpZ+VENeTxEoIUaVEXkxh44lEzDQw7kHpW1Xeuvu7YaaBo5dSuXgt09ThCGFyklgJIaqU2frWqtr4SWtVuatpr6WNb+ESQRvkcqAQklgJIaqO6CsZbDiegEYDYx+UvlX3SvcmbgBsjrps4kiEMD1JrIQQVcbCPTEAPNDIlfqu9iaOpvp40L8wsQo/e1UWZRbVniRWQogqISe/gKX7LgAwuJ2viaOpXhq42VPbyYbcfB3hZ66aOhwhTEoSKyFElbD2SDzXMvPwdLTmgcaupg6nWtFoNPpWq00nEk0cjRCmJYmVEKJKCP238DLgwLY+WJjLV9u91s2/MJndcDxBFmUW1Zp8+wghKr2TCWnsOZeEuZmGp9p6mzqcaqlD/Vq42FmRkJrDRmm1EtWYJFZCiEpvwe7C1qrgJm54OFqbOJrqydrSXJ/U/hZ+zrTBCGFCklgJISq1rNwClu2/CMDT0mndpAa388FMAztPX+V0oqwdKKonSayEEJXazzujScvOx9vFhs4Napk6nGqtjrMt3ZsULnEz/78+b0JUN5JYCSEqrYvXMvlqU+FM65MeaoSZmcbEEYmn2hReDtx6UiYLFdWTJFZCiErrnX+OkZ2n434/F/q3qG3qcATQtm7h8jbRVzK4mp5j4miEuPcksRJCVEqbTySy/lgC5mYa3n2kGRqNtFZVBI62ljR0K5z1PuL8NRNHI8S9J4mVEKLSyc4r4M2/jwLwQse6NPaoYeKIxPXa1HUGICJGEitR/UhiJYSodBbvvUBMUibuDlpeDm5k6nDEDVr5/JdYnZPESlQ/klgJISoVpRShu88DMLprfey1FiaOSNyozX/9rA7HppCTX2DiaIS4tySxEkJUKhHnr3EyIR1rSzMebVXH1OGIEtStaYuLnRW5+TqOx8l8VqJ6kcRKCFGpFM2y3vc+LxxtLE0cjSiJRqPhvjqOAEReTDZtMELcY5JYCSEqjWsZuayMjAPg6XY+Jo5G3Mp9tQsTq8MXU0wciRD3liRWQohKY9n+i+Tm6wjwdKCFt5OpwxG3EFjHCZDESlQ/klgJISoFpRQL9hReBny6nY/MW1XBFV0KPJWYRmZuvomjEeLekcRKCFEp7I5O4uzlDGytzHmkhZepwxG34e5gjbuDFp2CY5dSTR2OEPeMJFZCiErhpx3RADzSwosa1tJpvTIIrO0EwH6ZKFRUIxU+sUpLS2PChAn4+vpiY2NDhw4d2Lt3r367UooZM2bg6emJjY0NwcHBnDp1ymAfSUlJDB48GAcHB5ycnBg2bBjp6ekGdQ4fPkznzp2xtrbG29ubWbNm3ZPjE0Lc3q4zVwj7b/maFzr6mTocUUrt6xXOZ7Xj9FUTRyLEvVPhE6sXX3yRsLAwfv/9dyIjI+nRowfBwcHExsYCMGvWLL788kvmzp3L7t27sbOzIyQkhOzsbP0+Bg8ezNGjRwkLC2PlypVs27aNESNG6LenpqbSo0cPfH19iYiI4OOPP+att97i+++/v+fHK4QwVKBTvPPPMQAGt/OhobssX1NZdGpYC4A90VfJzpOJQkU1oSqwzMxMZW5urlauXGlQ3qpVK/XGG28onU6nPDw81Mcff6zflpycrLRarVq4cKFSSqljx44pQO3du1dfZ82aNUqj0ajY2FillFJz5sxRzs7OKicnR19n6tSpqnHjxqWONSUlRQEqJSXljo5VCFGyBbvPK9+pK1Xgm2tVUnrO7R8gKgydTqfavBemfKeuVDtPXTZ1OEKUyNi/3xW6xSo/P5+CggKsra0Nym1sbNixYwfR0dHEx8cTHBys3+bo6Ei7du0IDw8HIDw8HCcnJ9q0aaOvExwcjJmZGbt379bX6dKlC1ZWVvo6ISEhREVFce1ayX0DcnJySE1NNbgJIYwrNTuPT9ZFATAhuBHOdla3eYSoSDQaDZ0aFLZabT99xcTRCHFvVOjEqkaNGgQFBfHuu+9y6dIlCgoKmD9/PuHh4cTFxREfHw+Au7u7wePc3d312+Lj43FzczPYbmFhgYuLi0GdkvZRtK0kM2fOxNHRUX/z9va++wMWQhj4ZtNprmbkUs/VjmeDfE0djrgDRYnV5hOJ6HTKxNEIUf4qdGIF8Pvvv6OUonbt2mi1Wr788ksGDRqEmZlpQ582bRopKSn624ULF0wajxBVzbkrGfy8s3Ak4PTeAViaV/ivK1GCBxq7YmNpzon4NBbvk+9JUfVV+G+q+vXrs3XrVtLT07lw4QJ79uwhLy+PevXq4eHhAUBCQoLBYxISEvTbPDw8SExMNNien59PUlKSQZ2S9lG0rSRarRYHBweDmxDCeGatO0FegaJrI1e6+bvd/gGiQqppr2Vyj0YAfLD6OIlp2bd5hBCVW4VPrIrY2dnh6enJtWvXWLduHY888gh+fn54eHiwceNGfb3U1FR2795NUFAQAEFBQSQnJxMREaGvs2nTJnQ6He3atdPX2bZtG3l5efo6YWFhNG7cGGdn53t0hEKIIqcT01lzpPAy/Bu9m5g4GnG3hnaoS2BtR9Ky8/liw6nbP0CISqzCJ1br1q1j7dq1REdHExYWRrdu3fD39+f5559Ho9EwYcIE3nvvPf7++28iIyN57rnn8PLyon///gA0adKEnj17Mnz4cPbs2cPOnTsZO3YsAwcOxMurcPbmp59+GisrK4YNG8bRo0dZvHgxs2fPZtKkSSY8ciGqrx+3n0UpeCjAnUYyvUKlZ2FuxvQ+AQAs3nuBM5fTb/MIISqvCp9YpaSkMGbMGPz9/Xnuuefo1KkT69atw9KycOblV199lXHjxjFixAjatm1Leno6a9euNRhJGBoair+/P927d6dXr1506tTJYI4qR0dH1q9fT3R0NK1bt2by5MnMmDHDYK4rIcS9kZiazfL9hfPUjepaz8TRCGO538+F4CZuFOgUs9aeMHU4QpQbjVJKhmkYQWpqKo6OjqSkpEh/KyHuwkdrT/DtljO08XXmj9EdTB2OMKKTCWn0/GIbOgX/jO1E4H8LNQthSsb+/ba4kwfpdDpOnz5NYmIiOp3OYFuXLl3uOighRPWUlp3H/H/PAzCya30TRyOMrZF7Dbo1dmPjiUT2nU+SxEpUSWVOrP7991+efvppzp8/z42NXRqNhoICWbZACHFnFu25QFp2PvVd7eguIwGrpAbu9mw8kci5KxmmDkWIclHmxGrUqFG0adOGVatW4enpiUajKY+4hBDVTG6+jp92FM5bNaJLPczM5LulKqpXyw6As5JYiSqqzInVqVOn+OOPP2jQoEF5xCOEqKaW7b9IfGo2rjW09G9Z29ThiHJSt2ZhYnXuqiRWomoq86jAdu3acfr06fKIRQhRTWXnFTD7v/mNRnaph9bC3MQRifLi91+LVey1LHLzdbepLUTlU6oWq8OHD+v/P27cOCZPnkx8fDyBgYH6aQ+K3HfffcaNUAhR5c3/9zzxqdl4OVrzTHtZE7Aqc62hxc7KnIzcAmKSMmngZm/qkIQwqlIlVi1atECj0Rh0Vn/hhRf0/y/aJp3XhRBllZ6Tz5wtZwB4Obgh1pbSWlWVaTQafGvacSwulXNXMiSxElVOqRKr6Ojo8o5DCFFN/bwjmqSMXPxq2TGgVR1ThyPuAb9ahYlVtHRgF1VQqRIrX9//Nc1v27aNDh06YGFh+ND8/Hx27dplUFcIIW4lNjmLH7adBWDSQ42wMK/wi0EII6jnWtjP6lRimokjETeKik/D2dYSNwfr21cWJSrzqMBu3boRFxeHm5vhHDMpKSl069ZNLgUKIUolN1/H2AX7ScvJp4W3E70DPU0dkrhHmtdxAmDf+WumDUQY2H32KoN++BdbKws+f6oF6Tl5nEnM4FJyFn2be9FN5pYrlTInVkV9qW509epV7OzsjBKUEKLqm7X2BAdiknGwtuCrQS1l3qpqpLWvMwBnL2dwNT2HmvZaE0ckcvILeP3PSHSqsN/j8N/2GWzfeCKR7VO74WBteZM9lL+8gsJRpJYVvGW71InVY489BhR2PBw6dCha7f8+CAUFBRw+fJgOHWRdLyHE7a0/Gs+P/00G+vETzfF2sTVxROJecrazoqGbPacS09l3/hohTT1MHVKVp5QiNTsfR5uSE6Pvtp7lzOUMatlb0cTTgX/PXqVZbUcCPB3YfuoKMUmZjF94gJPxaWTn63C2taS5txPjH2xI3Vrl36hyJDaFV/84TO/7PBnTrWLPo1nqxMrRsXBNJ6UUNWrUwMbGRr/NysqK9u3bM3z4cONHKISoUjJz85m6rHAKl2Gd/ORHtZpq6+dSmFidS5L3QDnJL9CxJeoygXUceW3ZYbadusKSkUH6FsMi565k8PXmwvkpp/cJoF9zL3QKzP9rRf770CXGLzzAlqjL+sckZeRy5nIGfx+8xNAOdZnwUCPstXe0/PAt5ebr+CzsJD9sP0uBTnE1I4dhnfwq9OjhUp+FX375BYC6desyZcoUuewnhLgjKw5c4lpmHj4utkzt6W/qcISJtK3rzILdMew5J/2sysubfx8ldHcM5mYaCnSF0yV9v+0M3z3bxqDeJ+ujyM3X0alBLfo190Kj0WB+3ZX53oGezP/3PKcS0ni5e0Pa169JfEo283adY0vUZX7cEc0/hy/x4WP3Gb0f1sw1x/ll5zkA+tznyZt9m1bopAruoI/Vm2++WR5xCCGqAaUUv4WfA+C5IF+sLCp2XwlRftr4ugBwNDaF9Jz8cmntqM62n7pM6O4YAAp0CgszDfk6RdixBOJSsvB0LLzqdCQ2hZWH4wB4vVeTEvtQm5tpWDS8PRoN+u3+Hg480NiNzVGJvPX3Uc5fzeT5eXt5vHUd+jX3wsPRmgKdIjO3ACdbSxxtLDl/NZOG7vY4WFuy6/QVvtx0ipFd69OtsRu5+ToUymDVhYjz15i36xwAXzzVotIsdVXmd3LLli1LPPEajQZra2saNGjA0KFD6datm1ECFEJUHXuikzgRn4aNpTlPtPY2dTjChLxdbPF2seFCUhZ7oq/yoL+7qUOqMlKz85j6R+Hl9sHtfGhb14XazjZ8uj6Kf88msWB3DL0CPZm+4ghRCYVTXjzSwosAL4eb7vNmg0u6NXYjaEJNPlp7gl92nuOPiIv8EXHxpvupaWfFpB6NmLn6BOk5+Ry8kEzPph6EHUvAwcaS5S91wNPRhjOX05mw+ABKwYBWdSpNUgV3sFZgz549OXv2LHZ2dnTr1o1u3bphb2/PmTNnaNu2LXFxcQQHB/PXX3+VR7xCiErst/DzAPRvWRtHW9ONLhIVQ6cGrgBsP3XFxJFUDUopfth2lv7f7ORSSjY+Lra83qsJ/VvWpm1dF55tXxeA0N0xDP5xN/vOXyMtOx9bK3MmPdTojp/X2tKcN/s2ZcnIIJ5oXYe6NW1xtrWklr0VdZxtsPnv0l0NrQVXM3J5488jpOfkY21pRnaejhUHL5GRW0BcSjajfo/g9/BzPPrNTi4kZeHtYsP0Pk2McXrumTK3WF25coXJkyczffp0g/L33nuP8+fPs379et58803effddHnnkEaMFKoSo3OJSslh7NB4ovAwoRKcGtVi4J4adp6teYpWcmcvPO6Lp0siVNnVd7slz/rzzHO+vPg6ArZU5nz7ZHLvrLrH2aOqOWw0tiWk5AATWduTd/s2o42xDLSNMeXG/nwv3+xU/Vp1OkfvfVAnTVxxhacRF6rna8fuwdny6LgorCzMeaOzKq38c5tDFFA5dTAEKp+X47tnWONla3XVs95JGXb8AYCk4OjoSERFBgwaGwx1Pnz5N69atSUlJ4cSJE7Rt25a0tOozq25qaiqOjo6kpKTg4HDz5lQhqqtP10fx1abT3O/nwpKRQaYOR1QA1zJyafVeGEpB+LQH9f1+rrfhWALeLrY09qhhggiL0+kURy6l4GRjhU9NW5IycrmUnEVTLwd9N5nsvAIG/7ibiPPXMDfTMLVnY4Z3rldiNxpjibyYwmPf7iSvQDG+e0OGBPmWOD/YVxtP8WnYSZp4OrBweDuTJC1HL6VQt6adQdIHhf29ft4ZzfmrmbT0dmJKSON70lHd2L/fZW6xsra2ZteuXcUSq127dmFtXTgFvk6n0/9fCCFy8gtYuKewI+2QoLqmDUZUGM52VrT0dmJ/TDLv/HOMOYNbGSQf647GM/L3COy1FvwxOgh/D+P/0ZqVW8CmE4l0bex62w70fx2M5ZP1UVxIygKgvqsdZ69koBRM6dGIsQ82RKdTTFl6iIjz1/Qdxj9YfYJDF1L49Mnm5ZIopOfkM27hfvIKFCFN3ZkY3PCmSdyoB+rT0L0GHRvUpIaJJvts6uVYYnmz2o589mSLextMOShzYjVu3DhGjRpFREQEbdu2BWDv3r38+OOPvP766wCsW7eOFi1aGDVQIUTltSYynivpuXg4WNOjqXRSFv/zVr+mDPh2F2uOxBO6O4Zn2hdeJs4r0PHhmhNAYeLwwi97WTGmo1HXsEvOzOX5eXs5EJPMgFZ1+PTJ5iXWy8kv4N2Vx5j/b+EfB7ZW5mTlFXDm8v8Wkf407CT+Hg4cuVQ4ys7CTMNvL9zP6cvpvPPPMVZFxhFYx5FRXesbLf4in60/ybmrmXg5WvPRgPtu2TJmaW5Gz2Yyb1h5KvOlQIDQ0FC+/vproqKiAGjcuDHjxo3j6aefBiArK0s/SrC6kEuBQtzco3N2ciAmmckPNWJc94amDkdUMD9uP8t7q45jZWHGx4/fxyMtavNb+Dlm/HWUmnZWONpacvZyBn2be/HVoJZGec6s3AKe+G4XR2JTAbAw07D11W7UdjK8HKmU4qXQ/aw5Utg/cNyDDXjpgQZcSc/hSGwKzb2d+HbLGX7/9zyW5oUtVErBrAH38WTbwpGvC/fEMG15JLWdbNj2ajf9xJvGkJSRS4cPN5Kdp2Pe8215oLGs51dWJr8UCDB48GAGDx580+3Xz8ouhKjeDl9M5kBMMpbmGgbe72PqcEQF9EJHP/aeS2Ld0QReXnSQqPg0Fu29AMCEhxoRWNuR/t/sJOxYPJm5+dha3fmcV2nZeVxOy+GrTac5EpuKi50VXk7WHIlN5ecd0UzvEwDAgZhrJGfmceBCMmuOxGNlbsbcZ1vpp4UonC6icCmm6X0CuJKeo0++nmnvo0+qAB5tWZuP1p4gNjmLTScSeSjAeK22v+46R3aejma1HejayNVo+xV37o7fnbm5uSQmJqLT6QzKfXzki1MI8T9FUyz0DvTEtYYstiuKMzPTMGdwaz4PO8nXm08zZ8sZoLAP08C23liYafBxsSUmKZNNJxLpc5/XHT3P4r0xTP/rKLn5hb9bZhr4elBL8nSKIT/vYeGeGMY/2JAtJxN5edFBg8e+0bvJTefasrIw45unW/Fb+DniUrKZ1MNw6gJrS3OeauPNd9vO8lv4OaMlVpm5+fz634S7o7rWL9fO8aL0ypxYnTp1ihdeeIFdu3YZlCul0Gg0FBQUGC04IUTllpSRy9+HLgHwXIe6pg1GVGjmZhqmhDTmzOV0fcvPtIebYGleON1i7/s8+XbLGVYdjit1YpWVW8DO01c4czmdM5fTWbKvcOJKOytznO2sGN+9IR0a1EIphb9HDU7EpzHtz8NsO1k4/UO9WnZoNNC5oettpwgxM9MwtKPfTbc/096X77efZfupK+w8fYWODWqV6hiup5QiJimTgxeSuZyWQ1JGLsn/LQ/1cDPPMu9PlI8yJ1ZDhw7FwsKClStX4unpKRmyEOKmFu2NITdfR2BtR1p6O5k6HFEJvP1IU85ezsDfswbdm/yvv1DvwMLEKuxYAuMWHuCdfk1xtrNi99mrLN8fy7NBvjT1cuCH7Wf588AlsnLziU/NJjvP8KrKyC71eO1hf4PfLo1Gw6iu9Zmw+CCrIwuTuvvrurBgeDsszI2z7JK3iy3PtPPl93/P8/qfkayb0KVMIwR3nbnCZ+tPsu988bUVh3epZ9R+W+LulDmxOnjwIBEREfj7y+KpQoiby83XMf+/y4BDOtSVP8JEqbjVsGbdxC7Fypt6OdCzqQdrj8bzz6FLxCVnEVjHkV93nUOnYMXBWB4KcNeve1ektpMNbes64+5oTUtvZ0Kaupf4XuzX3IvM3AL2nUsiPSeftx9parSkqsirPRsTdiyB81cz+WLDKV572J+r6TlEJaRxNT0Xa0tzHmjsqm+lAzhzOZ0ft0frpyuxNNfopys4eCEZdwctT7SuY9Q4xd0p86jAtm3b8vnnn9OpU6fyiqlSklGBQhj6Pfwc0/86imsNLdtf7VbhV6QXFZ9Siojz13h+3l7SsvP15X617Ii+8r+pDyYEN6Rjg1o421pS39W+QiX164/GM+L3CMw00LWRKztPX9XPSg6FiWATTwdy8gs4fzWTmKRM/bZn2vsw7sGGuDtYo5Ti4IVkXGtoqeNsa4pDqTKM/ftd5sRq06ZN/N///R8ffPABgYGBWFoaTjBWXZMKSayE+J+s3AK6fryZxLQc3nmkKc/JpKDCiHaducKMv47S0M2ex1vXoVtjN77adJoftp9leOd6vBxcsaf0mL7iCL//e15/37emLe4O1py9nMGV9ByDupbmGtrXq8lLDzQgqH7Nex1qtWDyxMrMrLCJ8sa/AKp753VJrIT4n++3neGD1Seo7WTD5ikPYGVh3EsqQpREp1OYVZK+RttPXWbtkXj63OelT5iy8wrYcDyB1Kx8rCzMcHfQ0sLbyWQzpFcXJp/HavPmzXf9pEKIqistO08/XP7l4IaSVIl7prIkVVA40rBzQ8N5p6wtze94KglRcZQ5seratWt5xCGEqCJ+2hFNcmYe9VzteKxlbVOHI4QQ99Qd/Sm5fft2nnnmGTp06EBsbCwAv//+Ozt27DBqcEKIyiUxLZsft0cDMOmhRkYfVSWEEBVdmb/1li1bRkhICDY2Nuzfv5+cnMKOdikpKXzwwQdGD1AIUXm8u/I46Tn53FfHkV4yYaEQohoqc2L13nvvMXfuXH744QeDEYEdO3Zk//79Rg1OCFF5bD15mX8OXcJMA+/3D6xU/V2EEMJYypxYRUVF0aVL8cnbHB0dSU5ONkZMQohKJjuvgOkrjgAwtIMfgXUcTRyREEKYRpkTKw8PD06fPl2sfMeOHdSrV88oQQkhKpe5W88Qk5SJp6N1sQVohRCiOilzYjV8+HBefvlldu/ejUaj4dKlS4SGhjJlyhRGjx5t1OAKCgqYPn06fn5+2NjYUL9+fd59912un3pLKcWMGTPw9PTExsaG4OBgTp06ZbCfpKQkBg8ejIODA05OTgwbNoz09HSDOocPH6Zz585YW1vj7e3NrFmzjHosQlRV2XkF/LrrHACv92qCvbbMg42FEKLKKPM34GuvvYZOp6N79+5kZmbSpUsXtFotU6ZMYdy4cUYN7qOPPuLbb7/l119/pWnTpuzbt4/nn38eR0dHxo8fD8CsWbP48ssv+fXXX/Hz82P69OmEhIRw7NgxrK2tARg8eDBxcXGEhYWRl5fH888/z4gRI1iwYAFQODlYjx49CA4OZu7cuURGRvLCCy/g5OTEiBEjjHpMQlQ1Kw7Eci0zj9pONvQKlA7rQojqrcwzrxfJzc3l9OnTpKenExAQgLW1NYmJiXh5GW9ysz59+uDu7s5PP/2kLxswYAA2NjbMnz8fpRReXl5MnjyZKVOmAIWjE93d3Zk3bx4DBw7k+PHjBAQEsHfvXtq0aQPA2rVr6dWrFxcvXsTLy4tvv/2WN954g/j4eKysrIDCBHLFihWcOHGiVLHKzOuiOlJK8fDs7ZyIT+P1Xv6M6FLf1CEJIUSZGPv3+44nmbGysiIgIID7778fe3t7jh49ire3910HdL0OHTqwceNGTp48CcChQ4fYsWMHDz/8MADR0dHEx8cTHBysf4yjoyPt2rUjPDwcgPDwcJycnPRJFUBwcDBmZmbs3r1bX6dLly76pAogJCSEqKgorl27VmJsOTk5pKamGtyEqG7Cz17lRHwaNpbmPNXGx9ThCCGEyVXozhCvvfYaqamp+Pv7Y25uTkFBAe+//z6DBw8GID4+HgB3d3eDx7m7u+u3xcfH4+bmZrDdwsICFxcXgzp+fn7F9lG0zdnZuVhsM2fO5O233zbCUQpRef2y8xwAA1rXxtFW1jMTQogKPS3ykiVLCA0NZcGCBezfv59ff/2VTz75hF9//dXUoTFt2jRSUlL0twsXLpg6JCHuqQtJmWw4ngAUTrEghBCigrdYvfLKK7z22msMHDgQgMDAQM6fP8/MmTMZMmQIHh4eACQkJODp+b9OswkJCbRo0QIonB4iMTHRYL/5+fkkJSXpH+/h4UFCQoJBnaL7RXVupNVq0Wq1d3+QQlRSP++MRino0siVBm72pg5HCCEqhFInVocPH77l9qioqLsO5kaZmZmYmRk2qpmbm6PT6QDw8/PDw8ODjRs36hOp1NRUdu/erZ/6ISgoiOTkZCIiImjdujUAmzZtQqfT0a5dO32dN954g7y8PP1s8mFhYTRu3LjEy4BCVHcpmXks3lvYSju8s7RWCSFEkVInVi1atECj0VDSIMKico3GuEtY9O3bl/fffx8fHx+aNm3KgQMH+Oyzz3jhhRf0zzthwgTee+89GjZsqJ9uwcvLi/79+wPQpEkTevbsyfDhw5k7dy55eXmMHTuWgQMH6kcwPv3007z99tsMGzaMqVOncuTIEWbPns3nn39u1OMRoqoI3XOezNwC/D1q0KlBLVOHI4QQFUapE6vo6OjyjKNEX331FdOnT+ell17ST+UwcuRIZsyYoa/z6quvkpGRwYgRI0hOTqZTp06sXbtWP4cVQGhoKGPHjqV79+6YmZkxYMAAvvzyS/12R0dH1q9fz5gxY2jdujW1atVixowZMoeVECVIy87Td1of0aWe0f+gEkKIyuyO57EShmQeK1FdvPFnJKG7Y/CtaUvYxK5YWVToMTBCCHFLFWYeKyFE9RN+5iqhu2MA+PCx+ySpEkKIG8i3ohCiVFIy85iy9BAAT7fzIah+TRNHJIQQFY8kVkKI21JKMXnpIWKTs/BxsWXaw/6mDkkIISokSayEELf1045oNhxPwMrcjDmDW1HDWmZZF0KIktxRYpWfn8+GDRv47rvvSEtLA+DSpUukp6cbNTghhOnl5Bcwe8MpAKb3DaBZbUcTRySEEBVXmWdeP3/+PD179iQmJoacnBweeughatSowUcffUROTg5z584tjziFECay49QV0nLy8XCwZvD9stCyEELcSplbrF5++WXatGnDtWvXsLGx0Zc/+uijbNy40ajBCSFMb3Vk4WLlPZt5YGYmc1YJIcStlLnFavv27ezatQsrKyuD8rp16xIbG2u0wIQQppebryPsWGFi9XCzktfNFEII8T9lbrHS6XQUFBQUK7948SI1atQwSlBCiIoh/OxVUrPzqWWvpU1dF1OHI4QQFV6ZE6sePXrwxRdf6O9rNBrS09N588036dWrlzFjE0KY2JrIOAB6NnPHXC4DCiHEbZX5UuCnn35KSEgIAQEBZGdn8/TTT3Pq1Clq1arFwoULyyNGIYQJRF5M4c8DhZf3ezXzNHE0QghROZQ5sapTpw6HDh1i0aJFHD58mPT0dIYNG8bgwYMNOrMLISqvxLRshv+2j5x8Hd0au9K+nsyyLoQQpVHmxArAwsKCZ555xtixCCEqgAKd4qX5+4lPzaaBmz2zB7WU0YBCCFFKpUqs/v7771LvsF+/fnccjBDC9H7eEc2+89eoobXgx+fa4CCzrAshRKmVKrHq37+/wX2NRoNSqlgZUOKIQSFE5XDuSgafrI8CYHqfAOrWsjNxREIIUbmUalSgTqfT39avX0+LFi1Ys2YNycnJJCcns2bNGlq1asXatWvLO14hRDn6bttZcvJ1dGpQiyfa1DF1OEIIUemUuY/VhAkTmDt3Lp06ddKXhYSEYGtry4gRIzh+/LhRAxRC3BsZOfn8fbBwFOCYbg30rdBCCCFKr8zzWJ05cwYnJ6di5Y6Ojpw7d84IIQkhTOGfQ5fIyC3Ar5Yd7evJZKBCCHEnypxYtW3blkmTJpGQkKAvS0hI4JVXXuH+++83anBCiHtn4d4LADzV1ltaq4QQ4g6VObH6+eefiYuLw8fHhwYNGtCgQQN8fHyIjY3lp59+Ko8YhRDl7HhcKocuJGNhpmFAK+lbJYQQd6rMfawaNGjA4cOHCQsL48SJEwA0adKE4OBg+StXiEpq0Z4YAB4KcMe1htbE0QghROV1RxOEajQaevToQY8ePYwdjxDiHkvLzmP5f0vXDLzfx8TRCCFE5VbmS4EAW7dupW/fvvpLgf369WP79u3Gjk0IcQ/8uuscadn51HO1o1ODWqYORwghKrUyJ1bz588nODgYW1tbxo8fz/jx47G2tqZ79+4sWLCgPGIUQpSTtOw8ftgeDcDL3RtiLkvXCCHEXdGoG6dQv40mTZowYsQIJk6caFD+2Wef8cMPP1TbeaxSU1NxdHQkJSUFBwcHU4cjRKnM2XKaWWujqOdqR9jErpJYCSGqHWP/fpe5xers2bP07du3WHm/fv2Ijo6+64CEEPeGUorF/02xMKprfUmqhBDCCMqcWHl7e7Nx48Zi5Rs2bMDb29soQQkhyt/ec9c4fzUTOytz+tznaepwhBCiSijzqMDJkyczfvx4Dh48SIcOHQDYuXMn8+bNY/bs2UYPUAhRPv6IKGyt6hXoia3VHQ0QFkIIcYMyf5uOHj0aDw8PPv30U5YsWQIU9rtavHgxjzzyiNEDFEIYX3pOPqsOxwHwRBtpaRZCCGO5oz9TH330UR599FFjxyKEuEc+W3+SjNwC6tWyo21dZ1OHI4QQVUaZ+1hduHCBixcv6u/v2bOHCRMm8P333xs1MCFE+TgQc41fdhUONJnRN0BWTBBCCCMqc2L19NNPs3nzZgDi4+MJDg5mz549vPHGG7zzzjtGD1AIYTy5+TqmLY9EKXi0ZW0eaOxm6pCEEKJKKXNideTIEe6//34AlixZQmBgILt27SI0NJR58+YZOz4hhBH9fegSJ+LTcLGzYnqfAFOHI4QQVU6ZE6u8vDy02sJFWjds2EC/fv0A8Pf3Jy4uzrjRCSGM6q+DhWsCDu1QFxc7KxNHI4QQVU+ZE6umTZsyd+5ctm/fTlhYGD179gTg0qVL1KxZ0+gBCiGM40p6DrvOXAWgX3MvE0cjhBBVU5kTq48++ojvvvuOBx54gEGDBtG8eXMA/v77b/0lQiFExbMmMo4CneK+Oo7UrWVn6nCEEKJKKnNi9cADD3DlyhWuXLnCzz//rC8fMWIEc+fONWpwAHXr1kWj0RS7jRkzBoDs7GzGjBlDzZo1sbe3Z8CAASQkJBjsIyYmht69e2Nra4ubmxuvvPIK+fn5BnW2bNlCq1at0Gq1NGjQQPqLiSrn70OXAGmtEkKI8lTmxArA3NwcZ2fDuW/q1q2Lm5vxRxjt3buXuLg4/S0sLAyAJ554AoCJEyfyzz//sHTpUrZu3cqlS5d47LHH9I8vKCigd+/e5ObmsmvXLn799VfmzZvHjBkz9HWio6Pp3bs33bp14+DBg0yYMIEXX3yRdevWGf14hDCF2OQs9p67hkYDfe6TxEoIIcqLRimlblepVatWbNy4EWdnZ1q2bHnLeW/2799v1ABvNGHCBFauXMmpU6dITU3F1dWVBQsW8PjjjwNw4sQJmjRpQnh4OO3bt2fNmjX06dOHS5cu4e7uDsDcuXOZOnUqly9fxsrKiqlTp7Jq1SqOHDmif56BAweSnJzM2rVrSxWXsVfHFsKYvtt6hplrTtDOz4XFI4NMHY4QQlQYxv79LtXM64888oh+JGD//v3v+knvVG5uLvPnz2fSpEloNBoiIiLIy8sjODhYX8ff3x8fHx99YhUeHk5gYKA+qQIICQlh9OjRHD16lJYtWxIeHm6wj6I6EyZMuGksOTk55OTk6O+npqYa70CFMLK/Dv53GbCFtFYJIUR5KlVi9eabb5b4/3ttxYoVJCcnM3ToUKBwglIrKyucnJwM6rm7uxMfH6+vc31SVbS9aNut6qSmppKVlYWNjU2xWGbOnMnbb79tjMMSolwdvZTCsbhUrMzN6NXM09ThCCFElXbHS9rv27eP48ePAxAQEEDr1q2NFtTN/PTTTzz88MN4eZn+r+5p06YxadIk/f3U1FS8vWUxW1HxLN1XuATVQwHuOMvcVUIIUa7KnFhdvHiRQYMGsXPnTn1LUXJyMh06dGDRokXUqVPH2DECcP78eTZs2MDy5cv1ZR4eHuTm5pKcnGzQapWQkICHh4e+zp49ewz2VTRq8Po6N44kTEhIwMHBocTWKgCtVqu/PCpERZWTX6CfFPTxNuXz2RRCCPE/ZR4V+OKLL5KXl8fx48dJSkoiKSmJ48ePo9PpePHFF8sjRgB++eUX3Nzc6N27t76sdevWWFpasnHjRn1ZVFQUMTExBAUVdtANCgoiMjKSxMREfZ2wsDAcHBwICAjQ17l+H0V1ivYhRGX198FLXMvMw91BS5eGrqYORwghqrxSjQq8no2NDbt27aJly5YG5REREXTu3JnMzEyjBgig0+nw8/Nj0KBBfPjhhwbbRo8ezerVq5k3bx4ODg6MGzcOgF27dgGF0y20aNECLy8vZs2aRXx8PM8++ywvvvgiH3zwAVA43UKzZs0YM2YML7zwAps2bWL8+PGsWrWKkJCQUsUoowJFRbP5RCIjf48gt0DHy90bMvGhRqYOSQghKhyTjAq8nre3N3l5ecXKCwoKyq3v04YNG4iJieGFF14otu3zzz/HzMyMAQMGkJOTQ0hICHPmzNFvNzc3Z+XKlYwePZqgoCDs7OwYMmQI77zzjr6On58fq1atYuLEicyePZs6derw448/ljqpEqKiOX81g3ELD5BboKNXoAdjH2xg6pCEEKJaKHOL1V9//cUHH3zAN998Q5s2bYDCjuzjxo1j6tSpJp2OwZSkxUpUFLn5Oh6fu4vDF1O4v64LocPbYWl+R3MBCyFElWfs3+8yJ1bOzs5kZmaSn5+PhUVhg1fR/+3sDNcfS0pKuusAKwtJrERF8f6qY/ywPRonW0tWj++Ml1PJAzCEEEJUgEuBX3zxxV0/qRCifFy8lsmPO6IBmDXgPkmqhBDiHitzYjVkyJDyiEMIYQSrDsehFLSv50KPph6mDkcIIaqdUne8WLJkCbm5ufr7Fy9eRKfT6e9nZmYya9Ys40YnhCiTVZFxgCy0LIQQplLqxGrQoEEkJyfr7wcEBHDu3Dn9/bS0NKZNm2bM2IQQZXD+agaHL6ZgpoGHm0lrlRBCmEKpE6sb+7iXsc+7EKKcFbVWdahfi5r2siqAEEKYgozBFqKKWHmo6DKgLLQshBCmIomVEFXA2cvpHItLxcJMQ4h0WhdCCJMp06jAdevW4ejoCBQuM7Nx40aOHDkCYND/Sghxb608XNha1bFBLZztrEwcjRBCVF9lSqxunGph5MiRBvc1Gs3dRySEKLOVhy8BchlQCCFMrdSJ1fVTKwghKo6TCWmcTEjHytxM5q4SQggTkz5WQlRyKw8VtlZ1aVQLRxtLE0cjhBDVmyRWQlRiSil9/yqZFFQIIUxPEishKrFjcamcvZKB1sKM4AB3U4cjhBDVniRWQlRiRa1V3Rq7Ya8t89KfQgghjEwSKyEqqcLLgP+NBmwuowGFEKIiuKPEKjk5mR9//JFp06aRlJQEwP79+4mNjTVqcEKImzt8MYULSVnYWJrzoL+bqcMRQghBGeexAjh8+DDBwcE4Ojpy7tw5hg8fjouLC8uXLycmJobffvutPOIUQtygqLWqexM3bK3kMqAQQlQEZW6xmjRpEkOHDuXUqVNYW1vry3v16sW2bduMGpwQomQ5+QX8eaCwhbhvcxkNKIQQFUWZE6u9e/cWm3EdoHbt2sTHxxslKCHErf1zKI4r6bl4OlrLZUAhhKhAypxYabVaUlNTi5WfPHkSV1dXowQlhLg5pRS/7IwG4NkgXyzNZQyKEEJUFGX+Ru7Xrx/vvPMOeXl5QOH6gDExMUydOpUBAwYYPUAhhKG9565x9FIq1pZmDGrrY+pwhBBCXKfMidWnn35Keno6bm5uZGVl0bVrVxo0aECNGjV4//33yyNGIcR1ilqrHm1ZG2c7KxNHI4QQ4nplHkrk6OhIWFgYO3bs4PDhw6Snp9OqVSuCg4PLIz4hxHXiU7JZd7SwL+PQDn4mjkYIIcSN7niMdqdOnejUqZMxYxFC3MbyAxfRKWhb15nGHjVMHY4QQogblDmx+vLLL0ss12g0WFtb06BBA7p06YK5ufldByeE+B+lFMsiLgLweOs6Jo5GCCFEScqcWH3++edcvnyZzMxMnJ2dAbh27Rq2trbY29uTmJhIvXr12Lx5M97e3kYPWIjq6uCFZM5czsDa0oxegbKEjRBCVERl7rz+wQcf0LZtW06dOsXVq1e5evUqJ0+epF27dsyePZuYmBg8PDyYOHFiecQrRLW1bH9ha1XPph7UsLY0cTRCCCFKUuYWq//7v/9j2bJl1K9fX1/WoEEDPvnkEwYMGMDZs2eZNWuWTL0ghBFl5xXwz6E4AB5vLS3BQghRUZW5xSouLo78/Pxi5fn5+fqZ1728vEhLS7v76IQQAGyJSiQlKw9PR2uC6tc0dThCCCFuosyJVbdu3Rg5ciQHDhzQlx04cIDRo0fz4IMPAhAZGYmfnwwFF8JYwo4lAtA70BNzM42JoxFCCHEzZU6sfvrpJ1xcXGjdujVarRatVkubNm1wcXHhp59+AsDe3p5PP/3U6MEKUR3pdIqtJwsTqwebyLqAQghRkZW5j5WHhwdhYWGcOHGCkydPAtC4cWMaN26sr9OtWzfjRShENXc4NoUr6bnU0FrQtq6LqcMRQghxC3c8Qai/vz/+/v7GjEUIUYJNxxMA6NLIVRZcFkKICu6OEquLFy/y999/ExMTQ25ursG2zz77zCiBCSEKbTxReBmwm79cBhRCiIquzInVxo0b6devH/Xq1ePEiRM0a9aMc+fOoZSiVatW5RGjENXW+qPxHL2UiqW5hgcau5o6HCGEELdR5usK06ZNY8qUKURGRmJtbc2yZcu4cOECXbt25YknniiPGIWoljJy8nnr76MADO9cj1r2WhNHJIQQ4nbKnFgdP36c5557DgALCwuysrKwt7fnnXfe4aOPPjJ6gLGxsTzzzDPUrFkTGxsbAgMD2bdvn367UooZM2bg6emJjY0NwcHBnDp1ymAfSUlJDB48GAcHB5ycnBg2bBjp6ekGdQ4fPkznzp2xtrbG29ubWbNmGf1YhCiLLzac5FJKNt4uNox7sKGpwxFCCFEKZU6s7Ozs9P2qPD09OXPmjH7blStXjBcZhWsQduzYEUtLS9asWcOxY8f49NNP9WsUAsyaNYsvv/ySuXPnsnv3buzs7AgJCSE7O1tfZ/DgwRw9epSwsDBWrlzJtm3bGDFihH57amoqPXr0wNfXl4iICD7++GPeeustvv/+e6MejxCldexSKj/vPAfAO/2aYWMli5oLIUSloMrokUceUd9//71SSqnJkyerBg0aqPfee0+1atVKde/evay7u6WpU6eqTp063XS7TqdTHh4e6uOPP9aXJScnK61WqxYuXKiUUurYsWMKUHv37tXXWbNmjdJoNCo2NlYppdScOXOUs7OzysnJMXjuxo0blzrWlJQUBaiUlJRSP0aIkhQU6FT/b3Yo36kr1ej5+0wdjhBCVGnG/v0uc4vVZ599Rrt27QB4++236d69O4sXL6Zu3br6CUKN5e+//6ZNmzY88cQTuLm50bJlS3744Qf99ujoaOLj4wkODtaXOTo60q5dO8LDwwEIDw/HycmJNm3a6OsEBwdjZmbG7t279XW6dOmClZWVvk5ISAhRUVFcu3atxNhycnJITU01uAlhDOuOxnMgJhk7K3Nm9Glq6nCEEEKUQZkSq4KCAi5evIiPjw9QeFlw7ty5HD58mGXLluHr62vU4M6ePcu3335Lw4YNWbduHaNHj2b8+PH8+uuvAPq1Cd3d3Q0e5+7urt8WHx+Pm5vhMHULCwtcXFwM6pS0j+uf40YzZ87E0dFRf/P2loVxhXGE7o4B4PmOfng4Wps4GiGEEGVRpsTK3NycHj163LQVx9h0Oh2tWrXigw8+oGXLlowYMYLhw4czd+7ce/L8tzJt2jRSUlL0twsXLpg6JFEFnL+awY7TV9BoYOD9kqwLIURlU+ZLgc2aNePs2bPlEUsxnp6eBAQEGJQ1adKEmJjCv+g9PDwASEhIMKiTkJCg3+bh4UFiYqLB9vz8fJKSkgzqlLSP65/jRlqtFgcHB4ObEHdr4Z7CBL1rI1fqONuaOBohhBBlVebE6r333mPKlCmsXLmSuLi4cu1n1LFjR6KiogzKTp48qb/k6Ofnh4eHBxs3btRvT01NZffu3QQFBQEQFBREcnIyERER+jqbNm1Cp9Pp+4oFBQWxbds28vLy9HXCwsJo3LixwQhEIcpTbr6OPyIKE6tB9/uYOBohhBB3pKy93TUajf5mZmamvxXdN6Y9e/YoCwsL9f7776tTp06p0NBQZWtrq+bPn6+v8+GHHyonJyf1119/qcOHD6tHHnlE+fn5qaysLH2dnj17qpYtW6rdu3erHTt2qIYNG6pBgwbptycnJyt3d3f17LPPqiNHjqhFixYpW1tb9d1335U6VhkVKO7WqsOXlO/Ularte2EqN7/A1OEIIUS1YOzf7zIvabN582bjZ3c30bZtW/7880+mTZvGO++8g5+fH1988QWDBw/W13n11VfJyMhgxIgRJCcn06lTJ9auXYu19f86/YaGhjJ27Fi6d++OmZkZAwYM4Msvv9Rvd3R0ZP369YwZM4bWrVtTq1YtZsyYYTDXlRDlbcF/ndafbOMtiy0LIUQlpVFKKVMHURWkpqbi6OhISkqK9LcSZXb+agZdP96CRgPbXumGt4v0rxJCiHvB2L/fd/Rn8fbt23nmmWfo0KEDsbGxAPz+++/s2LHjrgMSojr6Lfw8AF0aukpSJYQQlViZE6tly5YREhKCjY0N+/fvJycnB4CUlBQ++OADowcoRFWXlp3H4r2Fndaf71jXtMEIIYS4K3c0KnDu3Ln88MMPWFpa6ss7duzI/v37jRqcENXB0n0XSc/Jp76rHV0aupo6HCGEEHehzIlVVFQUXbp0KVbu6OhIcnKyMWISotoo0Cnm7ToHwAud/DAz05g2ICGEEHelzImVh4cHp0+fLla+Y8cO6tWrZ5SghKguNp9IJCYpE0cbSx5rWcfU4QghhLhLZU6shg8fzssvv8zu3bvRaDRcunSJ0NBQpkyZwujRo8sjRiGqrAV7CqdYeKqtNzZW5iaORgghxN0q8zxWr732Gjqdju7du5OZmUmXLl3QarVMmTKFcePGlUeMQlRJsclZbIkqXG5pYFtZF1AIIaqCMidWGo2GN954g1deeYXTp0+Tnp5OQEAA9vb25RGfEFXWkr0X0CkIqleTeq7y+RFCiKqgzJcC58+fT2ZmJlZWVgQEBHD//fdLUiVEGSml+OfQJaDwMqAQQoiqocyJ1cSJE3Fzc+Ppp59m9erVFBQUlEdcQlRpJ+LTOHslAysLM4ID3E0djhBCCCMpc2IVFxfHokWL0Gg0PPnkk3h6ejJmzBh27dpVHvEJUSWtjowDoGsjV+y1Zb4iL4QQooIqc2JlYWFBnz59CA0NJTExkc8//5xz587RrVs36tevXx4xClGlKKVY9V9i1TvQ08TRCCGEMKa7+lPZ1taWkJAQrl27xvnz5zl+/Lix4hKiytofc42zlzOwMjfjwSZupg5HCCGEEd3RIsyZmZmEhobSq1cvateuzRdffMGjjz7K0aNHjR2fEFXO7I2FE+z2b+mFg7XlbWoLIYSoTMrcYjVw4EBWrlyJra0tTz75JNOnTycoKKg8YhOiytkfc41tJy9jbqZhbLeGpg5HCCGEkZU5sTI3N2fJkiWEhIRgbm44U/SRI0do1qyZ0YIToipRSvHRmhMAPNayNj41bU0ckRBCCGMrc2IVGhpqcD8tLY2FCxfy448/EhERIdMvCHETG48nsjs6CSsLM14OltYqIYSoiu6ojxXAtm3bGDJkCJ6ennzyySc8+OCD/Pvvv8aMTYgqI79Ax8w1hYM7nu9YlzrO0lolhBBVUZlarOLj45k3bx4//fQTqampPPnkk+Tk5LBixQoCAgLKK0YhKr0/Ii5y5nIGzraWvPRAA1OHI4QQopyUusWqb9++NG7cmMOHD/PFF19w6dIlvvrqq/KMTYgqIb9Ax5wtZwAY060BjjYyElAIIaqqUrdYrVmzhvHjxzN69GgaNpT+IUKU1qrIOGKSMnG2teTpdj6mDkcIIUQ5KnWL1Y4dO0hLS6N169a0a9eOr7/+mitXrpRnbEJUelm5BXyx4RQAz3f0w9ZKlq8RQoiqrNSJVfv27fnhhx+Ii4tj5MiRLFq0CC8vL3Q6HWFhYaSlpZVnnEJUSp+ujyL6SgYeDtYM7VjX1OEIIYQoZ2UeFWhnZ8cLL7zAjh07iIyMZPLkyXz44Ye4ubnRr1+/8ohRiErpWkYuv+w6B8DMxwJllnUhhKgG7ni6BYDGjRsza9YsLl68yMKFC40VkxBVwrZTlynQKfw9atDNX9YEFEKI6uCuEqsi5ubm9O/fn7///tsYuxOiSth8IhGABxpLUiWEENWFURIrIYShAp1i68nLAHRr7GriaIQQQtwrklgJUQ4OXUzmWmYeNawtaOXrbOpwhBBC3COSWAlhZFm5BXy2/iQAXRq5YmkuHzMhhKgu5BtfCCN7Z+VRdpy+go2lOcM71zN1OEIIIe4hSayEMKLE1Gz+iLgIwPfPtaaFt5NpAxJCCHFPSWIlhBH9Gn6OvAJFG19nOjeUTutCCFHdSGIlhJGkZecx/98YAF6US4BCCFEtSWIlhJH8uD2alKw86rva8VCAu6nDEUIIYQKSWAlhBEkZufy4/SwAk3s0xtxMY+KIhBBCmIIkVkIYwdytZ8jILaCplwM9m3qYOhwhhBAmUuETq7feeguNRmNw8/f312/Pzs5mzJgx1KxZE3t7ewYMGEBCQoLBPmJiYujduze2tra4ubnxyiuvkJ+fb1Bny5YttGrVCq1WS4MGDZg3b969ODxRBeTkFxD673kApvRojJm0VgkhRLVV4RMrgKZNmxIXF6e/7dixQ79t4sSJ/PPPPyxdupStW7dy6dIlHnvsMf32goICevfuTW5uLrt27eLXX39l3rx5zJgxQ18nOjqa3r17061bNw4ePMiECRN48cUXWbdu3T09TlE57T6bREZuAW41tHRtJCMBhRCiOrMwdQClYWFhgYdH8csrKSkp/PTTTyxYsIAHH3wQgF9++YUmTZrw77//0r59e9avX8+xY8fYsGED7u7utGjRgnfffZepU6fy1ltvYWVlxdy5c/Hz8+PTTz8FoEmTJuzYsYPPP/+ckJCQe3qsovLZ9N9iyw/6u0lrlRBCVHOVosXq1KlTeHl5Ua9ePQYPHkxMTOGQ9oiICPLy8ggODtbX9ff3x8fHh/DwcADCw8MJDAzE3f1/o7RCQkJITU3l6NGj+jrX76OoTtE+SpKTk0NqaqrBTVQ/Sik2nii89NzN383E0QghhDC1Cp9YtWvXjnnz5rF27Vq+/fZboqOj6dy5M2lpacTHx2NlZYWTk5PBY9zd3YmPjwcgPj7eIKkq2l607VZ1UlNTycrKKjGumTNn4ujoqL95e3sb43BFJXPmcjoXkrKwMjejU4Napg5HCCGEiVX4S4EPP/yw/v/33Xcf7dq1w9fXlyVLlmBjY2OyuKZNm8akSZP091NTUyW5qoaW7Y8FoH39mthpK/zHSQghRDmr8C1WN3JycqJRo0acPn0aDw8PcnNzSU5ONqiTkJCg75Pl4eFRbJRg0f3b1XFwcLhp8qbVanFwcDC4ieolMzefBbsLL0sPbudj4miEEEJUBJUusUpPT+fMmTN4enrSunVrLC0t2bhxo357VFQUMTExBAUFARAUFERkZCSJiYn6OmFhYTg4OBAQEKCvc/0+iuoU7UOIkizfH0tKVh4+LrYEN5GZ1oUQQlSCxGrKlCls3bqVc+fOsWvXLh599FHMzc0ZNGgQjo6ODBs2jEmTJrF582YiIiJ4/vnnCQoKon379gD06NGDgIAAnn32WQ4dOsS6dev4v//7P8aMGYNWqwVg1KhRnD17lldffZUTJ04wZ84clixZwsSJE0156KIC0+kUP++MBuD5jnVlpnUhhBBAJehjdfHiRQYNGsTVq1dxdXWlU6dO/Pvvv7i6Fs4X9Pnnn2NmZsaAAQPIyckhJCSEOXPm6B9vbm7OypUrGT16NEFBQdjZ2TFkyBDeeecdfR0/Pz9WrVrFxIkTmT17NnXq1OHHH3+UqRbETW09eZmzlzOoobXgiTbSt04IIUQhjVJKmTqIqiA1NRVHR0dSUlKkv1UVp5Tiibnh7Dt/jRc7+fF/fQJMHZIQQog7ZOzf7wp/KVCIimb7qSvsO38NrYUZw7vUM3U4QgghKhBJrIQog5TMPN5YEQnA0+18cHewNnFEQgghKhJJrIQopcTUbIb/vo8LSVl4u9gwoXsjU4ckhBCigqnwndeFqAh0OsWzP+0hKiENG0tz5jzdGkdbS1OHJYQQooKRxEqIUth66jJRCWnUsLbgz5c60sDN3tQhCSGEqIDkUqAQpfDbrnMAPNHaW5IqIYQQNyWJlRC3EZeSxZaTlwF4LsjXxNEIIYSoyCSxEuI2tkRdRilo6eNE3Vp2pg5HCCFEBSaJlRC3sflE4TqTDzZ2M3EkQgghKjpJrIS4hZz8AnaevgLAA5JYCSGEuA1JrIS4he0nr5CRW0Atey1NvWSpIiGEELcmiZUQN5Gek89b/xwFoF9zL8zMNCaOSAghREUniZUQN/H9trNcvJZFbScbJvWQWdaFEELcniRWQpQgO6+A38PPAfB6rybYa2UuXSGEELcniZUQJVi2/yLXMvOo42xDSFN3U4cjhBCikpDESogb6HSKn3ZEA/BCRz8szOVjIoQQonTkF0OIG+w4fYWzlzOoobXgybbepg5HCCFEJSKJlRA3+P3f8wA81qq29K0SQghRJpJYCXGdqPg0wo4loNHAs7IuoBBCiDKSxEqI63yz+TQAvZp50sCthomjEUIIUdlIYiXEf66m57DmSBwAox+ob+JohBBCVEaSWAnxnz8PxJJXoAis7Uiz2o6mDkcIIUQlJImVEEB+gY5Fey8AyEhAIYQQd0yGPIlqLb9Ax7xd5/hq02lSsvKoYW1Bv+Zepg5LCCFEJSWJlajWXl58kFWH4/T333mkKY42liaMSAghRGUmiZWotradvMyqw3FYmGl4sq03Lb2d6N+itqnDEkIIUYlJYiWqnQtJmSRl5PL6n5EAPBdUlxl9A0wclRBCiKpAEitRrew7l8TTP+wmt0AHQG0nG17u3tDEUQkhhKgqZFSgqDb+OhjLkJ/36JMqL0drvhncCkdb6VMlhBDCOKTFSlRpV9JzWLA7BmtLMz5YfQKAwNqOLBzRXtYBFEIIYXTyyyKqrAKd4qXQ/eyJTtKX1XO1Y8WYjpibaUwYmRBCiKpKEitRZf204yx7opOwtTKnta8z5mYaxj3YQJIqIYQQ5UYSK1Elnb2czifrTgIwo08AA+/3MXFEQgghqgPpvC6qpDlbzpBboKNLI1eekiVqhBBC3CPSYiUqvavpOcz4+ygWZhr8atnh71GDFQdiAZgY3BCNRi79CSGEuDcksRKVWkpWHqPmR7D33LVi2zo3rEVLH2cTRCWEEKK6qlSXAj/88EM0Gg0TJkzQl2VnZzNmzBhq1qyJvb09AwYMICEhweBxMTEx9O7dG1tbW9zc3HjllVfIz883qLNlyxZatWqFVqulQYMGzJs37x4ckbgbSikmLT6oT6pa+jhhZV74lnawtmDmY4GmDE8IIUQ1VGlarPbu3ct3333HfffdZ1A+ceJEVq1axdKlS3F0dGTs2LE89thj7Ny5E4CCggJ69+6Nh4cHu3btIi4ujueeew5LS0s++OADAKKjo+nduzejRo0iNDSUjRs38uKLL+Lp6UlISMg9P1Zxa0op1h2NZ/HeC2yOuoyVuRlLRgXRwtuJAp1if8w1vJxsqO1kY+pQhRBCVDMapZQydRC3k56eTqtWrZgzZw7vvfceLVq04IsvviAlJQVXV1cWLFjA448/DsCJEydo0qQJ4eHhtG/fnjVr1tCnTx8uXbqEu7s7AHPnzmXq1KlcvnwZKysrpk6dyqpVqzhy5Ij+OQcOHEhycjJr164tVYypqak4OjqSkpKCg4OD8U+CAP5rpVpyiD//60NlpoG3+zXl2aC6pg1MCCFEpWTs3+9KcSlwzJgx9O7dm+DgYIPyiIgI8vLyDMr9/f3x8fEhPDwcgPDwcAIDA/VJFUBISAipqakcPXpUX+fGfYeEhOj3ISqOfw7H8eeBWMw0MLyzH2sndJGkSgghRIVR4S8FLlq0iP3797N3795i2+Lj47GyssLJycmg3N3dnfj4eH2d65Oqou1F225VJzU1laysLGxsil9SysnJIScnR38/NTW17AcnSuVKeg6vLTvMhuOJ+rIRXerz2sP+JoxKCCGEKK5Ct1hduHCBl19+mdDQUKytrU0djoGZM2fi6Oiov3l7y1xJZaWUYu2ReN76+ygJqdn68hUHYhn+2z7iUrLYH3ONgd//a5BUPdLCi8k9GpkiZCGEEOKWKnSLVUREBImJibRq1UpfVlBQwLZt2/j6669Zt24dubm5JCcnG7RaJSQk4OHhAYCHhwd79uwx2G/RqMHr69w4kjAhIQEHB4cSW6sApk2bxqRJk/T3U1NTJbkqA6UUb6w4woLdMQCsOxrP0A51aeRegwmLDwIQdiyh2ONmDbiPJ2XCTyGEEBVUhU6sunfvTmRkpEHZ888/j7+/P1OnTsXb2xtLS0s2btzIgAEDAIiKiiImJoagoCAAgoKCeP/990lMTMTNzQ2AsLAwHBwcCAgI0NdZvXq1wfOEhYXp91ESrVaLVqs12rFWJ9l5Baw4EMuC3TGYm2lwq6ElLiWbmWtOlFi/V6AHb/drhmsNOd9CCCEqtgqdWNWoUYNmzZoZlNnZ2VGzZk19+bBhw5g0aRIuLi44ODgwbtw4goKCaN++PQA9evQgICCAZ599llmzZhEfH8///d//MWbMGH1iNGrUKL7++mteffVVXnjhBTZt2sSSJUtYtWrVvT3gaiD8zFUG/fCv/v7E4IY81daHh2dv40p6LgDWlmasHNcZrYUZdloLXOysTBWuEEIIUSYVOrEqjc8//xwzMzMGDBhATk4OISEhzJkzR7/d3NyclStXMnr0aIKCgrCzs2PIkCG88847+jp+fn6sWrWKiRMnMnv2bOrUqcOPP/4oc1gZgVKKveeukZ1XgIudFS+FRui3Pd66Di890AAzMw1rJ3QhIyef/THXqFvTjgZu9iaMWgghhLgzlWIeq8pA5rH6nyvpOZhpNFxOy2H7qcu8t+p4sTrvPtKUp9v5Ym4m6/gJIYQwHWP/flf6FitRsfyw7SwfrDnOjem6t4sNF5Ky0GhgwYvtCapf0zQBCiGEEOVIEithoKgB81hcKnZWFmw6kYiFuYa+93lhY2XOnwdiib2WRU17Kwa29cHGylz/2PNXM/h4XVSxpCqwtiPLRncgLiWLjJwCAryqd4ueEEKIqksSq2ou8mIKobvPk5aTT3xKNnHJWVhbmXP2coZBvQ/XnKCJpwMR56/py/aeS8LOqrBzeR1nG37YHk1ugY7ODWvx1aCWXEgqnIfqqbbeWFmY4VvT7l4fnhBCCHFPSWJVDaVl53EpOZuvNp1i5eG4Uj0mM7fAIKkCWB0ZX6ye1sKM6X0CcLK1wsnWisA6jkaJWQghhKgMJLGqJnacusLH605wPimTzNwCcvN1+m1WFmY0crensbsDtZ2scbaz4kF/N6wszPB0tOH38HP8G52EjaU5ox+oj4+LLRMWH2TVDUlZ+3ouvNc/UEb0CSGEqLZkVKCRmHJU4IWkTFZFxvFAY1f8PRzIK9Dxz6FL+NWyI8DLgUV7LvDuymPk6wxfaisLMz59ojl9m3vd0fNeTc/Bxc4KjUZDUkYuzraWaDQyyk8IIUTlIaMChYGYq5k8OmcnVzNy+XR9FPfVcSp2ya7I/XVd6N+yNm3qOuNb05a8AoW99s7fAjXt/zcTukziKYQQQkhiVanpdIpJSw5yNaNwxvK8AlViUuVsa8mLnesxqmt9g3mj7iKnEkIIIUQJ5Ke1Evtl1zn2nb+GnZU5ayd04dzVDE4mpNPa1xkvJ2v+ORRHenY+T7fzkXX2hBBCiHtAEqtK6p9Dl3h35TEAxnVviLeLLd4utnRu6KqvM6yTn6nCE0IIIaolSawqmeTMXP5vxRH9NAnPtPdheOd6Jo5KCCGEECCJVaVy/moGj87ZRdJ/faoebubBm32bynp7QgghRAUhiVUFl51XwO7oJH4PP8/mqEQKdArfmra83a8pXRu5yvQGQgghRAUiiVUFd/ZyBkN+3qO/X8PagvnD2uHtYmvCqIQQQghREkmsKrisvHzqu9qRkJpD54a1eL1XE0mqhBBCiApKEqsKrrWvCxsnP2DqMIQQQghRCmamDkAIIYQQoqqQxEoIIYQQwkgksRJCCCGEMBJJrIQQQgghjEQSKyGEEEIII5HESgghhBDCSCSxEkIIIYQwEkmshBBCCCGMRBIrIYQQQggjkcRKCCGEEMJIJLESQgghhDASSayEEEIIIYxEEishhBBCCCORxEoIIYQQwkgsTB1AVaGUAiA1NdXEkQghhBCitIp+t4t+x++WJFZGkpaWBoC3t7eJIxFCCCFEWaWlpeHo6HjX+9EoY6Vo1ZxOp+PSpUvUqFEDjUZj6nBMIjU1FW9vby5cuICDg4Opw6kw5LwUJ+ekZHJeSibnpWRyXoq7k3OilCItLQ0vLy/MzO6+h5S0WBmJmZkZderUMXUYFYKDg4N8yEsg56U4OSclk/NSMjkvJZPzUlxZz4kxWqqKSOd1IYQQQggjkcRKCCGEEMJIJLESRqPVannzzTfRarWmDqVCkfNSnJyTksl5KZmcl5LJeSmuIpwT6bwuhBBCCGEk0mIlhBBCCGEkklgJIYQQQhiJJFZCCCGEEEYiiZUQQgghhJFIYiUMzJw5k7Zt21KjRg3c3Nzo378/UVFRBnWys7MZM2YMNWvWxN7engEDBpCQkGBQJyYmht69e2Nra4ubmxuvvPIK+fn5BnW2bNlCq1at0Gq1NGjQgHnz5pX34RnFhx9+iEajYcKECfqy6npOYmNjeeaZZ6hZsyY2NjYEBgayb98+/XalFDNmzMDT0xMbGxuCg4M5deqUwT6SkpIYPHgwDg4OODk5MWzYMNLT0w3qHD58mM6dO2NtbY23tzezZs26J8d3JwoKCpg+fTp+fn7Y2NhQv3593n33XYN1yKr6edm2bRt9+/bFy8sLjUbDihUrDLbfy+NfunQp/v7+WFtbExgYyOrVq41+vKV1q/OSl5fH1KlTCQwMxM7ODi8vL5577jkuXbpksI/qdl5uNGrUKDQaDV988YVBeYU6L0qI64SEhKhffvlFHTlyRB08eFD16tVL+fj4qPT0dH2dUaNGKW9vb7Vx40a1b98+1b59e9WhQwf99vz8fNWsWTMVHBysDhw4oFavXq1q1aqlpk2bpq9z9uxZZWtrqyZNmqSOHTumvvrqK2Vubq7Wrl17T4+3rPbs2aPq1q2r7rvvPvXyyy/ry6vjOUlKSlK+vr5q6NChavfu3ers2bNq3bp16vTp0/o6H374oXJ0dFQrVqxQhw4dUv369VN+fn4qKytLX6dnz56qefPm6t9//1Xbt29XDRo0UIMGDdJvT0lJUe7u7mrw4MHqyJEjauHChcrGxkZ999139/R4S+v9999XNWvWVCtXrlTR0dFq6dKlyt7eXs2ePVtfp6qfl9WrV6s33nhDLV++XAHqzz//NNh+r45/586dytzcXM2aNUsdO3ZM/d///Z+ytLRUkZGR5X4OSnKr85KcnKyCg4PV4sWL1YkTJ1R4eLi6//77VevWrQ32Ud3Oy/WWL1+umjdvrry8vNTnn39usK0inRdJrMQtJSYmKkBt3bpVKVX44be0tFRLly7V1zl+/LgCVHh4uFKq8ENiZmam4uPj9XW+/fZb5eDgoHJycpRSSr366quqadOmBs/11FNPqZCQkPI+pDuWlpamGjZsqMLCwlTXrl31iVV1PSdTp05VnTp1uul2nU6nPDw81Mcff6wvS05OVlqtVi1cuFAppdSxY8cUoPbu3auvs2bNGqXRaFRsbKxSSqk5c+YoZ2dn/Xkqeu7GjRsb+5CMonfv3uqFF14wKHvsscfU4MGDlVLV77zc+EN5L4//ySefVL179zaIp127dmrkyJFGPcY7casEosiePXsUoM6fP6+Uqt7n5eLFi6p27drqyJEjytfX1yCxqmjnRS4FiltKSUkBwMXFBYCIiAjy8vIIDg7W1/H398fHx4fw8HAAwsPDCQwMxN3dXV8nJCSE1NRUjh49qq9z/T6K6hTtoyIaM2YMvXv3LhZ3dT0nf//9N23atOGJJ57Azc2Nli1b8sMPP+i3R0dHEx8fb3BMjo6OtGvXzuC8ODk50aZNG32d4OBgzMzM2L17t75Oly5dsLKy0tcJCQkhKiqKa9eulfdhllmHDh3YuHEjJ0+eBODQoUPs2LGDhx9+GKi+56XIvTz+yvaZulFKSgoajQYnJyeg+p4XnU7Hs88+yyuvvELTpk2Lba9o50USK3FTOp2OCRMm0LFjR5o1awZAfHw8VlZW+g96EXd3d+Lj4/V1rk8girYXbbtVndTUVLKyssrjcO7KokWL2L9/PzNnziy2rbqek7Nnz/Ltt9/SsGFD1q1bx+jRoxk/fjy//vor8L/jKumYrj9mNzc3g+0WFha4uLiU6dxVJK+99hoDBw7E398fS0tLWrZsyYQJExg8eDBQfc9LkXt5/DerU5HPT5Hs7GymTp3KoEGD9IsJV9fz8tFHH2FhYcH48eNL3F7RzotFmWqLamXMmDEcOXKEHTt2mDoUk7pw4QIvv/wyYWFhWFtbmzqcCkOn09GmTRs++OADAFq2bMmRI0eYO3cuQ4YMMXF0prNkyRJCQ0NZsGABTZs25eDBg0yYMAEvL69qfV5E6eXl5fHkk0+ilOLbb781dTgmFRERwezZs9m/fz8ajcbU4ZSKtFiJEo0dO5aVK1eyefNm6tSpoy/38PAgNzeX5ORkg/oJCQl4eHjo69w4Iq7o/u3qODg4YGNjY+zDuSsREREkJibSqlUrLCwssLCwYOvWrXz55ZdYWFjg7u5e7c4JgKenJwEBAQZlTZo0ISYmBvjfcZV0TNcfc2JiosH2/Px8kpKSynTuKpJXXnlF32oVGBjIs88+y8SJE/WtndX1vBS5l8d/szoV+fwUJVXnz58nLCxM31oF1fO8bN++ncTERHx8fPTfv+fPn2fy5MnUrVsXqHjnRRIrYUApxdixY/nzzz/ZtGkTfn5+Bttbt26NpaUlGzdu1JdFRUURExNDUFAQAEFBQURGRhq80Yu+IIp+iIOCggz2UVSnaB8VSffu3YmMjOTgwYP6W5s2bRg8eLD+/9XtnAB07Nix2FQcJ0+exNfXFwA/Pz88PDwMjik1NZXdu3cbnJfk5GQiIiL0dTZt2oROp6Ndu3b6Otu2bSMvL09fJywsjMaNG+Ps7Fxux3enMjMzMTMz/Go1NzdHp9MB1fe8FLmXx1/ZPlNFSdWpU6fYsGEDNWvWNNheHc/Ls88+y+HDhw2+f728vHjllVdYt24dUAHPS5m6uosqb/To0crR0VFt2bJFxcXF6W+ZmZn6OqNGjVI+Pj5q06ZNat++fSooKEgFBQXptxdNLdCjRw918OBBtXbtWuXq6lri1AKvvPKKOn78uPrmm28q9NQCN7p+VKBS1fOc7NmzR1lYWKj3339fnTp1SoWGhipbW1s1f/58fZ0PP/xQOTk5qb/++ksdPvz/7d1rTBRXGwfw/xZkZN1dMWLWCqWFCIgWAW3Ti7dilUojWky9rLoi6hctoqhtbKxKtS3a0CZoY0xpwsUYxShNVOrKB9AoKmWFhYhkg0Rso6SNGIsUDArP+6Fx0pHVl5qxS/X/SyZhn3NmzoVkeHJm51Ans2fP9vhafVxcnFRWVsrZs2clPDxc85r07du3xWq1it1ul0uXLsnBgwfFaDT2i20FPElJSZGgoCB1u4Xi4mIJDAyUTz75RK3zrM/LnTt3pKamRmpqagSAfPvtt1JTU6O+3fZvjb+iokJ8fX0lOztbGhoaZOvWrV7dVuBx89LV1SWzZs2S4OBgcblcmvvv399ke97mxZOH3woU6V/zwsSKNAB4PPLy8tQ6nZ2dsmrVKhkyZIgYjUZJTk6WlpYWzXWam5slMTFR/P39JTAwUNavXy/37t3T1CkvL5fY2Fjx8/OTsLAwTRv93cOJ1fM6J8eOHZNXX31VFEWRUaNGyffff68p7+npkc2bN4vVahVFUeTdd98Vt9utqdPa2io2m01MJpNYLBZJTU2VO3fuaOrU1tbKxIkTRVEUCQoKkh07djz1sT2ptrY2WbNmjYSEhMjAgQMlLCxMNm3apPnj+KzPS3l5ucf7SEpKioj8u+M/dOiQREREiJ+fn4wZM0ZKSkqe2rj/n8fNy9WrVx95/y0vL1ev8bzNiyeeEqv+NC8Gkb9tB0xERERET4zfsSIiIiLSCRMrIiIiIp0wsSIiIiLSCRMrIiIiIp0wsSIiIiLSCRMrIiIiIp0wsSIiIiLSCRMrIiIiIp0wsSIir1i6dCk++OADr7Vvt9vx1Vdfea19PeTn5yMgIKBPdR0OB2JjY9X/WUhETwcTKyLSncFgeOyRmZmJnJwc5Ofne6V/tbW1+Omnn5Cenu6V9r1hxowZGDBgAPbv3+/trhA903y93QEieva0tLSoPxcVFWHLli1wu91qzGQywWQyeaNrAIDdu3dj7ty5Xu2DNyxduhS7du2C3W73dleInllcsSIi3Q0fPlw9Bg8eDIPBoImZTKZejwLfeecdrF69GmvXrsWQIUNgtVqRm5uLP//8E6mpqTCbzRg5ciROnDihaevSpUtITEyEyWSC1WqF3W7HzZs3H9m37u5uHD58GElJSZr4nj17EB4ejoEDB8JqteLDDz9Uy3p6epCVlYXQ0FD4+/sjJiYGhw8f1pxfX1+PmTNnwmKxwGw2Y9KkSWhqalLP37ZtG4KDg6EoCmJjY+FwONRzm5ubYTAYUFxcjPj4eBiNRsTExOD8+fOaNvLz8xESEgKj0Yjk5GS0trZqymtraxEfHw+z2QyLxYLx48fD6XSq5UlJSXA6nWq/iEh/TKyIqN8oKChAYGAgfv75Z6xevRorV67E3Llz8fbbb6O6uhoJCQmw2+3o6OgAANy+fRtTp05FXFwcnE4nHA4HfvvtN8ybN++RbdTV1eGPP/7Aa6+9psacTifS09Oxbds2uN1uOBwOTJ48WS3PyspCYWEh9u7di/r6emRkZGDx4sU4ffo0AOD69euYPHkyFEVBWVkZLl68iGXLluH+/fsAgJycHHzzzTfIzs5GXV0d3nvvPcyaNQuNjY2avm3atAkbNmyAy+VCREQEbDabeo3KykosX74caWlpcLlciI+PxxdffKE5f9GiRQgODkZVVRUuXryIjRs3YsCAAWp5SEgIrFYrzpw58yS/HiLqCyEieory8vJk8ODBveIpKSkye/Zs9fOUKVNk4sSJ6uf79+/LoEGDxG63q7GWlhYBIOfPnxcRke3bt0tCQoLmur/++qsAELfb7bE/P/74o/j4+EhPT48aO3LkiFgsFmlra+tV/+7du2I0GuXcuXOa+PLly8Vms4mIyKeffiqhoaHS1dXlsc0RI0bIl19+qYm9/vrrsmrVKhERuXr1qgCQH374QS2vr68XANLQ0CAiIjabTd5//33NNebPn6+ZW7PZLPn5+R778EBcXJxkZmY+tg4RPTmuWBFRvzF27Fj1Zx8fHwwdOhTR0dFqzGq1AgB+//13AH89+iovL1e/s2UymTBq1CgAeOTjrs7OTiiKAoPBoMamT5+Ol19+GWFhYbDb7di/f7+6KnblyhV0dHRg+vTpmnYKCwvVNlwuFyZNmqRZHXqgra0NN27cwIQJEzTxCRMmoKGh4ZHjf/HFFzVjbWhowBtvvKGp/9Zbb2k+r1u3DitWrMC0adOwY8cOj3Pg7++vjo2I9McvrxNRv/FwYmIwGDSxB8nQgy0D2tvbkZSUhJ07d/a61oPE5GGBgYHo6OhAV1cX/Pz8AABmsxnV1dU4deoUSktLsWXLFmRmZqKqqgrt7e0AgJKSEgQFBWmupSgKgL+SFT08bqx9kZmZiYULF6KkpAQnTpzA1q1bcfDgQSQnJ6t1bt26hWHDhunSXyLqjStWRPSfNW7cONTX1+OVV17ByJEjNcegQYM8nhMbGwsAuHz5sibu6+uLadOm4euvv0ZdXR2am5tRVlaG0aNHQ1EU/PLLL73aeOmllwD8tdJ05swZ3Lt3r1d7FosFI0aMQEVFhSZeUVGB0aNH93msUVFRqKys1MQuXLjQq15ERAQyMjJQWlqKOXPmIC8vTy27e/cumpqaEBcX1+d2ieifYWJFRP9ZH330EW7dugWbzYaqqio0NTXh5MmTSE1NRXd3t8dzhg0bhnHjxuHs2bNq7Pjx49i1axdcLheuXbuGwsJC9PT0IDIyEmazGRs2bEBGRgYKCgrQ1NSE6upq7N69GwUFBQCAtLQ0tLW1YcGCBXA6nWhsbMS+ffvULSY+/vhj7Ny5E0VFRXC73di4cSNcLhfWrFnT57Gmp6fD4XAgOzsbjY2N+O677zRvFnZ2diItLQ2nTp3CtWvXUFFRgaqqKkRFRal1Lly4AEVRej1CJCL9MLEiov+sBytB3d3dSEhIQHR0NNauXYuAgAC88MKjb28rVqzQbJQZEBCA4uJiTJ06FVFRUdi7dy8OHDiAMWPGAAC2b9+OzZs3IysrC1FRUZgxYwZKSkoQGhoKABg6dCjKysrQ3t6OKVOmYPz48cjNzVUf7aWnp2PdunVYv349oqOj4XA4cPToUYSHh/d5rG+++SZyc3ORk5ODmJgYlJaW4rPPPlPLfXx80NraiiVLliAiIgLz5s1DYmIiPv/8c7XOgQMHsGjRIhiNxj63S0T/jEFExNudICL6N3V2diIyMhJFRUXPzerNzZs3ERkZCafTqSaERKQ/rlgR0XPH398fhYWFj91I9FnT3NyMPXv2MKkiesq4YkVERESkE65YEREREemEiRURERGRTphYEREREemEiRURERGRTphYEREREemEiRURERGRTphYEREREemEiRURERGRTphYEREREenkf0wMUtBwKLUhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Compute a rolling mean with a window size of 50 for the episode length\n",
    "results['episode_length'] = results['l'].rolling(window=50).mean()\n",
    "# Plot the smoothed episode length over time\n",
    "plt.plot(results['t'], results['episode_length'])\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Average Episode Length')\n",
    "plt.title('Average Episode Length over Time during Training A2C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b7143af-2486-414e-ba8f-ca83030dd4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest experiment, id=2\n",
      "Loading logs/a2c/PongNoFrameskip-v4_2/PongNoFrameskip-v4.zip\n",
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n",
      "Stacking 4 frames\n",
      "Atari Episode Score: 16.00\n",
      "Atari Episode Length 10127\n",
      "Atari Episode Score: 18.00\n",
      "Atari Episode Length 8018\n",
      "Atari Episode Score: 18.00\n",
      "Atari Episode Length 9182\n",
      "Atari Episode Score: 20.00\n",
      "Atari Episode Length 8340\n"
     ]
    }
   ],
   "source": [
    "#Evaluationg our trained agent over 1000 timesteps\n",
    "!python -m rl_zoo3.enjoy  --algo a2c  --env PongNoFrameskip-v4  --no-render  --n-timesteps 10000  --folder logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
